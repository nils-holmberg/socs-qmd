[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "The following info is fetched from timeedit:"
  },
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "Materials",
    "section": "",
    "text": "d1-python.ipynb\nd2-analysis.ipynb\nd3-visualization.ipynb\nd4-language.ipynb\nd5-collect.ipynb"
  },
  {
    "objectID": "materials.html#jupyter-notebooks",
    "href": "materials.html#jupyter-notebooks",
    "title": "Materials",
    "section": "",
    "text": "d1-python.ipynb\nd2-analysis.ipynb\nd3-visualization.ipynb\nd4-language.ipynb\nd5-collect.ipynb"
  },
  {
    "objectID": "d5-collection.html",
    "href": "d5-collection.html",
    "title": "Day 5: Finishing with data collections",
    "section": "",
    "text": "As researchers we not only need to analyze data, but the first step is usually to collect empirical data. Day 5 of the workshop we will turn our attention towards this challenge, and we will explore a couple of packages that can help with this. First off, there is Beautiful Soup which can be used to scrape and analyze web pages. Second, we will have a look at solutions for collecting survey responses using python!"
  },
  {
    "objectID": "d5-collection.html#web-scraping",
    "href": "d5-collection.html#web-scraping",
    "title": "Day 5: Finishing with data collections",
    "section": "Web scraping",
    "text": "Web scraping\nThe content of the webpage “https://books.toscrape.com/catalogue/page-2.html” was fetched using the requests library. Once retrieved, the content was saved as an HTML file named “books_page_2.html”. The Beautiful Soup library was then employed to parse and analyze the HTML structure. The analysis focused on identifying all article tags with the class product_pod. Within each of these tags, the title and link attributes from the nested h3 and a tags were extracted. This extracted data was subsequently organized into a pandas dataframe, displaying the title of each book alongside its corresponding link.\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\n# URL to fetch\nbooks_url = 'https://books.toscrape.com/catalogue/page-2.html'\n\n# Make a request to the website using the requests library\nbooks_response = requests.get(books_url)\nbooks_content = books_response.content\n\n# Save the content to an HTML file\nbooks_filename = '../../xml/books_page_2.html'\nwith open(books_filename, 'wb') as file:\n    file.write(books_content)\n\n# Parse the content with BeautifulSoup\nbooks_soup = BeautifulSoup(books_content, 'lxml')\n\n# Look for all article tags of class 'product_pod'\nproduct_pods = books_soup.find_all('article', class_='product_pod')\n\n# Extract the h3 child tag and its a tag attributes as text\nbooks_data = []\nfor pod in product_pods:\n    h3_tag = pod.find('h3')\n    a_tag = h3_tag.find('a') if h3_tag else None\n    books_data.append({\n        'title': a_tag.attrs.get('title') if a_tag else None,\n        'link': a_tag.attrs.get('href') if a_tag else None\n    })\n\n# Convert the data to a pandas dataframe\nbooks_df = pd.DataFrame(books_data)\nbooks_df.head()\n\n\n\n\n\n\n\n\ntitle\nlink\n\n\n\n\n0\nIn Her Wake\nin-her-wake_980/index.html\n\n\n1\nHow Music Works\nhow-music-works_979/index.html\n\n\n2\nFoolproof Preserving: A Guide to Small Batch J...\nfoolproof-preserving-a-guide-to-small-batch-ja...\n\n\n3\nChase Me (Paris Nights #2)\nchase-me-paris-nights-2_977/index.html\n\n\n4\nBlack Dust\nblack-dust_976/index.html"
  },
  {
    "objectID": "d5-collection.html#survey-experiments",
    "href": "d5-collection.html#survey-experiments",
    "title": "Day 5: Finishing with data collections",
    "section": "Survey experiments",
    "text": "Survey experiments\n\nUpcoming, streamlit ? pyscript ? shiny ?"
  },
  {
    "objectID": "d3-visualization.html",
    "href": "d3-visualization.html",
    "title": "Day 3: Visualizing and reporting results",
    "section": "",
    "text": "Analyzing data is only half of a data scientist’s work. The other half is about using graphs and diagrams to visualize data in ways that will allow us to report our results as intuitive and compelling stories. On day 3 of the workshop we are going to focus on packages such as matplotlib and seaborn to produce various types of diagrams ranging from simple frequency diagrams to time series and heatmaps."
  },
  {
    "objectID": "d3-visualization.html#frequency-diagrams",
    "href": "d3-visualization.html#frequency-diagrams",
    "title": "Day 3: Visualizing and reporting results",
    "section": "Frequency diagrams",
    "text": "Frequency diagrams\nThe frequency diagram, or histogram, visually represents the distribution of the “sepal_length” variable from the df DataFrame. In the example, we used Seaborn’s histplot function to plot the diagram with 20 bins. The kernel density estimation (KDE) curve is also overlaid on the histogram to give a smoother representation of the data distribution. The x-axis represents the range of “sepal_length” values, while the y-axis shows the frequency of occurrences for each bin.\nHere’s the code snippet to generate the frequency diagram:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create a frequency diagram for 'sepal_length'\nsns.histplot(df['sepal_length'], bins=20, kde=True)\n\n# Add labels and title\nplt.xlabel('Sepal Length')\nplt.ylabel('Frequency')\nplt.title('Frequency Diagram of Sepal Length')\n\n# Show the plot\nplt.show()\n\n/tmp/scom/envp-nlp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/tmp/scom/envp-nlp/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\nThis visualization allows you to quickly grasp the shape, center, and spread of the “sepal_length” data."
  },
  {
    "objectID": "d3-visualization.html#bar-plots",
    "href": "d3-visualization.html#bar-plots",
    "title": "Day 3: Visualizing and reporting results",
    "section": "Bar plots",
    "text": "Bar plots\nBar plots are useful for displaying the relationship between a categorical variable and a numerical variable. In the example, we use Seaborn’s barplot function to visualize the average “sepal_length” for each species in the df DataFrame. The x-axis represents the different species, and the y-axis shows the average “sepal_length” for each.\nHere’s the code snippet to generate the bar plot:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create a barplot for the 'species' column showing the average 'sepal_length'\nsns.barplot(x='species', y='sepal_length', data=df, ci=None)\n\n# Add labels and title\nplt.xlabel('Species')\nplt.ylabel('Average Sepal Length')\nplt.title('Average Sepal Length by Species')\n\n# Show the plot\nplt.show()\n\n/tmp/ipykernel_87616/4094272089.py:5: FutureWarning: \n\nThe `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n\n  sns.barplot(x='species', y='sepal_length', data=df, ci=None)\n/tmp/scom/envp-nlp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/tmp/scom/envp-nlp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/tmp/scom/envp-nlp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n\n\n\n\n\nIn this case, the ci=None parameter removes the confidence interval bars, focusing solely on the mean values. The plot provides a quick way to compare the average “sepal_length” across different species."
  },
  {
    "objectID": "d3-visualization.html#scatter-plots",
    "href": "d3-visualization.html#scatter-plots",
    "title": "Day 3: Visualizing and reporting results",
    "section": "Scatter plots",
    "text": "Scatter plots\nScatter plots are excellent tools for visualizing relationships between two numerical variables. In the given example, we use Seaborn’s scatterplot function to create a scatter plot of “sepal_length” against “sepal_width” from the df DataFrame. The points are colored based on the “species” category, providing an additional layer of information.\nHere’s the code snippet to generate the scatter plot:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create a scatter plot for 'sepal_length' and 'sepal_width' colored by 'species'\nsns.scatterplot(x='sepal_length', y='sepal_width', hue='species', data=df)\n\n# Add labels and title\nplt.xlabel('Sepal Length')\nplt.ylabel('Sepal Width')\nplt.title('Scatter Plot of Sepal Dimensions by Species')\n\n# Show the plot\nplt.show()\n\n/tmp/scom/envp-nlp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/tmp/scom/envp-nlp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/tmp/scom/envp-nlp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/tmp/scom/envp-nlp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/tmp/scom/envp-nlp/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n\n\n\n\n\nThis scatter plot allows you to identify patterns or relationships between “sepal_length” and “sepal_width” while also considering the species. It’s a powerful way to explore multidimensional data."
  },
  {
    "objectID": "d3-visualization.html#heatmaps",
    "href": "d3-visualization.html#heatmaps",
    "title": "Day 3: Visualizing and reporting results",
    "section": "Heatmaps",
    "text": "Heatmaps\nHeatmaps are excellent tools for visualizing complex relationships between numerical variables. In Python, the Seaborn library provides an easy-to-use heatmap function for this purpose. For instance, you can create a heatmap of the correlation matrix of numerical features in the df DataFrame. The color gradients in the heatmap represent the strength and direction of correlation, making it easier to identify highly or weakly correlated variables.\nHere’s an inline code example to generate the heatmap:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Drop the 'species' column to only keep numerical columns\nnumerical_df = df.drop('species', axis=1)\n\n# Calculate the correlation matrix for the numerical columns\ncorrelation_matrix = numerical_df.corr()\n\n# Create a heatmap to visualize the correlation matrix\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n\n# Add title\nplt.title('Heatmap of Feature Correlations')\n\n# Show the plot\nplt.show()\n\n\n\n\nThis heatmap makes it easier to understand the relationships between different numerical features, aiding in feature selection and further data analysis."
  },
  {
    "objectID": "d1-python.html",
    "href": "d1-python.html",
    "title": "Day 1: Setting up a python environment",
    "section": "",
    "text": "There are many ways of setting up a python environment. Day 1 of the workshop we are going to have a look at some of the most common use cases, and discuss pros and cons with different setups depending on the specific research tasks you are trying to solve. We will also have a look at basic python syntax and write some simple python programs. The main format of instruction will be Jupyter notebooks."
  },
  {
    "objectID": "d1-python.html#the-python-interpreter",
    "href": "d1-python.html#the-python-interpreter",
    "title": "Day 1: Setting up a python environment",
    "section": "The python interpreter",
    "text": "The python interpreter\nThe Python interpreter is a tool that allows for the execution of Python code directly. To install, one typically downloads the appropriate version from the official Python website and follows the installation instructions. Once installed, the interpreter can be launched by typing python in the command line or terminal. To open a file on disk, use file = open('filename.txt', 'r'). To run a simple script, save the code to a .py file, like script.py, and execute it with python script.py from the command line. From the Python prompt, scripts can be run using exec(open('script.py').read()). Working with python in this environment can be improved with a good cross-platform code editor, e.g. Visual Studio Code."
  },
  {
    "objectID": "d1-python.html#integrated-development-environments",
    "href": "d1-python.html#integrated-development-environments",
    "title": "Day 1: Setting up a python environment",
    "section": "Integrated development environments",
    "text": "Integrated development environments\nAnaconda is a distribution that simplifies Python and R data science and machine learning on Linux, Windows, and Mac OS X. It bundles a suite of tools, including an IDE called Spyder. To use Python in Spyder, one installs Anaconda3, launches Spyder, and begins coding in its interactive editor. Spyder offers features like variable exploration, integrated IPython console, and a multi-language editor. Compared to a standalone Python interpreter, which offers a basic environment for script execution, Spyder provides a more comprehensive development experience with debugging tools, code completion, and an integrated documentation viewer, streamlining the coding and analysis process. NB! If you are on a LU work computer, you should consult with IT support () to istall from Software center."
  },
  {
    "objectID": "d1-python.html#jupyter-notebooks",
    "href": "d1-python.html#jupyter-notebooks",
    "title": "Day 1: Setting up a python environment",
    "section": "Jupyter notebooks",
    "text": "Jupyter notebooks\nJupyter Notebooks offer an interactive environment to write and execute Python code. Accessible via a web browser, users can combine code, text, and visualizations in a single document. To use Python in Jupyter, one installs Jupyter via pip or conda, then launches it, creating or opening notebooks. Each notebook cell can be executed independently, allowing iterative development. Compared to a Python interpreter, Jupyter provides a more visual and structured approach. While Python IDEs offer robust development tools and debugging capabilities, Jupyter excels in data exploration, analysis, and presenting results, making it a favorite among data scientists and researchers. Try jupyter notebooks on Anaconda cloud."
  },
  {
    "objectID": "d1-python.html#python-in-the-cloud",
    "href": "d1-python.html#python-in-the-cloud",
    "title": "Day 1: Setting up a python environment",
    "section": "Python in the cloud",
    "text": "Python in the cloud\nGoogle colab is a cloud-based platform that allows users to write and execute Python code in a web-based environment. To use Python in Google Colab, one simply navigates to the Colab website, starts a new notebook, and begins coding. The platform provides free access to GPUs, making it suitable for machine learning tasks. Unlike local Python installations, there’s no setup required, and notebooks are easily shareable. However, while Colab offers the convenience of working in the cloud, using Python locally provides more control over the environment, dependencies, and data storage. In contrast, Colab sessions are ephemeral, and prolonged inactivity can lead to disconnection."
  },
  {
    "objectID": "d1-python.html#basic-python-syntax",
    "href": "d1-python.html#basic-python-syntax",
    "title": "Day 1: Setting up a python environment",
    "section": "Basic python syntax",
    "text": "Basic python syntax\n\nPython Data Types\nPython, a versatile and powerful programming language, boasts several built-in data types. Among the most fundamental are:\n\nIntegers (int): Whole numbers like 3 or -11.\nFloating Point (float): Numbers with a decimal point, e.g., 3.14.\nStrings (str): Sequences of characters, like “Hello”.\nLists: Ordered collections, e.g., [1, 2, “a”].\nTuples: Immutable ordered collections, e.g., (1, 2, “b”).\nDictionaries (dict): Key-value pairs, e.g., {“name”: “John”, “age”: 30}.\n\nEach type has its unique properties and methods, catering to diverse programming needs. Now, here’s a code example showcasing these data types:\n\n# Integer\nmy_int = 5\nprint(\"Integer:\", my_int)\n\n# Float\nmy_float = 5.5\nprint(\"Float:\", my_float)\n\n# String\nmy_string = \"Hello, World!\"\nprint(\"String:\", my_string)\n\n# List\nmy_list = [1, 2, \"three\", 4.0]\nprint(\"List:\", my_list)\n\n# Tuple\nmy_tuple = (1, \"two\", 3.0)\nprint(\"Tuple:\", my_tuple)\n\n# Dictionary\nmy_dict = {\"first_name\": \"John\", \"last_name\": \"Doe\", \"age\": 30}\nprint(\"Dictionary:\", my_dict)\n\n\n\nPython Control Statements\nHere’s a concise explanation of Python’s most important control statements, followed by a code example. Control statements in Python determine the flow of execution in a program. The primary ones include:\n\nIf Statement: Tests a condition and executes a block of code if true.\nElif and Else: Supplements the if statement, allowing for multiple conditions or a default action.\nFor Loop: Iterates over sequences like lists or strings.\nWhile Loop: Continues execution as long as a condition remains true.\nBreak: Exits the current loop.\nContinue: Skips the rest of the current loop iteration, moving to the next one.\n\nThese tools provide the foundation for complex decision-making and repetitive tasks in Python programs. Here’s a code example showcasing these control statements:\n# If, Elif, and Else statements\nx = 10\nif x &gt; 5:\n    print(\"x is greater than 5\")\nelif x == 5:\n    print(\"x is equal to 5\")\nelse:\n    print(\"x is less than 5\")\n\n# For Loop\nfor i in range(3):\n    print(f\"For Loop iteration {i}\")\n\n# While Loop\ncount = 0\nwhile count &lt; 3:\n    print(f\"While Loop iteration {count}\")\n    count += 1\n\n# Break and Continue\nfor num in range(5):\n    if num == 2:\n        break\n    elif num == 1:\n        continue\n    print(f\"Value after applying break and continue: {num}\")\n\n\nPython Functions\nPython functions are modular blocks of code designed to perform a specific task. Their key aspects include:\n\nDefinition: Using the def keyword, functions are declared, followed by their name and parameters.\nPositional Arguments: Values passed based on their position in the function call.\nNamed (or Keyword) Arguments: Values passed by explicitly naming the parameter.\nReturn Statement: Outputs a value from the function.\nDefault Values: Assigns a default value to a parameter if no argument is provided.\nVariable-Length Arguments: *args and **kwargs allow for arbitrary numbers of positional and keyword arguments, respectively.\n\nFunctions enhance code reusability and structure, making complex programs more manageable. Here’s a code example showcasing a function with named and positional arguments:\ndef example_function(positional_arg, named_arg=\"default\", *args, **kwargs):\n    print(f\"Positional Argument: {positional_arg}\")\n    print(f\"Named Argument: {named_arg}\")\n    for index, value in enumerate(args):\n        print(f\"Additional positional argument {index}: {value}\")\n    for key, value in kwargs.items():\n        print(f\"Keyword argument {key}: {value}\")\n    return [positional_arg, named_arg, args, kwargs]\n\nresult = example_function(1, \"test\", 2, 3, 4, key1=\"value1\", key2=\"value2\")\nprint(\"\\nReturned Value:\", result)\n\n\nSome python tasks\nHere are 10 tasks to test students’ understanding of Python data types, control statements, and functions:\n# Task 1: Change the string to output \"Hello, Python!\".\nstring_task = \"Hello, World!\"\nprint(string_task)\n\n# Task 2: Add a floating point number to the integer to get a result of 15.5.\ninteger_task = 10\nprint(integer_task)\n\n# Task 3: Update the list to have a fifth element \"apple\".\nlist_task = [1, 2, 3, 4]\nprint(list_task)\n\n# Task 4: Use a control statement to print \"Positive\" if number_task is greater than zero.\nnumber_task = 5\n\n# Task 5: Modify the function to return the product of a and b.\ndef function_task(a, b):\n    return a + b\nprint(function_task(3, 4))\n\n# Task 6: Call the function above with named arguments in reverse order (b first, then a).\nprint(function_task(a=1, b=2))\n\n# Task 7: Modify the loop to print numbers from 1 to 5.\nfor i in range(3):\n    print(i)\n\n# Task 8: Use a control statement in the function below to return \"Even\" for even numbers and \"Odd\" for odd numbers.\ndef even_odd(num):\n    return \"Number\"\nprint(even_odd(3))\n\n# Task 9: Add a default value to the function parameter so that calling function_default() returns \"Hello, World!\".\ndef function_default(message):\n    return message\nprint(function_default())\n\n# Task 10: Correct the function to return the sum of all numbers in the list, use a control statement.\ndef sum_function(numbers):\n    return 0\nprint(sum_function([1, 2, 3, 4, 5]))\n\nFor each task, students should modify the Python code to achieve the described goal."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Transparency, replicability, collaboration, and open source are four pivotal pillars of open science. Transparency ensures that research processes and findings are openly shared, fostering trust and accountability. Replicability emphasizes the importance of detailed documentation, enabling others to reproduce experiments and validate results, thereby enhancing the reliability of scientific discoveries. Collaboration encourages interdisciplinary and international teamwork, harnessing diverse expertise for more comprehensive insights."
  },
  {
    "objectID": "about.html#open-science-methods",
    "href": "about.html#open-science-methods",
    "title": "About",
    "section": "",
    "text": "Transparency, replicability, collaboration, and open source are four pivotal pillars of open science. Transparency ensures that research processes and findings are openly shared, fostering trust and accountability. Replicability emphasizes the importance of detailed documentation, enabling others to reproduce experiments and validate results, thereby enhancing the reliability of scientific discoveries. Collaboration encourages interdisciplinary and international teamwork, harnessing diverse expertise for more comprehensive insights."
  },
  {
    "objectID": "about.html#python-workshops",
    "href": "about.html#python-workshops",
    "title": "About",
    "section": "Python workshops",
    "text": "Python workshops\nOpen source practices promote the development and sharing of accessible research tools and software, making science more accessible and cost-effective. Together, these principles redefine how science is conducted, making it more rigorous, inclusive, and efficient, ultimately benefiting both researchers and society at large."
  },
  {
    "objectID": "about.html#workshop-instructor",
    "href": "about.html#workshop-instructor",
    "title": "About",
    "section": "Workshop instructor",
    "text": "Workshop instructor\nNils Holmberg"
  },
  {
    "objectID": "d2-analysis.html",
    "href": "d2-analysis.html",
    "title": "Day 2: Getting started with data analysis",
    "section": "",
    "text": "One of the most common use cases for python is data analysis. On day 2 of the workshop we are going to focus on a package called pandas. This package will allow us to import structured data such as excel files and other quantitative tabular data, and transform and summarize such data. We will also introduce scikit-learn for statistical analysis. Participants are invited to bring their own data files."
  },
  {
    "objectID": "d2-analysis.html#import-data-files",
    "href": "d2-analysis.html#import-data-files",
    "title": "Day 2: Getting started with data analysis",
    "section": "Import data files",
    "text": "Import data files\nTo read a CSV file into a Pandas DataFrame, you’ll first need to install the Pandas library if you haven’t already. You can install it using pip with the command pip install pandas. Once installed, you can use the read_csv() function to load the data from the file into a DataFrame. The function takes the file path as an argument and returns a DataFrame containing the data. Here’s how you can read the uploaded file, “iris.csv”, into a Pandas DataFrame:\nimport pandas as pd\n\n# Read the CSV file into a DataFrame\ndf = pd.read_csv('/path/to/your/iris.csv')\n\n# Display the first few rows of the DataFrame\nprint(df.head())\nReplace '/path/to/your/iris.csv' with the actual path where your file is located. This will give you a DataFrame df that contains all the data from “iris.csv”."
  },
  {
    "objectID": "d2-analysis.html#selecting-and-filtering",
    "href": "d2-analysis.html#selecting-and-filtering",
    "title": "Day 2: Getting started with data analysis",
    "section": "Selecting and filtering",
    "text": "Selecting and filtering\nIn Pandas, two basic yet powerful operations are selecting specific columns and filtering rows. To select columns, you can use the syntax df[['column1', 'column2']], which creates a new DataFrame containing only the selected columns. For example:\n\n# To select 'sepal_length' and 'species' columns\nselected_columns = df[['sepal_length', 'species']]\n# Show the first few rows of the resulting DataFrame\nselected_columns.head()\n\n\n\n\n\n\n\n\nsepal_length\nspecies\n\n\n\n\n0\n5.1\nsetosa\n\n\n1\n4.9\nsetosa\n\n\n2\n4.7\nsetosa\n\n\n3\n4.6\nsetosa\n\n\n4\n5.0\nsetosa\n\n\n\n\n\n\n\nTo filter rows based on a condition, boolean indexing can be employed. The syntax df[df['column'] &gt; value] filters rows where the values in the specified column meet the condition. For instance:\n\n# To filter rows where 'sepal_length' is greater than 5\nfiltered_rows = df[df['sepal_length'] &gt; 5]\n# Show the first few rows of the resulting DataFrame\nfiltered_rows.head()\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n5\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n10\n5.4\n3.7\n1.5\n0.2\nsetosa\n\n\n14\n5.8\n4.0\n1.2\n0.2\nsetosa\n\n\n15\n5.7\n4.4\n1.5\n0.4\nsetosa\n\n\n\n\n\n\n\nBoth operations return new DataFrames, which can then be used for further analysis."
  },
  {
    "objectID": "d2-analysis.html#grouping-and-summarizing",
    "href": "d2-analysis.html#grouping-and-summarizing",
    "title": "Day 2: Getting started with data analysis",
    "section": "Grouping and summarizing",
    "text": "Grouping and summarizing\nGrouping and summarizing data in Pandas is primarily achieved using the groupby() function. This function allows you to group rows based on one or multiple columns, and then you can apply aggregation methods like mean(), sum(), or count() to summarize the data. For instance, if you want to find the average measurements for each species in the df DataFrame, you can group by the ‘species’ column and then apply the mean() function to get the average for each numerical column.\nHere’s an inline code example:\n\n# Group by 'species' and calculate the mean for each numerical column\ngrouped_by_species_mean = df.groupby('species').mean()\n# Show the first few rows of the resulting DataFrame\ngrouped_by_species_mean.head()\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\nspecies\n\n\n\n\n\n\n\n\nsetosa\n5.006\n3.428\n1.462\n0.246\n\n\nversicolor\n5.936\n2.770\n4.260\n1.326\n\n\nvirginica\n6.588\n2.974\n5.552\n2.026\n\n\n\n\n\n\n\nThis will give you a new DataFrame that contains the summarized data, facilitating easier comparisons between different groups."
  },
  {
    "objectID": "d2-analysis.html#statistical-analysis",
    "href": "d2-analysis.html#statistical-analysis",
    "title": "Day 2: Getting started with data analysis",
    "section": "Statistical analysis",
    "text": "Statistical analysis\nRegression analysis is used to explore the relationship between dependent and independent variables. In Python, Scikit-learn is a popular library for performing regression. You typically use the LinearRegression class to create a regression model. After separating your features and target variables, you can fit the model using the fit() method and make predictions with predict(). For example, if you want to predict ‘petal_length’ based on ‘sepal_length’ in the df DataFrame:\n\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\n\n# Prepare the features and target variable\nX = df[['sepal_length']]  # Feature (independent variable)\ny = df['petal_length']  # Target (dependent variable)\n\n# Create a LinearRegression object\nmodel = LinearRegression()\n\n# Fit the model to the data\nmodel.fit(X, y)\n\n# Make predictions\npredictions = model.predict(X)\n\n# Create a DataFrame to display the analysis result\nresult_df = pd.DataFrame({'Actual': y, 'Predicted': predictions})\n# Show the first few rows of the result DataFrame\nresult_df.head()\n\n\n\n\n\n\n\n\nActual\nPredicted\n\n\n\n\n0\n1.4\n2.376565\n\n\n1\n1.4\n2.004878\n\n\n2\n1.3\n1.633192\n\n\n3\n1.5\n1.447348\n\n\n4\n1.4\n2.190722\n\n\n\n\n\n\n\nThis will create a DataFrame result_df that contains both the actual and predicted ‘petal_length’, facilitating the evaluation of the model’s performance."
  },
  {
    "objectID": "d4-language.html",
    "href": "d4-language.html",
    "title": "Day 4: Processing natural language texts",
    "section": "",
    "text": "The opposite of structured data is natural language texts. However, as social scientists, we often encounter such unstructured text data. The challenge of managing and analyzing natural language data will be the main focus on day 4. To our help we will utilize both classical packages for data cleaning and tokenization such as NLTK, and more modern packages such as spaCy that relies on machine learning models to infer syntactic function and named entities."
  },
  {
    "objectID": "d4-language.html#tokenization",
    "href": "d4-language.html#tokenization",
    "title": "Day 4: Processing natural language texts",
    "section": "Tokenization",
    "text": "Tokenization\nLet’s assume the following text data:\n\nimport pandas as pd\n# Read the uploaded TSV file into a Pandas DataFrame named 'df'\ndf = pd.read_csv('../../txt/zen_of_python.tsv', sep='\\t')\n\n# Display the first few rows of the DataFrame\ndf.head()\n\n\n\n\n\n\n\n\nid\ntext\n\n\n\n\n0\n1\nBeautiful is better than ugly.\n\n\n1\n2\nExplicit is better than implicit.\n\n\n2\n3\nSimple is better than complex.\n\n\n3\n4\nComplex is better than complicated.\n\n\n4\n5\nFlat is better than nested.\n\n\n\n\n\n\n\nTo tokenize the text data using the Natural Language Toolkit (NLTK) package, you can follow these steps:\n\nFirst, import the necessary NLTK library: from nltk.tokenize import word_tokenize.\nCreate an empty DataFrame to store the tokenized words along with their corresponding ‘id’ from the original text.\nLoop through each row of the original DataFrame (df), tokenize the text in the ‘text’ column using word_tokenize(), and append the tokens along with their ‘id’ to the new DataFrame.\n\nHere’s an inline code example:\n\nfrom nltk.tokenize import word_tokenize\nimport pandas as pd\n\n# Create an empty DataFrame to store tokens and ids\ntokens_df = pd.DataFrame(columns=['id', 'token'])\n\n# Loop through each row in the original DataFrame\nfor index, row in df.iterrows():\n    id_value = row['id']\n    text_value = row['text']\n    \n    # Tokenize the text\n    tokens = word_tokenize(text_value)\n    \n    # Create a temporary DataFrame to hold tokens and ids\n    temp_df = pd.DataFrame({'id': [id_value]*len(tokens), 'token': tokens})\n    \n    # Append to the main DataFrame\n    tokens_df = pd.concat([tokens_df, temp_df], ignore_index=True)\n\n# Show the first few rows of the resulting DataFrame\ntokens_df.head()\n\n\n\n\n\n\n\n\nid\ntoken\n\n\n\n\n0\n1\nBeautiful\n\n\n1\n1\nis\n\n\n2\n1\nbetter\n\n\n3\n1\nthan\n\n\n4\n1\nugly\n\n\n\n\n\n\n\nRunning this code will create a new DataFrame tokens_df that contains one token per row, along with the original ‘id’ to associate each token with its originating text."
  },
  {
    "objectID": "d4-language.html#parts-of-speech",
    "href": "d4-language.html#parts-of-speech",
    "title": "Day 4: Processing natural language texts",
    "section": "Parts of speech",
    "text": "Parts of speech\nSpacy is a prominent Python library for natural language processing. To analyze the Zen of Python with Spacy, one must first install the package and its English model. After loading the model, the Zen text can be processed to tokenize it. For a visual syntactic analysis of the first sentence, Spacy’s displacy module can be employed.\n\nimport spacy\nfrom spacy import displacy\n\n# Open the file in read mode\nwith open('../../txt/zen_of_python.txt', 'r') as file:\n    zen_text = file.read()\n\n# Load the English model\nnlp = spacy.load('en_core_web_sm')\n#nlp._path\n\n# Process the Zen of Python text\ndoc = nlp(zen_text)\n\n# Visualize the syntactic structure of the first sentence\ndisplacy.render(list(doc.sents)[0], style='dep', jupyter=True)\n\n\n\n    Beautiful\n    PROPN\n\n\n\n    is\n    AUX\n\n\n\n    better\n    ADJ\n\n\n\n    than\n    ADP\n\n\n\n    ugly.\n    PUNCT\n\n\n\n    \n\n    SPACE\n\n\n\n    \n    \n        nsubj\n    \n    \n\n\n\n    \n    \n        acomp\n    \n    \n\n\n\n    \n    \n        prep\n    \n    \n\n\n\n    \n    \n        punct\n    \n    \n\n\n\n    \n    \n        dep\n    \n    \n\n\n\n\nThis code provides a graphical representation of the sentence’s grammatical relationships."
  },
  {
    "objectID": "d4-language.html#topic-modeling",
    "href": "d4-language.html#topic-modeling",
    "title": "Day 4: Processing natural language texts",
    "section": "Topic modeling",
    "text": "Topic modeling\n\nUpcoming"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Python",
    "section": "",
    "text": "The Python programming language (logo: Figure 1) has gained popularity in all types of data science in recent years. On this introductory workshop, we are aiming at getting acquainted with the basic syntax of Python, as well as learning how to extend this basic functionality by calling powerful modules built by members of the large Python user community. Thus, we take a “standing on the shoulders of giants” approach to Python!\n\n\n\nFigure 1: Official logo of the Python programming language\n\n\nAnother principle that has guided the design of this course is that participants should gain an understanding of how Python can be used to accomplish fairly common research tasks within the social sciences more effectively and more transparently. Such applications of Python include collecting and analyzing survey data, performing content analyses on textual and visual material, and using charts to visualize results. (workshop syllabus)\n\nThis is a Quarto website. To learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "index.html#social-science-methods-workshops-2023-lund-university",
    "href": "index.html#social-science-methods-workshops-2023-lund-university",
    "title": "Introduction to Python",
    "section": "",
    "text": "The Python programming language (logo: Figure 1) has gained popularity in all types of data science in recent years. On this introductory workshop, we are aiming at getting acquainted with the basic syntax of Python, as well as learning how to extend this basic functionality by calling powerful modules built by members of the large Python user community. Thus, we take a “standing on the shoulders of giants” approach to Python!\n\n\n\nFigure 1: Official logo of the Python programming language\n\n\nAnother principle that has guided the design of this course is that participants should gain an understanding of how Python can be used to accomplish fairly common research tasks within the social sciences more effectively and more transparently. Such applications of Python include collecting and analyzing survey data, performing content analyses on textual and visual material, and using charts to visualize results. (workshop syllabus)\n\nThis is a Quarto website. To learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "resources.html#articles",
    "href": "resources.html#articles",
    "title": "Resources",
    "section": "Articles",
    "text": "Articles"
  },
  {
    "objectID": "resources.html#websites",
    "href": "resources.html#websites",
    "title": "Resources",
    "section": "Websites",
    "text": "Websites"
  },
  {
    "objectID": "resources.html#notebooks",
    "href": "resources.html#notebooks",
    "title": "Resources",
    "section": "Notebooks",
    "text": "Notebooks"
  }
]