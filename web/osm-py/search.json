[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "The following info is fetched from timeedit:"
  },
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "Materials",
    "section": "",
    "text": "d1-python.ipynb\nd2-analysis.ipynb\nd3-visualization.ipynb\nd4-language.ipynb\nd5-collection.ipynb"
  },
  {
    "objectID": "materials.html#jupyter-notebooks",
    "href": "materials.html#jupyter-notebooks",
    "title": "Materials",
    "section": "",
    "text": "d1-python.ipynb\nd2-analysis.ipynb\nd3-visualization.ipynb\nd4-language.ipynb\nd5-collection.ipynb"
  },
  {
    "objectID": "d5-collection.html",
    "href": "d5-collection.html",
    "title": "Day 5: Finishing with data collections",
    "section": "",
    "text": "As researchers we not only need to analyze data, but the first step is usually to collect empirical data. Day 5 of the workshop we will turn our attention towards this challenge, and we will explore a couple of packages that can help with this. First off, there is Beautiful Soup which can be used to scrape and analyze web pages. Second, we will have a look at solutions for collecting survey responses using python!"
  },
  {
    "objectID": "d5-collection.html#web-scraping",
    "href": "d5-collection.html#web-scraping",
    "title": "Day 5: Finishing with data collections",
    "section": "Web scraping",
    "text": "Web scraping\nIn the realm of web scraping, robots.txt is a standard used by websites to instruct web crawling and scraping bots about which pages should not be processed or scanned. It helps website owners control how search engines index their content. The sitemap.xml file, on the other hand, provides a roadmap of a website’s structure, aiding search engines in navigation. Both files are essential for ethical and effective web scraping.\nSample robots.txt:\nUser-agent: *\nDisallow: /private/\nDisallow: /temp/\nSample sitemap.xml:\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"&gt;\n   &lt;url&gt;\n      &lt;loc&gt;http://www.example.com/&lt;/loc&gt;\n      &lt;lastmod&gt;2022-09-24&lt;/lastmod&gt;\n   &lt;/url&gt;\n&lt;/urlset&gt;\nFor web scraping in Python, the BeautifulSoup and Scrapy packages are popular choices. While BeautifulSoup is ideal for parsing HTML and XML documents, Scrapy provides a powerful framework for large-scale web scraping tasks.\n\nBeautifulSoup\nThe content of the webpage “https://books.toscrape.com/catalogue/page-2.html” was fetched using the requests library. Once retrieved, the content was saved as an HTML file named “books_page_2.html”. The Beautiful Soup library was then employed to parse and analyze the HTML structure. The analysis focused on identifying all article tags with the class product_pod. Within each of these tags, the title and link attributes from the nested h3 and a tags were extracted. This extracted data was subsequently organized into a pandas dataframe, displaying the title of each book alongside its corresponding link.\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\n# URL to fetch\nbooks_url = 'https://books.toscrape.com/catalogue/page-2.html'\n\n# Make a request to the website using the requests library\nbooks_response = requests.get(books_url)\nbooks_content = books_response.content\n\n# Save the content to an HTML file\nbooks_filename = 'books_page_2.html'\nwith open(books_filename, 'wb') as file:\n    file.write(books_content)\n\n# Parse the content with BeautifulSoup\nbooks_soup = BeautifulSoup(books_content, 'lxml')\n\n# Look for all article tags of class 'product_pod'\nproduct_pods = books_soup.find_all('article', class_='product_pod')\n\n# Extract the h3 child tag and its a tag attributes as text\nbooks_data = []\nfor pod in product_pods:\n    h3_tag = pod.find('h3')\n    a_tag = h3_tag.find('a') if h3_tag else None\n    books_data.append({\n        'title': a_tag.attrs.get('title') if a_tag else None,\n        'link': a_tag.attrs.get('href') if a_tag else None\n    })\n\n# Convert the data to a pandas dataframe\nbooks_df = pd.DataFrame(books_data)\nbooks_df.head()"
  },
  {
    "objectID": "d5-collection.html#put-it-together",
    "href": "d5-collection.html#put-it-together",
    "title": "Day 5: Finishing with data collections",
    "section": "Put it together!",
    "text": "Put it together!\n\nwebsite data collection\nnlp\ndata analysis\nsummarize results\nvisualization, wordcloud"
  },
  {
    "objectID": "d5-collection.html#online-survey-experiments",
    "href": "d5-collection.html#online-survey-experiments",
    "title": "Day 5: Finishing with data collections",
    "section": "Online survey experiments",
    "text": "Online survey experiments\nWork in progress. Check out streamlit and github for more info!\n\nhttps://github.com/nils-holmberg/scom-expm\n\n\n#!pip install streamlit\n\nimport streamlit as st\nimport random\nimport pandas as pd\nimport seaborn as sns\n\nst.title(\"ab test, random assignment..\")\n\nconditions = [\"a\", \"b\"]\nselected_condition = random.choice(conditions)\n\nif selected_condition == \"a\":\n    st.image(\"img/exps-stim-c.png\")\nelse:\n    st.image(\"img/exps-stim-t.png\")\n\ngenre = st.radio(\n    \"What's your favorite movie genre\",\n    [\":rainbow[Comedy]\", \"***Drama***\", \"Documentary :movie_camera:\"],\n    captions = [\"Laugh out loud.\", \"Get the popcorn.\", \"Never stop learning.\"], horizontal=True)\n\nif genre == ':rainbow[Comedy]':\n    st.write('You selected comedy.')\nelse:\n    st.write(\"You didn\\'t select comedy.\")\n\n#!streamlit hello\n#!streamlit run app.py\n\n\nhttps://osm-exp.streamlit.app/\n\nDjango vs. Flask: Python Web Frameworks\nDjango and Flask are two prominent web frameworks in Python, but they serve different philosophies. Django, often termed the “framework for perfectionists with deadlines”, is a high-level, all-inclusive framework. It follows the “batteries-included” approach, offering a built-in admin panel, ORM, and directory structure, making it suitable for larger applications and rapid development. In contrast, Flask is a micro-framework. It’s lightweight, flexible, and gives developers more control over components they wish to use. Flask doesn’t prescribe a directory structure or include extras like Django does, making it more suitable for small applications or microservices. Both have their merits, and the choice depends on the project’s needs."
  },
  {
    "objectID": "d3-visualization.html",
    "href": "d3-visualization.html",
    "title": "Day 3: Visualizing and reporting results",
    "section": "",
    "text": "Analyzing data is only half of a data scientist’s work. The other half is about using graphs and diagrams to visualize data in ways that will allow us to report our results as intuitive and compelling stories. On day 3 of the workshop we are going to focus on packages such as matplotlib and seaborn to produce various types of diagrams ranging from simple frequency diagrams to time series and heatmaps."
  },
  {
    "objectID": "d3-visualization.html#frequency-diagrams",
    "href": "d3-visualization.html#frequency-diagrams",
    "title": "Day 3: Visualizing and reporting results",
    "section": "Frequency diagrams",
    "text": "Frequency diagrams\nThe frequency diagram, or histogram, visually represents the distribution of the “sepal_length” variable from the df DataFrame. In the example, we used Seaborn’s histplot function to plot the diagram with 20 bins. The kernel density estimation (KDE) curve is also overlaid on the histogram to give a smoother representation of the data distribution. The x-axis represents the range of “sepal_length” values, while the y-axis shows the frequency of occurrences for each bin.\nHere’s the code snippet to generate the frequency diagram:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create a frequency diagram for 'sepal_length'\nsns.histplot(df['sepal_length'], bins=20, kde=True)\n\n# Add labels and title\nplt.xlabel('Sepal Length')\nplt.ylabel('Frequency')\nplt.title('Frequency Diagram of Sepal Length')\n\n# Show the plot\nplt.show()\n\n\n\n\nThis visualization allows you to quickly grasp the shape, center, and spread of the “sepal_length” data."
  },
  {
    "objectID": "d3-visualization.html#bar-plots",
    "href": "d3-visualization.html#bar-plots",
    "title": "Day 3: Visualizing and reporting results",
    "section": "Bar plots",
    "text": "Bar plots\nBar plots are useful for displaying the relationship between a categorical variable and a numerical variable. In the example, we use Seaborn’s barplot function to visualize the average “sepal_length” for each species in the df DataFrame. The x-axis represents the different species, and the y-axis shows the average “sepal_length” for each.\nHere’s the code snippet to generate the bar plot:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create a barplot for the 'species' column showing the average 'sepal_length'\nsns.barplot(x='species', y='sepal_length', data=df, errorbar=\"ci\")\n\n# Add labels and title\nplt.xlabel('Species')\nplt.ylabel('Average Sepal Length')\nplt.title('Average Sepal Length by Species')\n\n# Show the plot\nplt.show()\n\n\n\n\nIn this case, the ci=None parameter removes the confidence interval bars, focusing solely on the mean values. The plot provides a quick way to compare the average “sepal_length” across different species."
  },
  {
    "objectID": "d3-visualization.html#scatter-plots",
    "href": "d3-visualization.html#scatter-plots",
    "title": "Day 3: Visualizing and reporting results",
    "section": "Scatter plots",
    "text": "Scatter plots\nScatter plots are excellent tools for visualizing relationships between two numerical variables. In the given example, we use Seaborn’s scatterplot function to create a scatter plot of “sepal_length” against “sepal_width” from the df DataFrame. The points are colored based on the “species” category, providing an additional layer of information.\nHere’s the code snippet to generate the scatter plot:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create a scatter plot for 'sepal_length' and 'sepal_width' colored by 'species'\ng = sns.scatterplot(x='sepal_length', y='sepal_width', hue='species', data=df)\n\n# Add labels and title\nplt.xlabel('Sepal Length')\nplt.ylabel('Sepal Width')\nplt.title('Scatter Plot of Sepal Dimensions by Species')\n\n# Show the plot\nplt.show()\n\n\n\n\nThis scatter plot allows you to identify patterns or relationships between “sepal_length” and “sepal_width” while also considering the species. It’s a powerful way to explore multidimensional data."
  },
  {
    "objectID": "d3-visualization.html#heatmaps",
    "href": "d3-visualization.html#heatmaps",
    "title": "Day 3: Visualizing and reporting results",
    "section": "Heatmaps",
    "text": "Heatmaps\nHeatmaps are excellent tools for visualizing complex relationships between numerical variables. In Python, the Seaborn library provides an easy-to-use heatmap function for this purpose. For instance, you can create a heatmap of the correlation matrix of numerical features in the df DataFrame. The color gradients in the heatmap represent the strength and direction of correlation, making it easier to identify highly or weakly correlated variables.\nHere’s an inline code example to generate the heatmap:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Drop the 'species' column to only keep numerical columns\nnumerical_df = df.drop('species', axis=1)\n\n# Calculate the correlation matrix for the numerical columns\ncorrelation_matrix = numerical_df.corr()\n\n# Create a heatmap to visualize the correlation matrix\ng = sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n\n# Add title\nplt.title('Heatmap of Feature Correlations')\n\n# Show the plot\nplt.show()\n\n\n\n\nThis heatmap makes it easier to understand the relationships between different numerical features, aiding in feature selection and further data analysis."
  },
  {
    "objectID": "d3-visualization.html#save-plot-images",
    "href": "d3-visualization.html#save-plot-images",
    "title": "Day 3: Visualizing and reporting results",
    "section": "Save plot images",
    "text": "Save plot images\n\n# write to image file\n#fig.savefig('../../tmp/some.png', format='png', dpi=300)\ng.figure.savefig('../../tmp/some.png', format='png', dpi=300)"
  },
  {
    "objectID": "d3-visualization.html#try-it-yourself",
    "href": "d3-visualization.html#try-it-yourself",
    "title": "Day 3: Visualizing and reporting results",
    "section": "Try it yourself!",
    "text": "Try it yourself!\n\nimport pandas as pd\n\npenguins_df = pd.read_csv('https://raw.githubusercontent.com/nils-holmberg/cca-cce/main/csv/palmerpenguins.tsv', sep='\\t')\n\n\n\n\nFigure 1: palmer penguins\n\n\nTasks:\n\nPlot a bar chart showing the average bill length for each species.\nVisualize the distribution of body mass using a histogram.\nCreate a scatter plot between bill length and bill depth, color-coded by species.\nPlot a pair plot for numerical columns to visualize pairwise relationships, colored by species.\nDisplay a box plot comparing the flipper length distributions of different species.\nCreate a scatter plot of bill length vs. flipper length with a regression line.\nVisualize the distribution of flipper length for each species using violin plots.\nDisplay a heatmap of the correlation matrix for the numerical columns.\nPlot a bar chart of the number of penguins per island.\nCreate a scatter plot of bill depth vs. body mass, color-coded by sex, with a regression line.\n\nThese tasks and their solutions offer students a comprehensive introduction to data visualization in Seaborn using the Palmer Penguins dataset."
  },
  {
    "objectID": "d1-python.html",
    "href": "d1-python.html",
    "title": "Day 1: Setting up a python environment",
    "section": "",
    "text": "There are many ways of setting up a python environment. Day 1 of the workshop we are going to have a look at some of the most common use cases, and discuss pros and cons with different setups depending on the specific research tasks you are trying to solve. We will also have a look at basic python syntax and write some simple python programs. The main format of instruction will be Jupyter notebooks."
  },
  {
    "objectID": "d1-python.html#the-python-interpreter",
    "href": "d1-python.html#the-python-interpreter",
    "title": "Day 1: Setting up a python environment",
    "section": "The python interpreter",
    "text": "The python interpreter\nThe Python interpreter is a tool that allows for the execution of Python code directly. To install, one typically downloads the appropriate version from the official Python website and follows the installation instructions. Once installed, the interpreter can be launched by typing python in the command line or terminal. To open a file on disk, use file = open('filename.txt', 'r'). To run a simple script, save the code to a .py file, like script.py, and execute it with python script.py from the command line. From the Python prompt, scripts can be run using exec(open('script.py').read()). Working with python in this environment can be improved with a good cross-platform code editor, e.g. Visual Studio Code."
  },
  {
    "objectID": "d1-python.html#integrated-development-environments",
    "href": "d1-python.html#integrated-development-environments",
    "title": "Day 1: Setting up a python environment",
    "section": "Integrated development environments",
    "text": "Integrated development environments\nAnaconda is a distribution that simplifies Python and R data science and machine learning on Linux, Windows, and Mac OS X. It bundles a suite of tools, including an IDE called Spyder. To use Python in Spyder, one installs Anaconda3, launches Spyder, and begins coding in its interactive editor. Spyder offers features like variable exploration, integrated IPython console, and a multi-language editor. Compared to a standalone Python interpreter, which offers a basic environment for script execution, Spyder provides a more comprehensive development experience with debugging tools, code completion, and an integrated documentation viewer, streamlining the coding and analysis process. NB! If you are on a LU work computer, you should consult with IT support to istall from Software center."
  },
  {
    "objectID": "d1-python.html#jupyter-notebooks",
    "href": "d1-python.html#jupyter-notebooks",
    "title": "Day 1: Setting up a python environment",
    "section": "Jupyter notebooks",
    "text": "Jupyter notebooks\nJupyter Notebooks offer an interactive environment to write and execute Python code. Accessible via a web browser, users can combine code, text, and visualizations in a single document. To use Python in Jupyter, one installs Jupyter via pip or conda, then launches it, creating or opening notebooks. Each notebook cell can be executed independently, allowing iterative development. Compared to a Python interpreter, Jupyter provides a more visual and structured approach. While Python IDEs offer robust development tools and debugging capabilities, Jupyter excels in data exploration, analysis, and presenting results, making it a favorite among data scientists and researchers. Try jupyter notebooks on Anaconda cloud."
  },
  {
    "objectID": "d1-python.html#python-in-the-cloud",
    "href": "d1-python.html#python-in-the-cloud",
    "title": "Day 1: Setting up a python environment",
    "section": "Python in the cloud",
    "text": "Python in the cloud\nGoogle colab is a cloud-based platform that allows users to write and execute Python code in a web-based environment. To use Python in Google Colab, one simply navigates to the Colab website, starts a new notebook, and begins coding. The platform provides free access to GPUs, making it suitable for machine learning tasks. Unlike local Python installations, there’s no setup required, and notebooks are easily shareable. However, while Colab offers the convenience of working in the cloud, using Python locally provides more control over the environment, dependencies, and data storage. In contrast, Colab sessions are ephemeral, and prolonged inactivity can lead to disconnection."
  },
  {
    "objectID": "d1-python.html#basic-python-syntax",
    "href": "d1-python.html#basic-python-syntax",
    "title": "Day 1: Setting up a python environment",
    "section": "Basic python syntax",
    "text": "Basic python syntax\n\nPython Data Types\nPython, a versatile and powerful programming language, boasts several built-in data types. Among the most fundamental are:\n\nIntegers (int): Whole numbers like 3 or -11.\nFloating Point (float): Numbers with a decimal point, e.g., 3.14.\nStrings (str): Sequences of characters, like “Hello”.\nLists: Ordered collections, e.g., [1, 2, “a”].\nTuples: Immutable ordered collections, e.g., (1, 2, “b”).\nDictionaries (dict): Key-value pairs, e.g., {“name”: “John”, “age”: 30}.\n\nEach type has its unique properties and methods, catering to diverse programming needs. Now, here’s a code example showcasing these data types:\n\n# Integer\nmy_int = 5\nprint(\"Integer:\", my_int)\n\n# Float\nmy_float = 5.5\nprint(\"Float:\", my_float)\n\n# String\nmy_string = \"Hello, World!\"\nprint(\"String:\", my_string)\n\n# List\nmy_list = [1, 2, \"three\", 4.0]\nprint(\"List:\", my_list)\n\n# Tuple\nmy_tuple = (1, \"two\", 3.0)\nprint(\"Tuple:\", my_tuple)\n\n# Dictionary\nmy_dict = {\"first_name\": \"John\", \"last_name\": \"Doe\", \"age\": 30}\nprint(\"Dictionary:\", my_dict)\n\n\n\nPython Control Statements\nHere’s a concise explanation of Python’s most important control statements, followed by a code example. Control statements in Python determine the flow of execution in a program. The primary ones include:\n\nIf Statement: Tests a condition and executes a block of code if true.\nElif and Else: Supplements the if statement, allowing for multiple conditions or a default action.\nFor Loop: Iterates over sequences like lists or strings.\nWhile Loop: Continues execution as long as a condition remains true.\nBreak: Exits the current loop.\nContinue: Skips the rest of the current loop iteration, moving to the next one.\n\nThese tools provide the foundation for complex decision-making and repetitive tasks in Python programs. Here’s a code example showcasing these control statements:\n\n# If, Elif, and Else statements\nx = 10\nif x &gt; 5:\n    print(\"x is greater than 5\")\nelif x == 5:\n    print(\"x is equal to 5\")\nelse:\n    print(\"x is less than 5\")\n\n# For Loop\nfor i in range(3):\n    print(f\"For Loop iteration {i}\")\n\n# While Loop\ncount = 0\nwhile count &lt; 3:\n    print(f\"While Loop iteration {count}\")\n    count += 1\n\n# Break and Continue\nfor num in range(5):\n    if num == 2:\n        break\n    elif num == 1:\n        continue\n    print(f\"Value after applying break and continue: {num}\")\n\n\n\nPython Functions\nPython functions are modular blocks of code designed to perform a specific task. Their key aspects include:\n\nDefinition: Using the def keyword, functions are declared, followed by their name and parameters.\nPositional Arguments: Values passed based on their position in the function call.\nNamed (or Keyword) Arguments: Values passed by explicitly naming the parameter.\nReturn Statement: Outputs a value from the function.\nDefault Values: Assigns a default value to a parameter if no argument is provided.\nVariable-Length Arguments: *args and **kwargs allow for arbitrary numbers of positional and keyword arguments, respectively.\n\nFunctions enhance code reusability and structure, making complex programs more manageable. Here’s a code example showcasing a function with named and positional arguments:\n\ndef example_function(positional_arg, named_arg=\"default\", *args, **kwargs):\n    print(f\"Positional Argument: {positional_arg}\")\n    print(f\"Named Argument: {named_arg}\")\n    for index, value in enumerate(args):\n        print(f\"Additional positional argument {index}: {value}\")\n    for key, value in kwargs.items():\n        print(f\"Keyword argument {key}: {value}\")\n    return [positional_arg, named_arg, args, kwargs]\n\nresult = example_function(1, \"test\", 2, 3, 4, key1=\"value1\", key2=\"value2\")\nprint(\"\\nReturned Value:\", result)\n\n\n\nSome python tasks\nHere are 10 tasks to test students’ understanding of Python data types, control statements, and functions:\n# Task 1: Change the string to output \"Hello, Python!\".\nstring_task = \"Hello, World!\"\nprint(string_task)\n\n# Task 2: Add a floating point number to the integer to get a result of 15.5.\ninteger_task = 10\nprint(integer_task)\n\n# Task 3: Update the list to have a fifth element \"apple\".\nlist_task = [1, 2, 3, 4]\nprint(list_task)\n\n# Task 4: Use a control statement to print \"Positive\" if number_task is greater than zero.\nnumber_task = 5\n\n# Task 5: Modify the function to return the product of a and b.\ndef function_task(a, b):\n    return a + b\nprint(function_task(3, 4))\n\n# Task 6: Call the function above with named arguments in reverse order (b first, then a).\nprint(function_task(a=1, b=2))\n\n# Task 7: Modify the loop to print numbers from 1 to 5.\nfor i in range(3):\n    print(i)\n\n# Task 8: Use a control statement in the function below to return \"Even\" for even numbers and \"Odd\" for odd numbers.\ndef even_odd(num):\n    return \"Number\"\nprint(even_odd(3))\n\n# Task 9: Add a default value to the function parameter so that calling function_default() returns \"Hello, World!\".\ndef function_default(message):\n    return message\nprint(function_default())\n\n# Task 10: Correct the function to return the sum of all numbers in the list, use a control statement.\ndef sum_function(numbers):\n    return 0\nprint(sum_function([1, 2, 3, 4, 5]))\nFor each task, students should modify the Python code to achieve the described goal.\n\n\nObject-Oriented Programming in Python\nObject-Oriented Programming (OOP) in Python revolves around the concept of objects, which represent both data and functions. Key aspects include:\n\nClasses: Blueprints for creating objects.\nObjects: Instances of classes.\nAttributes: Variables associated with a class.\nMethods: Functions defined within a class.\nInheritance: Allows a class to inherit attributes and methods from another class.\nEncapsulation: Bundling data and methods into a single unit (class).\nPolymorphism: Using a single interface for different data types.\n\nOOP in Python facilitates organizing complex programs by modeling real-world entities and their interactions. Here’s a code example showcasing a Python class with named arguments:\n\nclass SampleClass:\n    def __init__(self, arg1, arg2=\"default\"):\n        self.arg1 = arg1\n        self.arg2 = arg2\n\n    def get_values(self):\n        return [self.arg1, self.arg2]\n\n# Instantiating the class with named arguments\nobj = SampleClass(arg1=\"value1\", arg2=\"value2\")\nresult = obj.get_values()\nprint(result)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Transparency, replicability, collaboration, and open source are four pivotal pillars of open science. Transparency ensures that research processes and findings are openly shared, fostering trust and accountability. Replicability emphasizes the importance of detailed documentation, enabling others to reproduce experiments and validate results, thereby enhancing the reliability of scientific discoveries. Collaboration encourages interdisciplinary and international teamwork, harnessing diverse expertise for more comprehensive insights."
  },
  {
    "objectID": "about.html#open-science-methods",
    "href": "about.html#open-science-methods",
    "title": "About",
    "section": "",
    "text": "Transparency, replicability, collaboration, and open source are four pivotal pillars of open science. Transparency ensures that research processes and findings are openly shared, fostering trust and accountability. Replicability emphasizes the importance of detailed documentation, enabling others to reproduce experiments and validate results, thereby enhancing the reliability of scientific discoveries. Collaboration encourages interdisciplinary and international teamwork, harnessing diverse expertise for more comprehensive insights."
  },
  {
    "objectID": "about.html#python-workshops",
    "href": "about.html#python-workshops",
    "title": "About",
    "section": "Python workshops",
    "text": "Python workshops\nOpen source practices promote the development and sharing of accessible research tools and software, making science more accessible and cost-effective. Together, these principles redefine how science is conducted, making it more rigorous, inclusive, and efficient, ultimately benefiting both researchers and society at large."
  },
  {
    "objectID": "about.html#workshop-instructor",
    "href": "about.html#workshop-instructor",
    "title": "About",
    "section": "Workshop instructor",
    "text": "Workshop instructor\nNils Holmberg"
  },
  {
    "objectID": "d2-analysis.html",
    "href": "d2-analysis.html",
    "title": "Day 2: Getting started with data analysis",
    "section": "",
    "text": "One of the most common use cases for python is data analysis. On day 2 of the workshop we are going to focus on a package called pandas. This package will allow us to import structured data such as excel files and other quantitative tabular data, and transform and summarize such data. We will also introduce scikit-learn for statistical analysis. Participants are invited to bring their own data files."
  },
  {
    "objectID": "d2-analysis.html#import-data-files",
    "href": "d2-analysis.html#import-data-files",
    "title": "Day 2: Getting started with data analysis",
    "section": "Import data files",
    "text": "Import data files\nTo read a CSV file into a Pandas DataFrame, you’ll first need to install the Pandas library if you haven’t already. You can install it using pip with the command pip install pandas. Once installed, you can use the read_csv() function to load the data from the file into a DataFrame. The function takes the file path as an argument and returns a DataFrame containing the data. Here’s how you can read the uploaded file, “iris.csv”, into a Pandas DataFrame:\nimport pandas as pd\n\n# Read the CSV file into a DataFrame\ndf = pd.read_csv('/path/to/your/iris.csv')\n\n# Display the first few rows of the DataFrame\nprint(df.head())\nReplace '/path/to/your/iris.csv' with the actual path where your file is located. This will give you a DataFrame df that contains all the data from “iris.csv”."
  },
  {
    "objectID": "d2-analysis.html#selecting-and-filtering",
    "href": "d2-analysis.html#selecting-and-filtering",
    "title": "Day 2: Getting started with data analysis",
    "section": "Selecting and filtering",
    "text": "Selecting and filtering\nIn Pandas, two basic yet powerful operations are selecting specific columns and filtering rows. To select columns, you can use the syntax df[['column1', 'column2']], which creates a new DataFrame containing only the selected columns. For example:\n\n# To select 'sepal_length' and 'species' columns\nselected_columns = df[['sepal_length', 'species']]\n# Show the first few rows of the resulting DataFrame\nselected_columns.head()\n\n\n\n\n\n\n\n\nsepal_length\nspecies\n\n\n\n\n0\n5.1\nsetosa\n\n\n1\n4.9\nsetosa\n\n\n2\n4.7\nsetosa\n\n\n3\n4.6\nsetosa\n\n\n4\n5.0\nsetosa\n\n\n\n\n\n\n\nTo filter rows based on a condition, boolean indexing can be employed. The syntax df[df['column'] &gt; value] filters rows where the values in the specified column meet the condition. For instance:\n\n# To filter rows where 'sepal_length' is greater than 5\nfiltered_rows = df[df['sepal_length'] &gt; 5]\n# Show the first few rows of the resulting DataFrame\nfiltered_rows.head()\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n5\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n10\n5.4\n3.7\n1.5\n0.2\nsetosa\n\n\n14\n5.8\n4.0\n1.2\n0.2\nsetosa\n\n\n15\n5.7\n4.4\n1.5\n0.4\nsetosa\n\n\n\n\n\n\n\nBoth operations return new DataFrames, which can then be used for further analysis."
  },
  {
    "objectID": "d2-analysis.html#grouping-and-summarizing",
    "href": "d2-analysis.html#grouping-and-summarizing",
    "title": "Day 2: Getting started with data analysis",
    "section": "Grouping and summarizing",
    "text": "Grouping and summarizing\nGrouping and summarizing data in Pandas is primarily achieved using the groupby() function. This function allows you to group rows based on one or multiple columns, and then you can apply aggregation methods like mean(), sum(), or count() to summarize the data. For instance, if you want to find the average measurements for each species in the df DataFrame, you can group by the ‘species’ column and then apply the mean() function to get the average for each numerical column.\nHere’s an inline code example:\n\n# Group by 'species' and calculate the mean for each numerical column\ngrouped_by_species_mean = df.groupby('species').mean()\n# Show the first few rows of the resulting DataFrame\ngrouped_by_species_mean.head()\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\nspecies\n\n\n\n\n\n\n\n\nsetosa\n5.006\n3.428\n1.462\n0.246\n\n\nversicolor\n5.936\n2.770\n4.260\n1.326\n\n\nvirginica\n6.588\n2.974\n5.552\n2.026\n\n\n\n\n\n\n\nThis will give you a new DataFrame that contains the summarized data, facilitating easier comparisons between different groups.\nLet’s try another example. Reading the Iris Dataset and Analyzing Sepal Length with Pandas. The Iris dataset, a foundational dataset in data science, comprises measurements of sepals and petals for three iris species. Using the Pandas library in Python, one can effortlessly read and analyze this dataset. To read the dataset into a DataFrame, utilize pd.read_csv() if you have a CSV file. With the DataFrame loaded, one can employ the groupby() method to group by species, and then use the mean() and std() functions to compute the mean and standard deviation of the sepal_length for each species. Here’s a code example that demonstrates the process:\n\nimport pandas as pd\n\n# Sample data mimicking the Iris dataset structure\ndata = {\n    'sepal_length': [5.1, 4.9, 5.8, 6.4, 5.7],\n    'sepal_width': [3.5, 3.0, 2.7, 3.2, 3.0],\n    'species': ['setosa', 'setosa', 'virginica', 'virginica', 'versicolor']\n}\ndf_data = pd.DataFrame(data)\n\n# Group by species and calculate mean and standard deviation for sepal_length\nmeans = df_data.groupby('species')['sepal_length'].mean()\nstd_devs = df_data.groupby('species')['sepal_length'].std()\n# Calculate both mean and std for sepal_length grouped by species in one line\nstats = df_data.groupby('species')['sepal_length'].agg(['mean', 'std'])\n\nprint(\"Mean Sepal Length by Species:\")\nprint(means)\nprint(\"\\nStandard Deviation of Sepal Length by Species:\")\nprint(std_devs)\n\nMean Sepal Length by Species:\nspecies\nsetosa        5.0\nversicolor    5.7\nvirginica     6.1\nName: sepal_length, dtype: float64\n\nStandard Deviation of Sepal Length by Species:\nspecies\nsetosa        0.141421\nversicolor         NaN\nvirginica     0.424264\nName: sepal_length, dtype: float64\n\n\nExecuting the above code will yield the mean and standard deviation of sepal_length for each species in the sample data."
  },
  {
    "objectID": "d2-analysis.html#statistical-analysis",
    "href": "d2-analysis.html#statistical-analysis",
    "title": "Day 2: Getting started with data analysis",
    "section": "Statistical analysis",
    "text": "Statistical analysis\nRegression analysis is used to explore the relationship between dependent and independent variables. In Python, Scikit-learn is a popular library for performing regression. You typically use the LinearRegression class to create a regression model. After separating your features and target variables, you can fit the model using the fit() method and make predictions with predict(). For example, if you want to predict ‘petal_length’ based on ‘sepal_length’ in the df DataFrame:\n\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\n\n# Prepare the features and target variable\nX = df[['sepal_length']]  # Feature (independent variable)\ny = df['petal_length']  # Target (dependent variable)\n\n# Create a LinearRegression object\nmodel = LinearRegression()\n\n# Fit the model to the data\nmodel.fit(X, y)\n\n# Make predictions\npredictions = model.predict(X)\n\n# Create a DataFrame to display the analysis result\nresult_df = pd.DataFrame({'Actual': y, 'Predicted': predictions})\n# Show the first few rows of the result DataFrame\nresult_df.head()\n\n\n\n\n\n\n\n\nActual\nPredicted\n\n\n\n\n0\n1.4\n2.376565\n\n\n1\n1.4\n2.004878\n\n\n2\n1.3\n1.633192\n\n\n3\n1.5\n1.447348\n\n\n4\n1.4\n2.190722\n\n\n\n\n\n\n\nThis will create a DataFrame result_df that contains both the actual and predicted ‘petal_length’, facilitating the evaluation of the model’s performance."
  },
  {
    "objectID": "d2-analysis.html#write-data-files",
    "href": "d2-analysis.html#write-data-files",
    "title": "Day 2: Getting started with data analysis",
    "section": "Write data files",
    "text": "Write data files\n\n# write to text file\ndf.to_csv(\"../../tmp/some.tsv\", sep='\\t', index=False)"
  },
  {
    "objectID": "d2-analysis.html#try-it-yourself",
    "href": "d2-analysis.html#try-it-yourself",
    "title": "Day 2: Getting started with data analysis",
    "section": "Try it yourself!",
    "text": "Try it yourself!\n\nimport pandas as pd\n\ndf = pd.read_csv('https://raw.githubusercontent.com/nils-holmberg/cca-cce/main/csv/palmerpenguins.tsv', sep='\\t')\n\n\n\n\nFigure 1: palmer penguins\n\n\nTasks:\n\nDisplay the first 5 rows of the dataset to get a quick overview.\nShow the summary statistics (mean, standard deviation, min, max, etc.) for numerical columns.\nDetermine the number of unique species in the dataset.\nFilter the dataset to show only Adelie penguins from Torgersen island.\nCalculate the average bill length of male penguins across all species.\nFind out the year with the highest recorded average body mass for penguins.\nDetermine the number of missing values in each column.\nDisplay all records for penguins with a flipper length greater than 210 mm.\nGroup the data by species and calculate the average bill depth for each group.\nCount the number of penguins on each island.\n\nThese tasks and their solutions offer a comprehensive introduction to basic data analysis functionalities in Pandas."
  },
  {
    "objectID": "d2-analysis.html#some-more-analysis",
    "href": "d2-analysis.html#some-more-analysis",
    "title": "Day 2: Getting started with data analysis",
    "section": "Some more analysis",
    "text": "Some more analysis\nSample Data from Iris Dataset:\nimport pandas as pd\n\n# Sample data mimicking a portion of the Iris dataset structure\ndata = {\n    'sepal_length': [5.1, 4.9, 5.8, 6.4, 5.7],\n    'sepal_width': [3.5, 3.0, 2.7, 3.2, 3.0],\n    'species': ['setosa', 'setosa', 'virginica', 'virginica', 'versicolor']\n}\niris_df = pd.DataFrame(data)\n\nSelecting and Filtering Data using loc, iloc, and dot notation:\n\nDot Notation: Access a column directly by its name.\nsepal_lengths = iris_df.sepal_length\nUsing loc: Select and filter data based on labels.\n\nSelect all rows and the sepal_length column:\nsepal_lengths_loc = iris_df.loc[:, 'sepal_length']\nFilter rows where species is setosa:\nsetosa_rows = iris_df.loc[iris_df.species == 'setosa']\n\nUsing iloc: Select and filter data based on integer index positions.\n\nSelect the first row and first two columns:\nfirst_row = iris_df.iloc[0, :2]\nSelect the first three rows and all columns:\nfirst_three_rows = iris_df.iloc[:3, :]\n\nSimultaneously Select and Filter:\n\nSelect sepal_width for rows where species is virginica:\nvirginica_sepal_width = iris_df.loc[iris_df.species == 'virginica', 'sepal_width']\n\n\nLet’s execute these operations and display the results."
  },
  {
    "objectID": "d4-language.html",
    "href": "d4-language.html",
    "title": "Day 4: Processing natural language texts",
    "section": "",
    "text": "The opposite of structured data is natural language texts. However, as social scientists, we often encounter such unstructured text data. The challenge of managing and analyzing natural language data will be the main focus on day 4. To our help we will utilize both classical packages for data cleaning and tokenization such as NLTK, and more modern packages such as spaCy that relies on machine learning models to infer syntactic function and named entities."
  },
  {
    "objectID": "d4-language.html#tokenization",
    "href": "d4-language.html#tokenization",
    "title": "Day 4: Processing natural language texts",
    "section": "Tokenization",
    "text": "Tokenization\nFirst we attempt to install needed packages, and their associated language data. These downloads may require quite a lot of free disk space!\n\n!pip install -q nltk spacy\n!python -m nltk.downloader popular\n!python -m spacy download en_core_web_sm\n\n# downloads to ~/nltk_data/\n#import nltk; nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nstop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n\nLet’s assume the following text data:\n\nimport pandas as pd\n\n# Read the uploaded TSV file into a Pandas DataFrame named 'df'\ndf = pd.read_csv('https://raw.githubusercontent.com/nils-holmberg/socs-qmd/main/txt/zen_of_python.tsv', sep='\\t')\n\n# Display the first few rows of the DataFrame\ndf.head()\n\n\n\n\n\n\n\n\nid\ntext\n\n\n\n\n0\n1\nBeautiful is better than ugly.\n\n\n1\n2\nExplicit is better than implicit.\n\n\n2\n3\nSimple is better than complex.\n\n\n3\n4\nComplex is better than complicated.\n\n\n4\n5\nFlat is better than nested.\n\n\n\n\n\n\n\nTo tokenize the text data using the Natural Language Toolkit (NLTK) package, you can follow these steps:\n\nFirst, import the necessary NLTK library: from nltk.tokenize import word_tokenize.\nCreate an empty DataFrame to store the tokenized words along with their corresponding ‘id’ from the original text.\nLoop through each row of the original DataFrame (df), tokenize the text in the ‘text’ column using word_tokenize(), and append the tokens along with their ‘id’ to the new DataFrame.\n\nHere’s an inline code example:\n\nimport pandas as pd\nfrom nltk.tokenize import word_tokenize\n\n# Create an empty DataFrame to store tokens and ids\ntokens_df = pd.DataFrame(columns=['id', 'token'])\n\n# Loop through each row in the original DataFrame\nfor index, row in df.iterrows():\n    id_value = row['id']\n    text_value = row['text']\n    \n    # Tokenize the text\n    tokens = word_tokenize(text_value)\n    \n    # Create a temporary DataFrame to hold tokens and ids\n    temp_df = pd.DataFrame({'id': [id_value]*len(tokens), 'token': tokens})\n    \n    # Append to the main DataFrame\n    tokens_df = pd.concat([tokens_df, temp_df], ignore_index=True)\n\n# Show the first few rows of the resulting DataFrame\ntokens_df.head()\n\n\n\n\n\n\n\n\nid\ntoken\n\n\n\n\n0\n1\nBeautiful\n\n\n1\n1\nis\n\n\n2\n1\nbetter\n\n\n3\n1\nthan\n\n\n4\n1\nugly\n\n\n\n\n\n\n\nRunning this code will create a new DataFrame tokens_df that contains one token per row, along with the original ‘id’ to associate each token with its originating text."
  },
  {
    "objectID": "d4-language.html#matrix-representation",
    "href": "d4-language.html#matrix-representation",
    "title": "Day 4: Processing natural language texts",
    "section": "Matrix representation",
    "text": "Matrix representation\nTurning unstructured language data into structured tables Figure 1\n\n\n\nFigure 1: matrix representation\n\n\n\n!pip install -q scikit-learn\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.decomposition import LatentDirichletAllocation as LDA\n#from nltk.corpus import stopwords\n\nConsider the following corpus composed of five short sentences (all taken from New York Times headlines). The algorithm should clearly identify one topic related to politics and coronavirus, and a second one related to Nadal and tennis.\n\ncorpus = [\"Rafael Nadal Joins Roger Federer in Missing U.S. Open\",\n          \"Rafael Nadal Is Out of the Australian Open\",\n          \"Biden Announces Virus Measures\",\n          \"Biden's Virus Plans Meet Reality\",\n          \"Where Biden's Virus Plan Stands\"]\n\nUsing CountVectorizer(), we generate the matrix that denotes the frequency of the words of each text using CountVectorizer(). Note that the CountVectorizer allows for preprocessing if you include parameters such as stop_words to include the stop words, ngram_range to include n-grams, or lowercase=True to convert all characters to lowercase.\n\nfrom nltk.corpus import stopwords\n\ncount_vect = CountVectorizer(stop_words=stopwords.words('english'), lowercase=True)\nx_counts = count_vect.fit_transform(corpus)\nx_counts.todense()\n#count_vect.get_feature_names_out()\n\nmatrix([[0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0],\n        [0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0],\n        [1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n        [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1],\n        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1]])\n\n\nTerm frequency–inverse document frequency (tf–idf). Use the coefficient of tf–idf instead of noting the frequency of each word within each cell of the matrix. It consists of two numbers, multiplied:\n\ntf: the frequency of a given term or word in a text, and\nidf: the logarithm of the total number of documents divided by the number of documents that contain that given term.\n\ntf-idf is a measure of how frequently a word is used in the corpus. To be able to subdivide words into groups, it is important to understand not only which words appear in each text, but also which words appear frequently in one text but not at all in others.\n\ntfidf_transformer = TfidfTransformer()\nx_tfidf = tfidf_transformer.fit_transform(x_counts)\nx_tfidf\n\n&lt;5x17 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 24 stored elements in Compressed Sparse Row format&gt;"
  },
  {
    "objectID": "d4-language.html#parts-of-speech",
    "href": "d4-language.html#parts-of-speech",
    "title": "Day 4: Processing natural language texts",
    "section": "Parts of speech",
    "text": "Parts of speech\nSpacy is a prominent Python library for natural language processing. To analyze the Zen of Python with Spacy, one must first install the package and its English model. After loading the model, the Zen text can be processed to tokenize it. For a visual syntactic analysis of the first sentence, Spacy’s displacy module can be employed.\n\nimport spacy\nfrom spacy import displacy\n\n# Open the file in read mode\nwith open('../../txt/zen_of_python.txt', 'r') as file:\n    zen_text = file.read()\n\n# Load the English model\nnlp = spacy.load('en_core_web_sm')\n#nlp._path\n\n# Process the Zen of Python text\ndoc = nlp(zen_text)\n\n# Visualize the syntactic structure of the first sentence\ndisplacy.render(list(doc.sents)[0], style='dep', jupyter=True)\n\n\n\n    Beautiful\n    PROPN\n\n\n\n    is\n    AUX\n\n\n\n    better\n    ADJ\n\n\n\n    than\n    ADP\n\n\n\n    ugly.\n    PUNCT\n\n\n\n    \n\n    SPACE\n\n\n\n    \n    \n        nsubj\n    \n    \n\n\n\n    \n    \n        acomp\n    \n    \n\n\n\n    \n    \n        prep\n    \n    \n\n\n\n    \n    \n        punct\n    \n    \n\n\n\n    \n    \n        dep\n    \n    \n\n\n\n\nThis code provides a graphical representation of the sentence’s grammatical relationships."
  },
  {
    "objectID": "d4-language.html#named-entities",
    "href": "d4-language.html#named-entities",
    "title": "Day 4: Processing natural language texts",
    "section": "Named entities",
    "text": "Named entities\n\ndoc = nlp(\"Apple is looking at buying a Hong Kong startup for $1 billion\")\nfor token in doc:\n    print(token.text)\n\nApple\nis\nlooking\nat\nbuying\na\nHong\nKong\nstartup\nfor\n$\n1\nbillion\n\n\nTo learn more about entity recognition in spaCy, how to add your own entities to a document and how to train and update the entity predictions of a model, see the usage guides on named entity recognition and training pipelines.\nA named entity is a “real-world object” that’s assigned a name – for example, a person, a country, a product or a book title. spaCy can recognize various types of named entities in a document, by asking the model for a prediction. Because models are statistical and strongly depend on the examples they were trained on, this doesn’t always work perfectly and might need some tuning later, depending on your use case.\nNamed entities are available as the ents property of a Doc:\n\n\nText: The original entity text.\nStart: Index of start of entity in the Doc.\nEnd: Index of end of entity in the Doc.\nLabel: Entity label, i.e. type.\n\n\n\n\n\n\n\n\n\n\n\n\nText\nStart\nEnd\nLabel\nDescription\n\n\n\n\nApple\n0\n5\nORG\nCompanies, agencies, institutions.\n\n\nU.K.\n27\n31\nGPE\nGeopolitical entity, i.e. countries, cities, states.\n\n\n$1 billion\n44\n54\nMONEY\nMonetary values, including unit.\n\n\n\nUsing spaCy’s built-in displaCy visualizer, here’s what our example sentence and its named entities look like:\n\nfor ent in doc.ents:\n    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n\nApple 0 5 ORG\nHong Kong 29 38 GPE\n$1 billion 51 61 MONEY\n\n\n\ntext = \"\"\"Apple decided to fire Tim Cook and hire somebody called John Doe as the new CEO.\nThey also discussed a merger with Google. On the long run it seems more likely that Apple\nwill merge with Amazon and Microsoft with Google. The companies will all relocate to\nAustin in Texas before the end of the century. John Doe bought a car.\"\"\"\n\ndoc = nlp(text)\ndisplacy.render(doc, style='ent', jupyter=True)\n\n\n\n    Apple\n    ORG\n\n decided to fire \n\n    Tim Cook\n    PERSON\n\n and hire somebody called \n\n    John Doe\n    PERSON\n\n as the new CEO.They also discussed a merger with \n\n    Google\n    ORG\n\n. On the long run it seems more likely that \n\n    Apple\n    ORG\n\nwill merge with \n\n    Amazon\n    ORG\n\n and \n\n    Microsoft\n    ORG\n\n with \n\n    Google\n    ORG\n\n. The companies will all relocate to\n\n    Austin\n    GPE\n\n in \n\n    Texas\n    GPE\n\n before \n\n    the end of the century\n    DATE\n\n. \n\n    John Doe\n    PERSON\n\n bought a car."
  },
  {
    "objectID": "d4-language.html#topic-modeling",
    "href": "d4-language.html#topic-modeling",
    "title": "Day 4: Processing natural language texts",
    "section": "Topic modeling",
    "text": "Topic modeling\nTo cluster our corpus, we can choose from several algorithms, including non-negative matrix factorization (NMF), sparse principal components analysis (sparse PCA), and latent dirichlet allocation (LDA). We’ll focus on LDA because it is widely used by the scientific community due to its good results in social media, medical science, political science, and software engineering.\nLDA is a model for unsupervised topic decomposition: It groups texts based on the words they contain and the probability of a word belonging to a certain topic. The LDA algorithm outputs the topic word distribution. With this information, we can define the main topics based on the words that are most likely associated with them. Once we have identified the main topics and their associated words, we can know which topic or topics apply to each text.\n\ntfidf_transformer = TfidfTransformer()\nx_tfidf = tfidf_transformer.fit_transform(x_counts)\n\nIn order to perform the LDA decomposition, we have to define the number of topics. In this simple case, we know there are two topics or “dimensions.” But in general cases, this is a hyperparameter that needs some tuning, which could be done using algorithms like random search or grid search:\n\ndimension = 2\nlda = LDA(n_components = dimension)\nlda_array = lda.fit_transform(x_tfidf)\nlda_array\n\narray([[0.85161955, 0.14838045],\n       [0.8235949 , 0.1764051 ],\n       [0.18073116, 0.81926884],\n       [0.16954463, 0.83045537],\n       [0.18072698, 0.81927302]])\n\n\nLDA is a probabilistic method. Here we can see the probability of each of the five headlines belonging to each of the two topics. We can see that the first two texts have a higher probability of belonging to the first topic and the next three to the second topic, as expected.\nFinally, if we want to understand what these two topics are about, we can see the most important words in each topic:\n\ncomponents = [lda.components_[i] for i in range(len(lda.components_))]\n#features = count_vect.get_feature_names()\n#features = count_vect.get_feature_names_out()\n#important_words = [sorted(features, key = lambda x: components[j][features.index(x)], reverse = True)[:3] for j in range(len(components))]\n#important_words\n\n[['virus', 'biden', 'plan'], ['open', 'nadal', 'rafael']]\nAs expected, LDA correctly assigned words related to tennis tournaments and Nadal to the first topic and words related to Biden and virus to the second topic."
  },
  {
    "objectID": "d4-language.html#try-it-yourself",
    "href": "d4-language.html#try-it-yourself",
    "title": "Day 4: Processing natural language texts",
    "section": "Try it yourself!",
    "text": "Try it yourself!\nHere are 10 tasks focusing on cleaning language data using the NLTK package:\nTasks:\n\nTokenize the given text into words.\nConvert all the tokens into lowercase.\nRemove any punctuation from the tokenized words.\nUse stemming to reduce the words to their root form.\nLemmatize the words to obtain their base or dictionary form.\nRemove English stopwords from the tokenized list.\nTokenize the given text into sentences.\nCount the frequency of each word in the tokenized list.\nFind the bigrams (two consecutive words) in the tokenized list.\nIdentify the parts of speech (POS) for each token.\n\nNote: Before executing the code, ensure you have the required NLTK data downloaded (e.g., tokenizers, stopwords, and the averaged_perceptron_tagger for POS tagging). You can use nltk.download('package_name') to download the necessary datasets."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Python",
    "section": "",
    "text": "The Python programming language (logo: Figure 1) has gained popularity in all types of data science in recent years. On this introductory workshop, we are aiming at getting acquainted with the basic syntax of Python, as well as learning how to extend this basic functionality by calling powerful modules built by members of the large Python user community. Thus, we take a “standing on the shoulders of giants” approach to Python!\n\n\n\nFigure 1: Official logo of the Python programming language\n\n\nAnother principle that has guided the design of this course is that participants should gain an understanding of how Python can be used to accomplish fairly common research tasks within the social sciences more effectively and more transparently. Such applications of Python include collecting and analyzing survey data, performing content analyses on textual and visual material, and using charts to visualize results. (workshop syllabus)\n\nThis is a Quarto website. To learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "index.html#social-science-methods-workshops-2023-lund-university",
    "href": "index.html#social-science-methods-workshops-2023-lund-university",
    "title": "Introduction to Python",
    "section": "",
    "text": "The Python programming language (logo: Figure 1) has gained popularity in all types of data science in recent years. On this introductory workshop, we are aiming at getting acquainted with the basic syntax of Python, as well as learning how to extend this basic functionality by calling powerful modules built by members of the large Python user community. Thus, we take a “standing on the shoulders of giants” approach to Python!\n\n\n\nFigure 1: Official logo of the Python programming language\n\n\nAnother principle that has guided the design of this course is that participants should gain an understanding of how Python can be used to accomplish fairly common research tasks within the social sciences more effectively and more transparently. Such applications of Python include collecting and analyzing survey data, performing content analyses on textual and visual material, and using charts to visualize results. (workshop syllabus)\n\nThis is a Quarto website. To learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "resources.html#articles",
    "href": "resources.html#articles",
    "title": "Resources",
    "section": "Articles",
    "text": "Articles"
  },
  {
    "objectID": "resources.html#websites",
    "href": "resources.html#websites",
    "title": "Resources",
    "section": "Websites",
    "text": "Websites"
  },
  {
    "objectID": "resources.html#notebooks",
    "href": "resources.html#notebooks",
    "title": "Resources",
    "section": "Notebooks",
    "text": "Notebooks"
  }
]