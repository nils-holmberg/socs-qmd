{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nils-holmberg/socs-qmd/blob/main/jnb/lab2_nlp2.ipynb)"
      ],
      "metadata": {
        "id": "t1__gpfMIXXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# running pyldavis in google colab env\n",
        "!pip install --upgrade pandas\n",
        "# gensim topic modeling plotting tools\n",
        "!pip install -q pyLDAvis\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "metadata": {
        "id": "z91vhlDa_COM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# load text data"
      ],
      "metadata": {
        "id": "p49C3yo5FWzq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82vt42saD2US"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install gdown\n",
        "!gdown https://drive.google.com/uc?id=1EMzJxxoBaN_NbvF7xhoc09K82vQ6H_LX"
      ],
      "metadata": {
        "id": "gr6BFu3fEGEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp = \"content.xlsx\"\n",
        "df = pd.read_excel(fp, header=None, names=['id', 'image', 'text'])\n",
        "df.head()"
      ],
      "metadata": {
        "id": "gyQtBs3qELJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "k7CKBfzuEOte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to list\n",
        "corpus = df.text.values.tolist()\n",
        "corpus[:2]"
      ],
      "metadata": {
        "id": "RO9btCpVsT9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\"Rafael Nadal Joins Roger Federer in Missing U.S. Open\",\n",
        "          \"Rafael Nadal Is Out of the Australian Open\",\n",
        "          \"Biden Announces Virus Measures\",\n",
        "          \"Biden's Virus Plans Meet Reality\",\n",
        "          \"Where Biden's Virus Plan Stands\"]"
      ],
      "metadata": {
        "id": "YV_hu3aUr373"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK data\n",
        "import nltk\n",
        "nltk.download('popular')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import string"
      ],
      "metadata": {
        "id": "wRoN317WHWWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define data cleaning function\n",
        "stop = set(stopwords.words('english'))\n",
        "exclude = set(string.punctuation)\n",
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "def clean(doc):\n",
        "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
        "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
        "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
        "    return normalized\n",
        "\n",
        "# clean data stored in a new list\n",
        "clean_corpus = [clean(doc).split() for doc in corpus]"
      ],
      "metadata": {
        "id": "m83WJ5idtIKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_corpus"
      ],
      "metadata": {
        "id": "kqg5u4vxtWiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# document term matrix (gensim approach)\n",
        "\n",
        "![](https://raw.githubusercontent.com/nils-holmberg/socs-qmd/main/res/img/nlp-image_0-259d7a671398a16dc7cdfe05d89d4880.png)\n"
      ],
      "metadata": {
        "id": "LhxrZBC4t8sx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Gensim\n",
        "import gensim\n",
        "from gensim import corpora"
      ],
      "metadata": {
        "id": "XLjPu5lUt8Gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the term dictionary of our courpus that is of all the words (Sepcific to Genism syntax perspective),\n",
        "# where every unique term is assigned an index.\n",
        "\n",
        "dict_ = corpora.Dictionary(clean_corpus)\n",
        "\n",
        "print(dict_)"
      ],
      "metadata": {
        "id": "xO57FPkQuCr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The dictionary had 18 unqiue words in the cleaned corpus.\n",
        "for i in dict_.values():\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "qjrSFB10uClG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting list of documents (corpus) into Document Term Matrix using the dictionary\n",
        "doc_term_matrix = [dict_.doc2bow(i) for i in clean_corpus]\n",
        "doc_term_matrix"
      ],
      "metadata": {
        "id": "WZTiGv2xuChY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# topic modeling (gensim approach)"
      ],
      "metadata": {
        "id": "9krgSoK4vHNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the object for LDA model using gensim library\n",
        "Lda = gensim.models.ldamodel.LdaModel"
      ],
      "metadata": {
        "id": "hzSoq7ipuZIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running and Training LDA model on the document term matrix.\n",
        "ldamodel = Lda(doc_term_matrix, num_topics=2, id2word=dict_, passes=1, random_state=0, eval_every=None)"
      ],
      "metadata": {
        "id": "PwNFKNwDuZC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prints the topics with the indexes: 0,1,2 :\n",
        "# we need to manually check whethere the topics are different from one another or not\n",
        "ldamodel.print_topics()"
      ],
      "metadata": {
        "id": "SlKZSpMZuY8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# num_topics mean: how many topics want to extract\n",
        "# num_words: the number of words that want per topic\n",
        "print(ldamodel.print_topics(num_topics=2, num_words=5))"
      ],
      "metadata": {
        "id": "j9SRCsTcuY1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# printing the topic associations with the documents\n",
        "count = 0\n",
        "for i in ldamodel[doc_term_matrix]:\n",
        "    print(\"doc : \",count,i)\n",
        "    count += 1"
      ],
      "metadata": {
        "id": "Y5R-V8t2u70_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# topic modeling optimization (gensim)"
      ],
      "metadata": {
        "id": "miWhlggGvRFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel"
      ],
      "metadata": {
        "id": "3qzFT6dfKC82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to list\n",
        "data = df.text.values.tolist()"
      ],
      "metadata": {
        "id": "RB40y3j_KbHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[:2])"
      ],
      "metadata": {
        "id": "bvPd_xA5Lf5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "if False:\n",
        "    df['text'] = df['text'].str.replace(\"&#039;\", \"'\", regex=False)\n",
        "    # Remove Emails\n",
        "    data = [re.sub(r'\\S*@\\S*\\s?', '', str(sent)) for sent in data]\n",
        "    # Remove new line characters\n",
        "    data = [re.sub(r'\\s+', ' ', str(sent)) for sent in data]\n",
        "    # Remove distracting single quotes\n",
        "    data = [re.sub(r\"\\'\", \"\", str(sent)) for sent in data]\n",
        "    print(data[:2])"
      ],
      "metadata": {
        "id": "vCAs9rMNLf1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spacy for nlp analysis\n",
        "import spacy\n",
        "\n",
        "def lemmatization(texts, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
        "    texts_out = []\n",
        "    for text in texts:\n",
        "        doc = nlp(text)\n",
        "        new_text = []\n",
        "        for token in doc:\n",
        "            if token.pos_ in allowed_postags:\n",
        "                new_text.append(token.lemma_)\n",
        "        final = \" \".join(new_text)\n",
        "        texts_out.append(final)\n",
        "    return (texts_out)\n",
        "\n",
        "lemmatized_texts = lemmatization(data)\n",
        "print(lemmatized_texts[0][0:50])"
      ],
      "metadata": {
        "id": "gcpYM_c03OKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_words(texts):\n",
        "    final = []\n",
        "    for text in texts:\n",
        "        new = gensim.utils.simple_preprocess(text, deacc=True)\n",
        "        final.append(new)\n",
        "    return (final)\n",
        "\n",
        "data_words = gen_words(lemmatized_texts)\n",
        "\n",
        "print(data_words[0][0:5])"
      ],
      "metadata": {
        "id": "w3Xh-7pA3_vW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2word = corpora.Dictionary(data_words)\n",
        "\n",
        "corpus = []\n",
        "for text in data_words:\n",
        "    new = id2word.doc2bow(text)\n",
        "    corpus.append(new)\n",
        "\n",
        "print(corpus[0][0:20])\n"
      ],
      "metadata": {
        "id": "r9k39NX24Nqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = id2word[[0][:1][0]]\n",
        "print(word)"
      ],
      "metadata": {
        "id": "MhJLDsiu4kM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2word[0]"
      ],
      "metadata": {
        "id": "oDe_iZmaHWEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Human readable format of corpus (term-frequency)\n",
        "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
      ],
      "metadata": {
        "id": "iW4Lx4uiP2Ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics = []\n",
        "score = []\n",
        "\n",
        "for i in range(1,20,1):\n",
        "  lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=i, iterations=10, passes=10, random_state=100)\n",
        "  cm = CoherenceModel(model=lda_model, corpus=corpus, dictionary=id2word, coherence='u_mass')\n",
        "  topics.append(i)\n",
        "  score.append(cm.get_coherence())\n",
        "\n",
        "_=plt.plot(topics, score)\n",
        "_=plt.xlabel('number of topics')\n",
        "_=plt.ylabel('u_mass coherence score (-14, 14)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Iw05SsxGP2M5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics = []\n",
        "score = []\n",
        "\n",
        "for i in range(1,20,1):\n",
        "  lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=i, iterations=10, passes=10, random_state=100)\n",
        "  cm = CoherenceModel(model=lda_model, texts=data_words, corpus=corpus, dictionary=id2word, coherence='c_v')\n",
        "  topics.append(i)\n",
        "  score.append(cm.get_coherence())\n",
        "\n",
        "_=plt.plot(topics, score)\n",
        "_=plt.xlabel('number of topics')\n",
        "_=plt.ylabel('c_v coherence score (0-1)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Tq1QEKCOP2KC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# modeling with optimal number of topics\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, iterations=10, passes=10, random_state=100)\n"
      ],
      "metadata": {
        "id": "JqWDHXDXQ1tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print topics\n",
        "lda_model.print_topics(-1)"
      ],
      "metadata": {
        "id": "ADGTXAI7Q1rQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# topic modeling visualization (gensim)"
      ],
      "metadata": {
        "id": "Qv9sdGqjHjmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.gensim\n",
        "\n",
        "# Visualize the topics\n",
        "#pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
        "pyLDAvis.display(vis)\n",
        "#vis"
      ],
      "metadata": {
        "id": "d9dPrCB1Q1oS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the report\n",
        "pyLDAvis.save_html(vis, 'lab2-nlp2-topics.html')"
      ],
      "metadata": {
        "id": "8nubS6FkQ1lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# topic modelling with gensim and pyldavis\n",
        "- [https://nils-holmberg.github.io/cca-nlp/jnb/scom-gpols-topics.html](https://nils-holmberg.github.io/cca-nlp/jnb/scom-gpols-topics.html)"
      ],
      "metadata": {
        "id": "xHXA4AEgmxLv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c3o1lKrOnbxW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}