{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgjbhLsQ04g9"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nils-holmberg/socs-qmd/blob/main/jnb/lab6_cv3.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6S071Ga38hiK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBlkruw903w6"
      },
      "source": [
        "# image dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgpiOcF78gTN"
      },
      "outputs": [],
      "source": [
        "!gdown https://drive.google.com/uc?id=1sBsckDIyQJ-zTsEpjCntYMIJcbBpF3wk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waLkp4aL8x-2"
      },
      "outputs": [],
      "source": [
        "!unzip content-images.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwbWedZR1lE_"
      },
      "source": [
        "# video dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFnmjMCX-2wL"
      },
      "outputs": [],
      "source": [
        "!gdown https://drive.google.com/uc?id=1T9S32UwmDnUd6YbxRYfyG7-gud8jh2Y_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5k3-XKp-6fu"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "# Path to the video file\n",
        "video_path = 'content-video.mp4'\n",
        "\n",
        "# Create a VideoCapture object\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Check if video opened successfully\n",
        "if not cap.isOpened():\n",
        "    print(\"Error: Could not open video.\")\n",
        "    exit()\n",
        "\n",
        "# Directory to save frames\n",
        "save_dir = 'images-video'\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "frame_idx = 0\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    # Break the loop if there are no more frames\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Save every 100th frame\n",
        "    if frame_idx % 100 == 0:\n",
        "        # Filename with 5 leading zeroes\n",
        "        save_path = os.path.join(save_dir, f'frame_{frame_idx:05d}.jpg')\n",
        "        cv2.imwrite(save_path, frame)\n",
        "        print(f'Saved frame {frame_idx:05d}')\n",
        "\n",
        "    frame_idx += 1\n",
        "\n",
        "# Release the VideoCapture object\n",
        "cap.release()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R85rRHYK5iI7"
      },
      "source": [
        "# image normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjI2Iu-Z-_py"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def resize_and_crop(img, size):\n",
        "    # Resize image to maintain aspect ratio\n",
        "    h, w, _ = img.shape\n",
        "    if h > w:\n",
        "        new_h, new_w = size * h / w, size\n",
        "    else:\n",
        "        new_h, new_w = size, size * w / h\n",
        "\n",
        "    new_h, new_w = int(new_h), int(new_w)\n",
        "    resized_img = cv2.resize(img, (new_w, new_h))\n",
        "\n",
        "    # Crop the center of the image\n",
        "    startx = new_w//2 - size//2\n",
        "    starty = new_h//2 - size//2\n",
        "    return resized_img[starty:starty+size, startx:startx+size]\n",
        "\n",
        "def normalize_image(img):\n",
        "    # Normalize pixel values to [0, 1]\n",
        "    return img / 255.0\n",
        "\n",
        "directory = 'images-video'\n",
        "output_directory = 'images-normalize'\n",
        "\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
        "#    if filename.endswith('.jpg'):\n",
        "        img = cv2.imread(os.path.join(directory, filename))\n",
        "        img = resize_and_crop(img, 256)\n",
        "        normalized_img = normalize_image(img)\n",
        "\n",
        "        # Convert the normalized image back to 8-bit format\n",
        "        img_to_save = (normalized_img * 255).astype(np.uint8)\n",
        "\n",
        "        # Save the normalized image\n",
        "        output_path = os.path.join(output_directory, filename)\n",
        "        cv2.imwrite(output_path, img_to_save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhYnNPpCCB_a"
      },
      "source": [
        "# object features\n",
        "\n",
        "- face object detection with haar features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrizoPc6Pf7T"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "# Load the cascade\n",
        "#face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
        "face_cascade=cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
        "# Read the input image\n",
        "img = cv2.imread('/content/images-video/frame_01600.jpg')\n",
        "# Convert into grayscale\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "# Detect faces\n",
        "faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
        "# Draw rectangle around the faces\n",
        "for (x, y, w, h) in faces:\n",
        "    cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
        "# Display the output\n",
        "#cv2.imshow('img', img)\n",
        "#cv2.waitKey()\n",
        "from google.colab.patches import cv2_imshow\n",
        "cv2_imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "el5sxUnxqJKs"
      },
      "outputs": [],
      "source": [
        "cv2.imwrite(\"haar-face.png\", img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNk1vljkCB_b"
      },
      "source": [
        "# image classification\n",
        "\n",
        "- deep learning, mnist dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_wjNxryakIQ"
      },
      "source": [
        "## intro\n",
        "\n",
        "![mnist](https://raw.githubusercontent.com/nils-holmberg/sfac-py/main/fig/mnist-digits.png)\n",
        "\n",
        "import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qy3lu7MxGyXo"
      },
      "outputs": [],
      "source": [
        "#!pip install numpy\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sovq8FWFq0G"
      },
      "outputs": [],
      "source": [
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "#import tensorflow as tf; print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYLua0R0bebx"
      },
      "source": [
        "## data collection, preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4AhSiruG_SC"
      },
      "outputs": [],
      "source": [
        "# load the MNIST dataset\n",
        "(x_train, y_trainO), (x_test, y_testO) = mnist.load_data()\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "num_classes = 10\n",
        "y_train = keras.utils.to_categorical(y_trainO, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_testO, num_classes)\n",
        "\n",
        "# visualize data\n",
        "plt.rcParams['figure.figsize'] = (6,6)\n",
        "obs_idx = 1000\n",
        "plt.imshow(x_train[obs_idx], cmap='gray')\n",
        "print(y_train[obs_idx])\n",
        "\n",
        "# convert images into vectors of numbers\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# normalize\n",
        "x_train /= 255\n",
        "x_test /= 255\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dezd9BvXcoxm"
      },
      "source": [
        "## model specification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7iKtZ27deAE"
      },
      "outputs": [],
      "source": [
        "# specify MLP model (multi-layer perceptron)\n",
        "model = Sequential()\n",
        "model.add(Dense(16, activation='relu', input_shape=(784,)))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# compile model\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=RMSprop(),\n",
        "              metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlKuH2mXjR1U"
      },
      "source": [
        "# deep learning model\n",
        "\n",
        "![mnist](https://raw.githubusercontent.com/nils-holmberg/sfac-py/main/fig/lecture-201116-aiml.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U26wHzCGqP_W"
      },
      "source": [
        "## train model\n",
        "\n",
        "- multilayer perceptron (mlp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuLV5IB9pss5"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "epochs = 10\n",
        "\n",
        "# train the model (output training history)\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "\n",
        "# training diagnostics\n",
        "plt.figure('Model training')\n",
        "plt.ylabel('training error')\n",
        "plt.xlabel('epoch')\n",
        "for k in history.history.keys():\n",
        "    plt.plot(history.history[k], label=k)\n",
        "plt.legend(loc='best')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZE9XJkYitSM_"
      },
      "source": [
        "## evaluate model\n",
        "\n",
        "- on test data, generalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U77FLd79tAfk"
      },
      "outputs": [],
      "source": [
        "# evaluate model\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "# make a prediction on the test set\n",
        "#predicted_classes = model.predict_classes(x_test)\n",
        "predicted_classes = np.argmax(model.predict(x_test), axis=-1)\n",
        "\n",
        "# see which we predicted correctly and which not\n",
        "correct_indices = np.nonzero(predicted_classes == y_testO)[0]\n",
        "incorrect_indices = np.nonzero(predicted_classes != y_testO)[0]\n",
        "print()\n",
        "print(len(correct_indices), \"classified correctly\")\n",
        "print(len(incorrect_indices), \"classified incorrectly\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivklZ-FFv4lM"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    The function is used to construct the confusion matrix\n",
        "    \"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    plt.rcParams.update({'font.size': 16})\n",
        "    ax  = fig.add_subplot(111)\n",
        "    matrix = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    fig.colorbar(matrix)\n",
        "    for i in range(0,10):\n",
        "        for j in range(0,10):\n",
        "            ax.text(j,i,cm[i,j],va='center', ha='center')\n",
        "    ticks = np.arange(len(labels))\n",
        "    ax.set_xticks(ticks)\n",
        "    ax.set_xticklabels(labels, rotation=45)\n",
        "    ax.set_yticks(ticks)\n",
        "    ax.set_yticklabels(labels)\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sGGDb3FHL5w"
      },
      "outputs": [],
      "source": [
        "# visualize true and predicted labels\n",
        "y_prob = model.predict(x_test, batch_size=32, verbose=0)\n",
        "y_pred = [np.argmax(prob) for prob in y_prob]\n",
        "y_true = [np.argmax(true) for true in y_test]\n",
        "labels = np.arange(0,10,1)\n",
        "plot_confusion_matrix(y_true, y_pred, cmap=plt.cm.YlOrRd_r)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIXCcDcoHTt4"
      },
      "outputs": [],
      "source": [
        "# Some visual feedback\n",
        "\n",
        "# Make a prediction on the test set\n",
        "predicted_classes = np.argmax(model.predict(x_test), axis=-1)\n",
        "\n",
        "# See which we predicted correctly and which not\n",
        "correct_indices = np.nonzero(predicted_classes == y_testO)[0]\n",
        "incorrect_indices = np.nonzero(predicted_classes != y_testO)[0]\n",
        "print()\n",
        "print(len(correct_indices),\" classified correctly\")\n",
        "print(len(incorrect_indices),\" classified incorrectly\")\n",
        "\n",
        "# Adapt figure size to accomodate 18 subplots\n",
        "plt.rcParams['figure.figsize'] = (20,40)\n",
        "\n",
        "figure_evaluation = plt.figure()\n",
        "\n",
        "# Plot 9 correct predictions\n",
        "for i, correct in enumerate(correct_indices[:9]):\n",
        "    plt.subplot(6,3,i+1)\n",
        "    plt.imshow(x_test[correct].reshape(28,28), cmap='gray', interpolation='none')\n",
        "    plt.title(\n",
        "      \"Predicted: {}, Truth: {}\".format(predicted_classes[correct],\n",
        "                                        y_testO[correct]))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "# Plot 9 incorrect predictions\n",
        "for i, incorrect in enumerate(incorrect_indices[:9]):\n",
        "    plt.subplot(6,3,i+10)\n",
        "    plt.imshow(x_test[incorrect].reshape(28,28), cmap='gray', interpolation='none')\n",
        "    plt.title(\n",
        "      \"Predicted {}, Truth: {}\".format(predicted_classes[incorrect],\n",
        "                                       y_testO[incorrect]))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_APGA_kKVJI"
      },
      "outputs": [],
      "source": [
        "#model.save(\"mnist.mod\")\n",
        "model.save('mnist.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPGeKJyR7MRA"
      },
      "source": [
        "# image object detection\n",
        "\n",
        "- [huggingface](https://huggingface.co/tasks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VN6t5-Nl7SY7"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers\n",
        "!pip install -q timm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HT-Lm8eI7Qwn"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "model = pipeline(\"object-detection\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYTtPDg17kJd"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "rows_list = []\n",
        "for img in sorted(glob.glob('images-video/frame_*.jpg')):\n",
        "  dict1 = {}\n",
        "  res = model(img)\n",
        "  dict1.update({\"image\": img, \"result\": res})\n",
        "  rows_list.append(dict1)\n",
        "\n",
        "df = pd.DataFrame(rows_list)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJbf6_6Y9cCT"
      },
      "outputs": [],
      "source": [
        "rows_list = []\n",
        "for index, row in df.iterrows():\n",
        "  for i in row.result:\n",
        "    dict1 = {}\n",
        "    dict1.update({\"image\":row.image})\n",
        "    dict1.update(i)\n",
        "    rows_list.append(dict1)\n",
        "\n",
        "df_res = pd.DataFrame(rows_list)\n",
        "df_res.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiaHR_1P9kjT"
      },
      "outputs": [],
      "source": [
        "df_res.label.value_counts().plot(kind='bar', figsize=(6, 4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbVliiwf9osW"
      },
      "outputs": [],
      "source": [
        "df_res[df_res.image==\"images-video/frame_00100.jpg\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqUe8ol493s9"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "img = cv2.imread('images-video/frame_00100.jpg')\n",
        "for index,row in df_res[df_res.image==\"images-video/frame_00100.jpg\"].iterrows():\n",
        "  x1,y1,x2,y2 = list(row.box.values())\n",
        "  cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "cv2_imshow(img)\n",
        "#cv2.imshow(\"display\", img)\n",
        "#cv2.imwrite(\"objects.png\", img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YO6lKKWSPsc5"
      },
      "source": [
        "# advanced content inference\n",
        "\n",
        "- [mediapipe](https://www.youtube.com/watch?v=01sAkU_NvOY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MUyIjCoG6hm"
      },
      "outputs": [],
      "source": [
        "!pip install -q mediapipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l93XkAq4PzNG"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize MediaPipe Pose\n",
        "mp_pose = mp.solutions.pose\n",
        "pose = mp_pose.Pose(static_image_mode=True, model_complexity=1, enable_segmentation=True)\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "\n",
        "# Read an image\n",
        "image_path = 'images-video/frame_02800.jpg'  # Replace with the path to your image\n",
        "image_path = 'images-video/frame_00400.jpg'  # Replace with the path to your image\n",
        "image = cv2.imread(image_path)\n",
        "\n",
        "# Check if image is loaded\n",
        "if image is None:\n",
        "    print(\"Error: Image not found.\")\n",
        "else:\n",
        "    # Convert the BGR image to RGB\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Process the image and find the pose\n",
        "    results = pose.process(image_rgb)\n",
        "\n",
        "    # Draw pose landmarks on the image\n",
        "    if results.pose_landmarks:\n",
        "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
        "\n",
        "    # Convert back to BGR for displaying with OpenCV\n",
        "    image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # Display the image\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Release resources\n",
        "pose.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkgYWxwHoaaY"
      },
      "source": [
        "# try detectron2\n",
        "\n",
        "- [facebook meta](https://ai.meta.com/tools/detectron2/)\n",
        "- requires gpu session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYm6lYNcm_NB"
      },
      "outputs": [],
      "source": [
        "!python -m pip install pyyaml==5.1\n",
        "import sys, os, distutils.core\n",
        "# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities.\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions\n",
        "!git clone 'https://github.com/facebookresearch/detectron2'\n",
        "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
        "!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
        "sys.path.insert(0, os.path.abspath('./detectron2'))\n",
        "\n",
        "# Properly install detectron2. (Please do not install twice in both ways)\n",
        "# !python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETV1VVPMnE3A"
      },
      "outputs": [],
      "source": [
        "import torch, detectron2\n",
        "!nvcc --version\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "print(\"detectron2:\", detectron2.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPic8RkYnMtc"
      },
      "outputs": [],
      "source": [
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RPYiwPRo9IK"
      },
      "outputs": [],
      "source": [
        "img = cv2.imread('/content/images-video/frame_00500.jpg')\n",
        "cv2_imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoeCJDgnoh56"
      },
      "outputs": [],
      "source": [
        "cfg = get_cfg()\n",
        "# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
        "# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "predictor = DefaultPredictor(cfg)\n",
        "outputs = predictor(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeBO9SIfuL9p"
      },
      "outputs": [],
      "source": [
        "# look at the outputs. See https://detectron2.readthedocs.io/tutorials/models.html#model-output-format for specification\n",
        "print(outputs[\"instances\"].pred_classes)\n",
        "print(outputs[\"instances\"].pred_boxes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRmNFR76uWq7"
      },
      "outputs": [],
      "source": [
        "# We can use `Visualizer` to draw the predictions on the image.\n",
        "v = Visualizer(img[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
        "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "cv2_imshow(out.get_image()[:, :, ::-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJ3qOizZumgE"
      },
      "outputs": [],
      "source": [
        "# Inference with a keypoint detection model\n",
        "cfg = get_cfg()   # get a fresh new config\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # set threshold for this model\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\")\n",
        "predictor = DefaultPredictor(cfg)\n",
        "outputs = predictor(img)\n",
        "v = Visualizer(img[:,:,::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
        "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "cv2_imshow(out.get_image()[:, :, ::-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmj9wsiduxew"
      },
      "outputs": [],
      "source": [
        "# Inference with a panoptic segmentation model\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\"))\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\")\n",
        "predictor = DefaultPredictor(cfg)\n",
        "panoptic_seg, segments_info = predictor(img)[\"panoptic_seg\"]\n",
        "v = Visualizer(img[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
        "out = v.draw_panoptic_seg_predictions(panoptic_seg.to(\"cpu\"), segments_info)\n",
        "cv2_imshow(out.get_image()[:, :, ::-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MSo0f3em8go"
      },
      "source": [
        "# video analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYSbI8MinXhY"
      },
      "outputs": [],
      "source": [
        "# load data from url\n",
        "#import urllib.request\n",
        "#url = \"https://raw.githubusercontent.com/nils-holmberg/cca-cv/main/vid/clip.mp4\"\n",
        "#fp = \"/content/content-video.mp4\"\n",
        "#urllib.request.urlretrieve(url, fp)\n",
        "!ffmpeg -i /content/content-video.mp4 -t 00:00:06 -c:v copy /content/content-video-clip.mp4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-rFB7zInfov"
      },
      "outputs": [],
      "source": [
        "# Run frame-by-frame inference demo on this video (takes 3-4 minutes) with the \"demo.py\" tool we provided in the repo.\n",
        "!git clone https://github.com/facebookresearch/detectron2\n",
        "# Note: this is currently BROKEN due to missing codec. See https://github.com/facebookresearch/detectron2/issues/2901 for workaround.\n",
        "%run detectron2/demo/demo.py --config-file detectron2/configs/COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml --video-input /content/content-video-clip.mp4 --confidence-threshold 0.6 --output content-video-clip.mkv \\\n",
        "  --opts MODEL.WEIGHTS detectron2://COCO-PanopticSegmentation/panoptic_fpn_R_101_3x/139514519/model_final_cafdb1.pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMexrXT9ninT"
      },
      "outputs": [],
      "source": [
        "# Download the results\n",
        "from google.colab import files\n",
        "files.download('content-video-clip.mkv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EYorTs6aIoRC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}