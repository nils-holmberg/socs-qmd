{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nils-holmberg/socs-qmd/blob/main/jnb/lecture3_nlp3.ipynb)"
      ],
      "metadata": {
        "id": "CzbZy5WrZX8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# chatgpt test (topic modeling)\n",
        "\n",
        "prompt:"
      ],
      "metadata": {
        "id": "0klmvA6WO4qq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# paste chatgpt code here..\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jAKpwMZQStCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download stopwords if not already available\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load your Excel file\n",
        "excel_file_path = 'your_excel_file.xlsx'\n",
        "data = pd.read_excel(excel_file_path)\n",
        "\n",
        "# Assuming your text data is in a column named 'text_column'\n",
        "text_data = data['text_column']\n",
        "\n",
        "# Tokenization and preprocessing\n",
        "stop_words = set(stopwords.words('english'))  # English stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words=stop_words, token_pattern=r'\\b\\w+\\b')\n",
        "tf = vectorizer.fit_transform(text_data)\n",
        "\n",
        "# Apply LDA topic modeling\n",
        "num_topics = 4  # You can adjust the number of topics\n",
        "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
        "lda.fit(tf)\n",
        "\n",
        "# Print topics and associated words\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "num_top_words = 10  # You can adjust the number of top words per topic\n",
        "\n",
        "for topic_idx, topic in enumerate(lda.components_):\n",
        "    print(f\"Topic {topic_idx + 1}:\")\n",
        "    top_words_idx = topic.argsort()[:-num_top_words - 1:-1]\n",
        "    top_words = [feature_names[i] for i in top_words_idx]\n",
        "    print(\" \".join(top_words))\n"
      ],
      "metadata": {
        "id": "WxWP8ljqPXPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# text classification (naive bayes)"
      ],
      "metadata": {
        "id": "uTtmtKLTO4LD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1EMzJxxoBaN_NbvF7xhoc09K82vQ6H_LX"
      ],
      "metadata": {
        "id": "rr9PPhGPQ-UL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "fUXZtNs8RKMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp = \"content.xlsx\"\n",
        "df = pd.read_excel(fp, header=0, sheet_name='reviews')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "OPaAo31bRMZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download the NLTK stopwords if not already downloaded\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "id": "riqSiWaxRTAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## define features and outcomes\n",
        "\n",
        "## split training and test data"
      ],
      "metadata": {
        "id": "NrCcgQQLRlvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Step 1: Assuming your Excel columns are named 'text' and 'sentiment'\n",
        "X = df['text']  # Text data\n",
        "y = df['sentiment']  # Binary sentiment labels\n",
        "\n",
        "# Step 2: Perform TF-IDF vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # You can adjust the number of features as needed\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
        "\n",
        "# Step 3: Split the dataset into training (750 rows) and testing (250 rows)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Now, X_train and y_train contain the training data and labels (750 rows),\n",
        "# and X_test and y_test contain the testing data and labels (250 rows).\n"
      ],
      "metadata": {
        "id": "HVh7N3gwQ30t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train naive bayes classifier\n",
        "\n",
        "## validate model performance"
      ],
      "metadata": {
        "id": "k_w_RmKJRwwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Step 4: Train a Multinomial Naive Bayes classifier\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions on the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-score: {f1:.2f}\")\n",
        "\n",
        "# Step 7: Create and plot a confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(4, 2))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "11kEuWuwQ5Es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## save classification model\n",
        "\n",
        "## make inference on new samples"
      ],
      "metadata": {
        "id": "_iyaZL5MSRVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the trained model and TF-IDF vectorizer to files\n",
        "model_filename = 'sentiment_model.pkl'\n",
        "vectorizer_filename = 'tfidf_vectorizer.pkl'\n",
        "\n",
        "joblib.dump(clf, model_filename)\n",
        "joblib.dump(tfidf_vectorizer, vectorizer_filename)\n"
      ],
      "metadata": {
        "id": "mVybW_afSO1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To load the model and vectorizer for inference:\n",
        "loaded_model = joblib.load(model_filename)\n",
        "loaded_vectorizer = joblib.load(vectorizer_filename)\n",
        "\n",
        "# Example: Using the loaded model and vectorizer to predict sentiment for a new sample string\n",
        "new_sample = \"This phone is amazing\"\n",
        "# Vectorize the new sample using the loaded vectorizer\n",
        "new_sample_tfidf = loaded_vectorizer.transform([new_sample])\n",
        "# Use the loaded model for prediction\n",
        "prediction = loaded_model.predict(new_sample_tfidf)\n",
        "\n",
        "# Convert the prediction to a human-readable label if needed\n",
        "sentiment_label = \"Positive\" if prediction == 1 else \"Negative\"\n",
        "\n",
        "print(f\"Predicted sentiment: {sentiment_label}\")\n"
      ],
      "metadata": {
        "id": "FP5-WeyGUKjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# spacy language models"
      ],
      "metadata": {
        "id": "gu7tWB2wO31f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# medium size language model\n",
        "!python -m spacy download en_core_web_md\n"
      ],
      "metadata": {
        "id": "CESDGvnaVqk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvSRg1InOtwF"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define random words\n",
        "word = \"cat\"\n",
        "\n",
        "# Process the words to get their word vectors\n",
        "token = nlp(word)\n",
        "\n",
        "# Check if the token has a vector\n",
        "if token.has_vector:\n",
        "    # Print the word vector as a NumPy array\n",
        "    print(f\"Vector for '{word}':\")\n",
        "    print(token.vector)\n",
        "else:\n",
        "    print(f\"No vector available for '{word}'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(token.vector))"
      ],
      "metadata": {
        "id": "XBpkqRDAWn7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define two random words\n",
        "word1 = \"cat\"\n",
        "word2 = \"tiger\"\n",
        "\n",
        "# Process the words to get their word vectors\n",
        "token1 = nlp(word1)\n",
        "token2 = nlp(word2)\n",
        "\n",
        "# Check if the tokens are valid words (have word vectors)\n",
        "if token1.has_vector and token2.has_vector:\n",
        "    # Compute the cosine similarity between the word vectors\n",
        "    similarity = token1.similarity(token2)\n",
        "    print(f\"Similarity between '{word1}' and '{word2}': {similarity}\")\n",
        "else:\n",
        "    print(\"One or both of the words do not have word vectors available in the spaCy model.\")\n"
      ],
      "metadata": {
        "id": "hQvp1WdRWUQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f0M5ffTaU7Ca"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}