{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Day 4: Processing natural language texts\"\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "{{< include _include_d4.qmd >}}\n",
        "\n",
        "## Tokenization\n",
        "\n",
        "First we attempt to install needed packages, and their associated language data. These downloads may require quite a lot of free disk space!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "!pip install -q nltk spacy\n",
        "!python -m nltk.downloader popular\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# downloads to ~/nltk_data/\n",
        "#import nltk; nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's assume the following text data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read the uploaded TSV file into a Pandas DataFrame named 'df'\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/nils-holmberg/socs-qmd/main/txt/zen_of_python.tsv', sep='\\t')\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To tokenize the text data using the Natural Language Toolkit (NLTK) package, you can follow these steps:\n",
        "\n",
        "1. First, import the necessary NLTK library: `from nltk.tokenize import word_tokenize`.\n",
        "2. Create an empty DataFrame to store the tokenized words along with their corresponding 'id' from the original text.\n",
        "3. Loop through each row of the original DataFrame (`df`), tokenize the text in the 'text' column using `word_tokenize()`, and append the tokens along with their 'id' to the new DataFrame.\n",
        "\n",
        "Here's an inline code example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Create an empty DataFrame to store tokens and ids\n",
        "tokens_df = pd.DataFrame(columns=['id', 'token'])\n",
        "\n",
        "# Loop through each row in the original DataFrame\n",
        "for index, row in df.iterrows():\n",
        "    id_value = row['id']\n",
        "    text_value = row['text']\n",
        "    \n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text_value)\n",
        "    \n",
        "    # Create a temporary DataFrame to hold tokens and ids\n",
        "    temp_df = pd.DataFrame({'id': [id_value]*len(tokens), 'token': tokens})\n",
        "    \n",
        "    # Append to the main DataFrame\n",
        "    tokens_df = pd.concat([tokens_df, temp_df], ignore_index=True)\n",
        "\n",
        "# Show the first few rows of the resulting DataFrame\n",
        "tokens_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running this code will create a new DataFrame `tokens_df` that contains one token per row, along with the original 'id' to associate each token with its originating text.\n",
        "\n",
        "## Matrix representation\n",
        "\n",
        "Turning unstructured language data into structured tables @fig-mr\n",
        "\n",
        "![matrix representation](../../res/img/nlp-image_0-259d7a671398a16dc7cdfe05d89d4880.png){#fig-mr}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "!pip install -q scikit-learn\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
        "#from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Consider the following corpus composed of five short sentences (all taken from New York Times headlines). The algorithm should clearly identify one topic related to politics and coronavirus, and a second one related to Nadal and tennis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "corpus = [\"Rafael Nadal Joins Roger Federer in Missing U.S. Open\",\n",
        "          \"Rafael Nadal Is Out of the Australian Open\",\n",
        "          \"Biden Announces Virus Measures\",\n",
        "          \"Biden's Virus Plans Meet Reality\",\n",
        "          \"Where Biden's Virus Plan Stands\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using `CountVectorizer()`, we generate the matrix that denotes the frequency of the words of each text using `CountVectorizer()`. Note that the CountVectorizer allows for preprocessing if you include parameters such as stop_words to include the stop words, ngram_range to include n-grams, or `lowercase=True` to convert all characters to lowercase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "count_vect = CountVectorizer(stop_words=stopwords.words('english'), lowercase=True)\n",
        "x_counts = count_vect.fit_transform(corpus)\n",
        "x_counts.todense()\n",
        "#count_vect.get_feature_names_out()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Term frequency–inverse document frequency (tf–idf). Use the coefficient of tf–idf instead of noting the frequency of each word within each cell of the matrix. It consists of two numbers, multiplied:\n",
        "\n",
        "- tf: the frequency of a given term or word in a text, and\n",
        "- idf: the logarithm of the total number of documents divided by the number of documents that contain that given term.\n",
        "\n",
        "tf-idf is a measure of how frequently a word is used in the corpus. To be able to subdivide words into groups, it is important to understand not only which words appear in each text, but also which words appear frequently in one text but not at all in others."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "x_tfidf = tfidf_transformer.fit_transform(x_counts)\n",
        "x_tfidf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parts of speech\n",
        "\n",
        "Spacy is a prominent Python library for natural language processing. To analyze the Zen of Python with Spacy, one must first install the package and its English model. After loading the model, the Zen text can be processed to tokenize it. For a visual syntactic analysis of the first sentence, Spacy's `displacy` module can be employed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "# Open the file in read mode\n",
        "with open('../../txt/zen_of_python.txt', 'r') as file:\n",
        "    zen_text = file.read()\n",
        "\n",
        "# Load the English model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "#nlp._path\n",
        "\n",
        "# Process the Zen of Python text\n",
        "doc = nlp(zen_text)\n",
        "\n",
        "# Visualize the syntactic structure of the first sentence\n",
        "displacy.render(list(doc.sents)[0], style='dep', jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code provides a graphical representation of the sentence's grammatical relationships.\n",
        "\n",
        "## Named entities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "doc = nlp(\"Apple is looking at buying a Hong Kong startup for $1 billion\")\n",
        "for token in doc:\n",
        "    print(token.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To learn more about entity recognition in spaCy, how to **add your own\n",
        "entities** to a document and how to **train and update** the entity predictions\n",
        "of a model, see the usage guides on\n",
        "[named entity recognition](/usage/linguistic-features#named-entities) and\n",
        "[training pipelines](/usage/training).\n",
        "\n",
        "A named entity is a \"real-world object\" that's assigned a name – for example, a\n",
        "person, a country, a product or a book title. spaCy can **recognize various\n",
        "types of named entities in a document, by asking the model for a\n",
        "prediction**. Because models are statistical and strongly depend on the\n",
        "examples they were trained on, this doesn't always work _perfectly_ and might\n",
        "need some tuning later, depending on your use case.\n",
        "\n",
        "Named entities are available as the `ents` property of a `Doc`:\n",
        "\n",
        "> - **Text:** The original entity text.\n",
        "> - **Start:** Index of start of entity in the `Doc`.\n",
        "> - **End:** Index of end of entity in the `Doc`.\n",
        "> - **Label:** Entity label, i.e. type.\n",
        "\n",
        "| Text        | Start | End | Label   | Description                                          |\n",
        "| ----------- | :---: | :-: | ------- | ---------------------------------------------------- |\n",
        "| Apple       |   0   |  5  | `ORG`   | Companies, agencies, institutions.                   |\n",
        "| U.K.        |  27   | 31  | `GPE`   | Geopolitical entity, i.e. countries, cities, states. |\n",
        "| \\$1 billion |  44   | 54  | `MONEY` | Monetary values, including unit.                     |\n",
        "\n",
        "Using spaCy's built-in [displaCy visualizer](/usage/visualizers), here's what\n",
        "our example sentence and its named entities look like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "text = \"\"\"Apple decided to fire Tim Cook and hire somebody called John Doe as the new CEO.\n",
        "They also discussed a merger with Google. On the long run it seems more likely that Apple\n",
        "will merge with Amazon and Microsoft with Google. The companies will all relocate to\n",
        "Austin in Texas before the end of the century. John Doe bought a car.\"\"\"\n",
        "\n",
        "doc = nlp(text)\n",
        "displacy.render(doc, style='ent', jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Topic modeling\n",
        "\n",
        "To cluster our corpus, we can choose from several algorithms, including non-negative matrix factorization (NMF), sparse principal components analysis (sparse PCA), and latent dirichlet allocation (LDA). We'll focus on LDA because it is widely used by the scientific community due to its good results in social media, medical science, political science, and software engineering.\n",
        "\n",
        "LDA is a model for unsupervised topic decomposition: It groups texts based on the words they contain and the probability of a word belonging to a certain topic. The LDA algorithm outputs the topic word distribution. With this information, we can define the main topics based on the words that are most likely associated with them. Once we have identified the main topics and their associated words, we can know which topic or topics apply to each text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "x_tfidf = tfidf_transformer.fit_transform(x_counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to perform the LDA decomposition, we have to define the number of topics. In this simple case, we know there are two topics or \"dimensions.\" But in general cases, this is a hyperparameter that needs some tuning, which could be done using algorithms like random search or grid search:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "dimension = 2\n",
        "lda = LDA(n_components = dimension)\n",
        "lda_array = lda.fit_transform(x_tfidf)\n",
        "lda_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LDA is a probabilistic method. Here we can see the probability of each of the five headlines belonging to each of the two topics. We can see that the first two texts have a higher probability of belonging to the first topic and the next three to the second topic, as expected.\n",
        "\n",
        "Finally, if we want to understand what these two topics are about, we can see the most important words in each topic:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "components = [lda.components_[i] for i in range(len(lda.components_))]\n",
        "#features = count_vect.get_feature_names()\n",
        "#features = count_vect.get_feature_names_out()\n",
        "#important_words = [sorted(features, key = lambda x: components[j][features.index(x)], reverse = True)[:3] for j in range(len(components))]\n",
        "#important_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```python\n",
        "[['virus', 'biden', 'plan'], ['open', 'nadal', 'rafael']]\n",
        "```\n",
        "\n",
        "As expected, LDA correctly assigned words related to tennis tournaments and Nadal to the first topic and words related to Biden and virus to the second topic.\n",
        "\n",
        "## Try it yourself!\n",
        "\n",
        "Here are 10 tasks focusing on cleaning language data using the NLTK package:\n",
        "\n",
        "**Tasks:**\n",
        "\n",
        "1. Tokenize the given text into words.\n",
        "2. Convert all the tokens into lowercase.\n",
        "3. Remove any punctuation from the tokenized words.\n",
        "4. Use stemming to reduce the words to their root form.\n",
        "5. Lemmatize the words to obtain their base or dictionary form.\n",
        "6. Remove English stopwords from the tokenized list.\n",
        "7. Tokenize the given text into sentences.\n",
        "8. Count the frequency of each word in the tokenized list.\n",
        "9. Find the bigrams (two consecutive words) in the tokenized list.\n",
        "10. Identify the parts of speech (POS) for each token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "#| echo: false\n",
        "#| output: false\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.util import bigrams\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Sample text for processing\n",
        "text = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
        "\n",
        "# 1. Tokenize\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# 2. Convert to lowercase\n",
        "lower_tokens = [token.lower() for token in tokens]\n",
        "\n",
        "# 3. Remove punctuation\n",
        "words = [word for word in lower_tokens if word.isalnum()]\n",
        "\n",
        "# 4. Stemming\n",
        "ps = PorterStemmer()\n",
        "stemmed_words = [ps.stem(word) for word in words]\n",
        "\n",
        "# 5. Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "# 6. Remove stopwords\n",
        "filtered_words = [word for word in lemmatized_words if word not in stopwords.words('english')]\n",
        "\n",
        "# 7. Sentence tokenization\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "# 8. Frequency distribution\n",
        "freq_dist = FreqDist(filtered_words)\n",
        "\n",
        "# 9. Bigrams\n",
        "word_bigrams = list(bigrams(tokens))\n",
        "\n",
        "# 10. POS tagging\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "tokens, lower_tokens, words, stemmed_words, lemmatized_words, filtered_words, sentences, freq_dist, word_bigrams, pos_tags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: Before executing the code, ensure you have the required NLTK data downloaded (e.g., tokenizers, stopwords, and the averaged_perceptron_tagger for POS tagging). You can use `nltk.download('package_name')` to download the necessary datasets.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}