{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Day 4: Processing natural language texts\"\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "{{< include _include_d4.qmd >}}\n",
        "\n",
        "## Tokenization\n",
        "\n",
        "Let's assume the following text data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "import pandas as pd\n",
        "# Read the uploaded TSV file into a Pandas DataFrame named 'df'\n",
        "df = pd.read_csv('../../txt/zen_of_python.tsv', sep='\\t')\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To tokenize the text data using the Natural Language Toolkit (NLTK) package, you can follow these steps:\n",
        "\n",
        "1. First, import the necessary NLTK library: `from nltk.tokenize import word_tokenize`.\n",
        "2. Create an empty DataFrame to store the tokenized words along with their corresponding 'id' from the original text.\n",
        "3. Loop through each row of the original DataFrame (`df`), tokenize the text in the 'text' column using `word_tokenize()`, and append the tokens along with their 'id' to the new DataFrame.\n",
        "\n",
        "Here's an inline code example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pandas as pd\n",
        "\n",
        "# Create an empty DataFrame to store tokens and ids\n",
        "tokens_df = pd.DataFrame(columns=['id', 'token'])\n",
        "\n",
        "# Loop through each row in the original DataFrame\n",
        "for index, row in df.iterrows():\n",
        "    id_value = row['id']\n",
        "    text_value = row['text']\n",
        "    \n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text_value)\n",
        "    \n",
        "    # Create a temporary DataFrame to hold tokens and ids\n",
        "    temp_df = pd.DataFrame({'id': [id_value]*len(tokens), 'token': tokens})\n",
        "    \n",
        "    # Append to the main DataFrame\n",
        "    tokens_df = pd.concat([tokens_df, temp_df], ignore_index=True)\n",
        "\n",
        "# Show the first few rows of the resulting DataFrame\n",
        "tokens_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running this code will create a new DataFrame `tokens_df` that contains one token per row, along with the original 'id' to associate each token with its originating text.\n",
        "\n",
        "## Parts of speech\n",
        "\n",
        "Spacy is a prominent Python library for natural language processing. To analyze the Zen of Python with Spacy, one must first install the package and its English model. After loading the model, the Zen text can be processed to tokenize it. For a visual syntactic analysis of the first sentence, Spacy's `displacy` module can be employed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "# Open the file in read mode\n",
        "with open('../../txt/zen_of_python.txt', 'r') as file:\n",
        "    zen_text = file.read()\n",
        "\n",
        "# Load the English model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "#nlp._path\n",
        "\n",
        "# Process the Zen of Python text\n",
        "doc = nlp(zen_text)\n",
        "\n",
        "# Visualize the syntactic structure of the first sentence\n",
        "displacy.render(list(doc.sents)[0], style='dep', jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code provides a graphical representation of the sentence's grammatical relationships.\n",
        "\n",
        "## Topic modeling\n",
        "\n",
        "- Upcoming\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}