the-content-analysis-guidebook-2e-6	00	Sage Research Methods The Content Analysis Guidebook For the most optimal reading experience we recommend using our website.
the-content-analysis-guidebook-2e-6	00	A free-to-view version of this content is available by clicking on this link, which includes an easy-to-navigate-and-search-entry, and may also include videos, embedded datasets, downloadable datasets, interactive questions, audio content, and downloadable tables and resources.
the-content-analysis-guidebook-2e-6	00	Author: Kimberly A Neuendorf Pub.
the-content-analysis-guidebook-2e-6	00	Date: 2019 Product: Sage Research Methods DOI: https://doi.org/10.4135/9781071802878 Methods: Content analysis, Measurement, Coding Keywords: coding schemes Disciplines: Business and Management, Communication and Media Studies, Political Science and International Relations, Psychology, Social Policy and Public Policy, Sociology Access Date: November 26, 2023 Publishing Company: SAGE Publications, Inc City: Thousand Oaks Online ISBN: 9781071802878 © 2019 SAGE Publications, Inc All Rights Reserved.
the-content-analysis-guidebook-2e-6	01	pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	01	Reliability Reliability can be defined as the extent to which a measuring procedure yields the same results on repeated trials (Carmines & Zeller, 1979).
the-content-analysis-guidebook-2e-6	01	When human coders are used in content analysis, this typically translates to intercoder reliability, or the amount of agreement or correspondence on a measured variable among two or more coders or raters.
the-content-analysis-guidebook-2e-6	01	Two other types of coder reliability, less studied and less frequently applied, are intracoder reliability, which considers the stability of a given coder’s measurements over time, and (intercoder) unitizing reliability, which assesses whether coders can agree on the delineation of units of data collection when that is part of the coding protocol.
the-content-analysis-guidebook-2e-6	01	Reliability is sometimes viewed as related to replicability.
the-content-analysis-guidebook-2e-6	01	A unique perspective on this is given by Rourke et al.
the-content-analysis-guidebook-2e-6	01	(2001, p 7): “The reliability of a coding scheme can be viewed as a continuum, beginning with coder stability [intracoder reliability] .
the-content-analysis-guidebook-2e-6	01	.
the-content-analysis-guidebook-2e-6	01	.
the-content-analysis-guidebook-2e-6	01	to inter-rater reliability .
the-content-analysis-guidebook-2e-6	01	.
the-content-analysis-guidebook-2e-6	01	.
the-content-analysis-guidebook-2e-6	01	and ultimately to replicability (the ability of multiple and distinct groups of researchers to apply a coding scheme reliably).” Similarly, Popping (2010) argues that the purpose of reliability assessment is to assure that a coding scheme can be replicated elsewhere.
the-content-analysis-guidebook-2e-6	01	And although the type of reliability referenced is internal consistency reliability, LeBel and Paunonen (2011, p 570) cite a classic piece by Nunnally (1982) and provide further support for his contention that “science is limited [in its replicability] by the reliability of measuring instruments and the reliability with which scientists use them.” Further, De Wever et al.
the-content-analysis-guidebook-2e-6	01	(2006) argue that the lack of standard coding schemes—that is, the dearth of replications—in content analysis research is a threat to both the validity and reliability of coding.
the-content-analysis-guidebook-2e-6	01	Given that a goal of content analysis is to identify and record relatively objective (or at least intersubjective) characteristics of messages, reliability is paramount.
the-content-analysis-guidebook-2e-6	01	Without the establishment of reliability, content analysis measures are useless.
the-content-analysis-guidebook-2e-6	01	Remember that without reliability, a measure cannot be considered valid (however, reliability does not ensure validity; ie., reliability is a necessary but not sufficient condition for validity).
the-content-analysis-guidebook-2e-6	01	The discussion in this chapter focuses on human coding techniques, with consideration given to how to achieve good intercoder reliability.
the-content-analysis-guidebook-2e-6	01	The chapter presents a variety of intercoder reliability coefficients, including key formulas, and gives attention to the use of multiple human coders, a widely used technique that is often ignored in the methodology literature.
the-content-analysis-guidebook-2e-6	01	Computer programs and applications that aid in the computation of intercoder reliability coefficients are summarized.
the-content-analysis-guidebook-2e-6	01	A portion of this chapter deals with procedures for the treatment of variables that do not achieve acceptable levels of reliability.
the-content-analysis-guidebook-2e-6	01	The chapter also mentions the rare but very real possibility of needing to drop a coder from a study.
the-content-analysis-guidebook-2e-6	02	And this chapter considers extensions of the Page 2 of 42 The Content Analysis Guidebook pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	02	traditional ways of looking at reliability in content analysis by introducing issues such as coding stability (intracoder reliability), unitizing reliability, the establishment of confidence intervals around reliability coefficients, and building models that indicate sources of variation in human coding.
the-content-analysis-guidebook-2e-6	02	Intercoder Reliability: An Introduction to Standards and Practices Achieving an acceptable level of intercoder reliability is important for two reasons: 1.
the-content-analysis-guidebook-2e-6	02	To provide basic validation of a coding scheme: That is, it must be established that more than one individual can use the coding scheme as a measurement tool, with similar results.
the-content-analysis-guidebook-2e-6	02	Put a different way, it must be confirmed that the coding scheme is not limited to use by only one individual (that would be more along the lines of expert analysis and not a true content analysis; Carletta, 1996).
the-content-analysis-guidebook-2e-6	02	As Tinsley and Weiss (1975) note, it is important to demonstrate that the “obtained ratings are not the idiosyncratic results of one rater’s subjective judgment” (p.
the-content-analysis-guidebook-2e-6	02	359).
the-content-analysis-guidebook-2e-6	02	This means that even if the principal investigator does all of the coding, a reliability check with a second coder is needed (Evans, 1996).
the-content-analysis-guidebook-2e-6	02	2.
the-content-analysis-guidebook-2e-6	02	For the practical advantage of using multiple coders: Splitting up the coding task allows for more messages to be processed, as long as the two or more coders are “calibrated” against one another.
the-content-analysis-guidebook-2e-6	02	For reason Number 1, at least two coders need to participate in any human-coding content analysis.
the-content-analysis-guidebook-2e-6	02	For Number 2, we may employ up to 30 or 40 different individuals (Potter & Levine-Donnerstein, 1999).
the-content-analysis-guidebook-2e-6	02	There is contemporary acknowledgment in the research literature that the establishment of intercoder reliability is essential, a necessary criterion for valid and useful research when human coding is employed.
the-content-analysis-guidebook-2e-6	02	However, this has followed a period during which many researchers were less than rigorous in their reliability assessment (see Feng, 2015).
the-content-analysis-guidebook-2e-6	02	As Perreault and Leigh (1989) note, the marketing research literature to that date had “no accepted standard for evaluating or reporting the reliability of coded data” (p.
the-content-analysis-guidebook-2e-6	02	137).
the-content-analysis-guidebook-2e-6	02	In consumer behavior research, Kolbe and Burnett (1991) found 31% of the content analysis articles reported no reliability coefficients, and an additional 19% had no discernible method of calculation for reliability.
the-content-analysis-guidebook-2e-6	02	A full 36% inappropriately reported a single so-called “overall” reliability for all variables in the study.
the-content-analysis-guidebook-2e-6	02	Pasadeos et al.
the-content-analysis-guidebook-2e-6	02	(1995) found that only 49% of content analyses published in four major communication journals between 1988 and 1993 made any mention of reliability assessment.
the-content-analysis-guidebook-2e-6	03	Reporting on 486 content analysis studies published in Journalism & Mass Communication Quarterly from 1971 through 1995, Riffe and Freitag (1997) found that only 56% of the studies reported intercoder reliability figures and that most of these failed to report reliability variable by Page 3 of 42 The Content Analysis Guidebook pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	03	variable.
the-content-analysis-guidebook-2e-6	03	An analysis of 200 content analyses in the communication literature by Lombard, Snyder-Duch, and Bracken (2002) found that 69% discussed intercoder reliability, with only 41% appropriately reporting reliability variable by variable.
the-content-analysis-guidebook-2e-6	03	In a study of health-related media content analysis publications from the years 1977 through 2005, Neuendorf (2009) identified 62% as reporting intercoder reliability, but with only 20% reporting figures variable by variable.
the-content-analysis-guidebook-2e-6	03	A review of 441 content analyses of health messages in US.
the-content-analysis-guidebook-2e-6	03	mass media between 1985 and 2005 by Manganello and Blake (2010) found 70% to report reliability assessment, with 59% reporting coefficients variable by variable.
the-content-analysis-guidebook-2e-6	03	And a review of 80 content analyses published in the journal Journalism & Mass Communication Quarterly after 1998 found that 26% failed to report reliability assessment, and only 16% demonstrated full reportage of appropriate tests on all variables (Riffe, Lacy, & Fico, 2014).
the-content-analysis-guidebook-2e-6	03	The practice of averaging reliability coefficients across variables is inappropriate.
the-content-analysis-guidebook-2e-6	03	It obviously results in the obscuring of low reliabilities that do not pass muster.
the-content-analysis-guidebook-2e-6	03	For example, in a study of television characters, a variable such as “empathetic or not empathetic” with an agreement reliability of only 20% (obviously unacceptable) could be averaged with such no-brainer variables as gender, race, age category, and marital status and might easily be hidden in an overall average reliability of over 90%.
the-content-analysis-guidebook-2e-6	03	Reliability coefficients should be reported separately for every measured variable.
the-content-analysis-guidebook-2e-6	03	Or, at the very least, researchers might report the minimum reliability coefficient that was achieved for all variables in the analysis.
the-content-analysis-guidebook-2e-6	03	What statistical criteria constitute acceptable levels of intercoder reliability for each variable is open to debate.
the-content-analysis-guidebook-2e-6	03	Unfortunately, uniform standards are not in place (Krippendorff, 2013; Neuendorf, 2009; Perreault & Leigh, 1989; Popping, 1988; Riffe, Lacy, & Fico, 2014), neither for what statistics should be employed nor for the critical value that should be achieved for a given statistic.
the-content-analysis-guidebook-2e-6	03	Generally, basic textbooks on research methods in the social sciences do not even present procedures for assessing intercoder reliability, nor do they offer a specific statistical criterion or cutoff figure.
the-content-analysis-guidebook-2e-6	03	Elsewhere, various rules of thumb have been proposed over the years: • Landis and Koch (1977) proposed criteria for the widely-used Cohen’s kappa, with .81 to 1.00 indicating “almost perfect” agreement, .61 to .80 substantial agreement, .41 to .60 moderate agreement, and .21 to .40 fair agreement.
the-content-analysis-guidebook-2e-6	03	• Banerjee et al.
the-content-analysis-guidebook-2e-6	03	(1999) proposed the following criteria for Cohen’s kappa: .75+ indicating excellent agreement beyond chance; .40 to .75, fair to good agreement beyond chance; and below .40, poor agreement beyond chance.
the-content-analysis-guidebook-2e-6	03	• Popping (1988) proposed a cutoff criterion of .80 or greater for Cohen’s kappa.
the-content-analysis-guidebook-2e-6	04	• Krippendorff (2013, p 325) set standards for his own alpha (α) statistic: “Rely only on variables with Page 4 of 42 The Content Analysis Guidebook pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	04	reliabilities above α = .800.
the-content-analysis-guidebook-2e-6	04	Consider variables with reliabilities between α = .667 and α = .800 only for drawing tentative conclusions.” It’s clear from a review of the work on reliability that agreement reliability coefficients that account for chance (e.g., Cohen’s kappa) of .80 or greater would be acceptable to all, .60 or greater would be acceptable in most situations, and below that, there exists disagreement.
the-content-analysis-guidebook-2e-6	04	Tests of statistical significance have occasionally been applied to reliability coefficients (e.g., the use of a z-statistic to test the difference between a kappa value and zero; Bartko & Carpenter, 1976),1 but the utility of such tests is open to debate.
the-content-analysis-guidebook-2e-6	04	That is, there is a difference between inferential statistical significance and substantive significance, or meaningfulness, as when a Pearson correlation coefficient of r = .12, with shared variance between the two sets of scores only about 1%, is found to be statistically significant.
the-content-analysis-guidebook-2e-6	04	What this indicates is that a very small relationship (substantively rather unimportant) may confidently be generalized to the population (statistically significant).
the-content-analysis-guidebook-2e-6	04	This problem can be shown in a reliability application with Bartko and Carpenter’s (p.
the-content-analysis-guidebook-2e-6	04	311) report of a kappa of .40 (rather low by most rules of thumb) that is highly statistically significant.
the-content-analysis-guidebook-2e-6	04	Our conclusion is that we may generalize a barely acceptable reliability to a population of messages.
the-content-analysis-guidebook-2e-6	04	In the absence of a uniform standard or test of meaningful significance (Popping, 1988), the best we can expect at present is full and clear reporting of at least one reliability coefficient for each variable measured in a human-coded content analysis.
the-content-analysis-guidebook-2e-6	04	Dixon and Linz (2000) provided a model example for such reportage, giving reliability coefficients for each of 14 variables, reported separately for each of five message sources.
the-content-analysis-guidebook-2e-6	04	Following an analysis of 200 communication content analyses, Lombard et al.
the-content-analysis-guidebook-2e-6	04	(2002) put forward a set of recommendations, including a standard of a minimum of two coders, the calculation of an appropriate reliability figure for each variable measured, and the clear reportage of the reliability sample size and its relation to the overall sample (including how it was selected).
the-content-analysis-guidebook-2e-6	04	Issues in the Assessment of Reliability Before exploring the options for calculating intercoder reliability, a consideration of the main issues inherent in selecting an appropriate process of reliability assessment will be presented.
the-content-analysis-guidebook-2e-6	05	Page 5 of 42 The Content Analysis Guidebook pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	05	Agreement Versus Covariation Three types of reliability assessment have been considered: agreement, chance-corrected agreement, and covariation.
the-content-analysis-guidebook-2e-6	05	Agreement looks at whether or not coders agree as to the precise values assigned to a variable across a set of cases—it looks at hits and misses.
the-content-analysis-guidebook-2e-6	05	Chance-corrected agreement makes the assumption that some portion of coders’ agreement is due to chance and adjusts for that.
the-content-analysis-guidebook-2e-6	05	Covariation assesses whether the scores assigned by coders (rating cases on an ordinal, interval, or ratio measure) go up and down together, but not necessarily in precise agreement.
the-content-analysis-guidebook-2e-6	05	It is generally not acceptable to conduct only agreement assessment without a correction for chance, although simple agreement might be reported along with other tests as an heuristic.
the-content-analysis-guidebook-2e-6	05	In later sections of this chapter, these three types of reliability assessment will be explained and selected coefficients will be introduced.
the-content-analysis-guidebook-2e-6	05	Reliability as a Function of Subsamples and Coders Although we would like to think of reliability analysis as reflecting the success of the coding scheme, reliability is a function of two other elements as well: the particular cases rated and the particular judges making the ratings.
the-content-analysis-guidebook-2e-6	05	It is for this reason, as Tinsley and Weiss (1975) point out, that it would be inappropriate to report a generalized reliability for a variable extracted from other studies.
the-content-analysis-guidebook-2e-6	05	The representativeness of the message units/cases and of the coders are important considerations.
the-content-analysis-guidebook-2e-6	05	In a later section, reliability subsample selection and coder assignment procedures that take this notion into account are described.
the-content-analysis-guidebook-2e-6	05	If we view a reliability test as a sample representative of all possible tests, then it makes sense to apply inferential techniques to indicate what the true population reliability might be.
the-content-analysis-guidebook-2e-6	05	Some researchers have promoted the application of the notion of standard errors and confidence intervals to reliability tests (Kraemer, 1980; Lacy & Riffe, 1996).
the-content-analysis-guidebook-2e-6	05	In other words, each reliability figure may have a confidence interval constructed around it; for example, we could hypothetically say, “the reliability for number of verbal nonfluencies was .92, plus or minus .04 at the 95% confidence level.”2 Confidence intervals are rarely reported in the business and social science literature at present, but consideration of their use is growing.
the-content-analysis-guidebook-2e-6	05	Some examples of their calculation will be given in an endnote that goes along with calculations in Boxes 6.2 and 6.3.
the-content-analysis-guidebook-2e-6	06	Page 6 of 42 The Content Analysis Guidebook pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	06	Threats to Reliability In practice, there are several key threats to reliability that should be taken into account: 1.
the-content-analysis-guidebook-2e-6	06	A poorly executed coding scheme: This could mean a poorly worded set of instructions in a codebook, the failure of the researcher to make changes in the coding scheme after a pilot test, or both.
the-content-analysis-guidebook-2e-6	06	2.
the-content-analysis-guidebook-2e-6	06	Inadequate coder training: As outlined in Chapter 5, coder training typically involves several sessions and practice codings to establish good initial reliability for the pilot test.
the-content-analysis-guidebook-2e-6	06	3.
the-content-analysis-guidebook-2e-6	06	Coder fatigue and coder drift: Coder performance may be impacted either short-term or long-term by fatigue brought about by an over-long codebook or a very long or intensive coding schedule (Potter et al., 1998).
the-content-analysis-guidebook-2e-6	06	Coder drift or rater drift is a broader term that has been applied to the general phenomenon of coders changing their habits of coding over time, a type of moving bias.
the-content-analysis-guidebook-2e-6	06	This drift may be a result of long-term coder fatigue or of coder learning (an increase in expertise as coders get more experience “under their belt”).
the-content-analysis-guidebook-2e-6	06	Or drift may occur as coders move away from their initial training (Haden & Hoffman, 2013) and begin to rely on their own “common sense” judgments.
the-content-analysis-guidebook-2e-6	06	All of these phenomena—even coder learning, which seems a good thing—are threats to both stability reliability and intercoder reliability (Lee et al., 2014).
the-content-analysis-guidebook-2e-6	06	4.
the-content-analysis-guidebook-2e-6	06	The presence of a rogue coder: Although rarely encountered, there is always the possibility of the appearance of a coder who simply cannot—or will not—be trained to achieve reliability (e.g., Capwell, 1997; Wagner & Hansen, 2002).
the-content-analysis-guidebook-2e-6	06	The coder may have to be removed from the study, but this should be done only after repeated attempts at training and the examination of that coder’s reliability performance against several other coders across a wide variety of variables (e.g., National Television Violence Study, 1997).
the-content-analysis-guidebook-2e-6	06	Reliability for Manifest Versus Latent Content “With manifest content, the coding task is one of clerical recording,” note Potter and Levine-Donnerstein (1999, p 265).
the-content-analysis-guidebook-2e-6	06	Although this might be an oversimplification, it does clarify the distinction between coding manifest versus latent content.
the-content-analysis-guidebook-2e-6	06	Objectivity is a much tougher criterion to achieve with variables closer to the latent pole of a continuum than variables that are more clearly manifest.
the-content-analysis-guidebook-2e-6	06	For this reason, we expect variables measuring latent content to receive generally lower reliability scores.
the-content-analysis-guidebook-2e-6	06	Obviously, this indicates a need for greater coder training efforts in instances of latent-content coding, perhaps very specialized training (Ahuvia, 2001).
the-content-analysis-guidebook-2e-6	07	And there may exist certain constructs that are problematic because of their inherent latency; for example, Page 7 of 42 The Content Analysis Guidebook pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	07	Box 6.1 discusses the problems inherent in attempted measurements of the construct “humor.” Box 6.1 Humor, A Problematic Construct Partitioning a Construct on the Basis of Reliability-Imposed Constraints Humor, so ubiquitous in messages of so many kinds (Martin, 2007), can be difficult to conceptualize and to operationalize.
the-content-analysis-guidebook-2e-6	07	It’s a popular construct for students trying to content analyze messages; their typical “I-know-itwhen-I-see-it” attitude often results in unacceptable reliabilities (e.g., Naccarato, 1990; Wongthongsri, 1993).
the-content-analysis-guidebook-2e-6	07	Many attempts to isolate individual instances of humor in messages and then code each with a scheme of mutually exclusive and exhaustive humor-type categories have failed to produce reliable outcomes.
the-content-analysis-guidebook-2e-6	07	There are several challenges to the construct of humor: It’s subjective, so much so that some scholars say it resides in the receiver rather than the message (Neuendorf & Skalski, 2000; Ziv, 1984).
the-content-analysis-guidebook-2e-6	07	Clearly, humor’s polysemy (i.e., its being open to multiple interpretations of meaning) has provided a challenge to the reliability of content analysis coding (Boxman-Shabtai & Shifman, 2014).
the-content-analysis-guidebook-2e-6	07	It’s multidimensional, and separable “senses of humor” do exist, including appreciation of incongruity, disparagement, arousal, and social currency types of humor (Crawford & Gressley, 1991; Neuendorf & Skalski, 2000; Neuendorf et al., 2014; Shifman & Blondheim, 2010).
the-content-analysis-guidebook-2e-6	07	It’s primarily latent in nature rather than manifest, with the typical challenges that go along with latent content.
the-content-analysis-guidebook-2e-6	07	Two different tactics have been used by researchers trying to reliably measure humor in messages.
the-content-analysis-guidebook-2e-6	07	First, some have targeted and measured very specific aspects of humor.
the-content-analysis-guidebook-2e-6	07	They have defined manifest characteristics related to portions of the overall humor construct and have achieved reliability for particular components or applications of humor—for instance, disparagement humor (Scharrer, Bergstrom et al., 2006; Stocking, Sapolsky, & Zillmann, as cited in Zillmann, 1977), nonsense humor (Bryant, Hezel, & Zillmann, 1979), incongruity humor (Alden, Hoyer, & Lee, 1993; Scharrer, Kim et al., 2006), aggressive-sexual humor (McCullough, 1993), fat stigmatization humor (Hussin, Frazier, & Thompson, 2011), level of brutality in humor (Zillmann, Bryant, & Cantor, 1974), and devices such as puns, understatement, and satire/irony (Sternthal & Craig, 1973; Yee, 2011).
the-content-analysis-guidebook-2e-6	08	Other researchers (e.g., Morris, 2009; Shifman, 2007) have sidestepped the process of identifying whether humor is present by sampling from designated humor content (e.g., humor blogs; jokes on The Daily Show) and then measuring other characteristics of the humorous messages (e.g., web site topics, type Page 8 of 42 The Content Analysis Guidebook pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	08	of posting [jokes vs.
the-content-analysis-guidebook-2e-6	08	funny photos vs.
the-content-analysis-guidebook-2e-6	08	cartoons], targets of jokes, finite joke tones such as self-deprecating and physical).
the-content-analysis-guidebook-2e-6	08	The second tactic is quite the opposite, taking a macroscopic approach by examining, simply, the presence or absence of humorous intent in the message without judging the nature or extent of the humor or how it is received (e.g., Kousha, Thelwall, & Abdoli, 2012; Papacharissi, 2007; Potter & Warren, 1998; Scharrer, Bergstrom et al., 2006; Weinberger et al., 1995).
the-content-analysis-guidebook-2e-6	08	This general approach seems to facilitate reliability but may fail to tap the nuances of humor types and may therefore result in poor predictive ability; its content validity is limited.
the-content-analysis-guidebook-2e-6	08	For example, there are highly mixed findings among surveys and experiments testing the effectiveness of humor in advertising (Laroche et al., 2011; Markiewicz, 1974; Weinberger et al., 1995) due to the highly divergent ways in which humor has been operationalized in those studies, as well as the divergent interpretations receivers have of the use of humor appeals in ads (Paek, Hove, & Jeon, 2013).
the-content-analysis-guidebook-2e-6	08	As Weinberger et al.
the-content-analysis-guidebook-2e-6	08	note, “Generalizations about its effects are difficult to come by because of its diverse nature” (p.
the-content-analysis-guidebook-2e-6	08	54).
the-content-analysis-guidebook-2e-6	08	Humor is a highly attractive construct, which many practitioners and scholars would agree is an important mediating variable for the reception of messages (Alden, Hoyer, & Lee, 1993).
the-content-analysis-guidebook-2e-6	08	But it means so many things to so many people, it must be partitioned carefully to develop measures that are reliable.
the-content-analysis-guidebook-2e-6	08	Pilot and Final Reliabilities Reliability should always be assessed at, minimally, two points in a content analysis: pilot and final.
the-content-analysis-guidebook-2e-6	08	The pilot reliability assessment is conducted after initial coder training and before the study begins in earnest.
the-content-analysis-guidebook-2e-6	08	It should be done on a sample that is either a subsample of the full sample message pool or a separate sample from the population under investigation.
the-content-analysis-guidebook-2e-6	08	If the pilot test reveals serious problems, then the coding scheme may need to be changed.
the-content-analysis-guidebook-2e-6	08	In that case, the pilot test data should not be included in the final data analysis; if necessary, the pilot subsample message units/cases should be recoded with the revised scheme.
the-content-analysis-guidebook-2e-6	08	The final reliability assessment should be done on a second, randomly selected subsample during the full data collection to fairly represent the coders’ performance throughout the study.
the-content-analysis-guidebook-2e-6	09	Optimally, final reliability assessment should be conducted at regular intervals Page 9 of 42 The Content Analysis Guidebook pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	09	across the time period of the final data collection for the study.
the-content-analysis-guidebook-2e-6	09	These final reliability figures are the ones to be reported with the study’s results.
the-content-analysis-guidebook-2e-6	09	Reliability assessment in a pilot study of the content under investigation is essential to the development of a valid, reliable, and useful coding scheme.
the-content-analysis-guidebook-2e-6	09	It addresses all four threats to reliability outlined earlier, by allowing the following three diagnostic measures: 1.
the-content-analysis-guidebook-2e-6	09	Identification of problematic measures: When a variable with poor reliability is identified in a pilot test, remedies include (a) further training and rechecking reliability, (b) rewriting coding instructions to clarify the measurement of the variable, (c) changing the categories of the variable (e.g., collapsing categories), and (d) splitting the variable into two or more simpler or more concrete (more manifest) variables.
the-content-analysis-guidebook-2e-6	09	2.
the-content-analysis-guidebook-2e-6	09	The identification of problematic categories or values within a variable: By looking at a Coder-A-byCoder-B matrix of coded values, we may see key confusions that indicate what categories within a variable are not clearly differentiated in the coding scheme.
the-content-analysis-guidebook-2e-6	09	This confusion matrix allows us to dig deeper than simply looking at an overall reliability figure.
the-content-analysis-guidebook-2e-6	09	For example, we may see that where Coder A is coding certain verbal utterances as “attacking,” Coder B tends to systematically code the same utterances as “opposing.” Further training or codebook revisions (or both) are needed to eliminate this systematic source of measurement error.
the-content-analysis-guidebook-2e-6	09	3.
the-content-analysis-guidebook-2e-6	09	The identification of problematic coders: By examining pairwise reliabilities for individual coders, we may see whether one coder simply doesn’t match up with the others.
the-content-analysis-guidebook-2e-6	09	Additional training for that coder may help, before the unpleasant decision to drop the “rogue” coder might be reached.
the-content-analysis-guidebook-2e-6	09	Intercoder Reliability Coefficients: Issues and Comparisons A variety of coefficients are available for reporting the level of agreement or correspondence between coders’ assessments.
the-content-analysis-guidebook-2e-6	09	Some of the more popular coefficients in business and the social and behavioral sciences seem to be raw-percent agreement (the “measure of crude association”), Scott’s pi, Cohen’s kappa, and Krippendorff’s alpha for nominal data, and Spearman’s rho, Pearson r, and Lin’s concordance correlation coefficient (CCC) for ordinal/interval/ratio data.
the-content-analysis-guidebook-2e-6	10	These and the dozens of other coefficients available make certain assumptions about the coders and the coding process (e.g., if “chance” agreement is to be taken into account Page 10 of 42 The Content Analysis Guidebook pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	10	and what type of coding distribution is to be assumed), and such assumptions are not universally agreed upon (Zhao, Liu, & Deng, 2013).
the-content-analysis-guidebook-2e-6	10	Further, only some coefficients accommodate more than two coders at a time, an issue to be addressed later.
the-content-analysis-guidebook-2e-6	10	The sections to follow will address the most important intercoder reliability coefficients, with critical issues related to their basic assumptions laid out.
the-content-analysis-guidebook-2e-6	10	All of these intercoder reliability coefficients are distinct in nature from internal-consistency reliability assessments, which typically rely on such coefficients as Cronbach’s alpha or the Spearman-Brown formula (Carmines & Zeller, 1979; Traub, 1994) to determine how well a set of variables fits together.
the-content-analysis-guidebook-2e-6	10	These internalconsistency statistics examine interitem correlations to see if they warrant combining a set of variables in a scale or index (Babbie, 2013).
the-content-analysis-guidebook-2e-6	10	Both types of reliability coefficient—intercoder and internal consistency—are based on the same core notions of reliability as dependability, reproducibility, or consistency (Traub, 1994), but they usually have quite different applications.3 Intercoder reliability coefficients are not intended to assess internal consistency among a variety of measures.
the-content-analysis-guidebook-2e-6	10	Rather, they are concerned with the assessment, one measure at a time, of one or more of the following criteria: agreement, agreement beyond chance, and covariation.
the-content-analysis-guidebook-2e-6	10	In the discussion that follows, it is assumed that we are looking at only two coders at a time.
the-content-analysis-guidebook-2e-6	10	The case of three-plus coders will be taken up later.
the-content-analysis-guidebook-2e-6	10	Also, note that the discussion includes conceptual formulas when appropriate but does not include calculation formulas for the coefficients (although some can be found in Boxes 6.2 and 6.3).
the-content-analysis-guidebook-2e-6	10	Agreement This criterion is concerned with whether coders agree on the precise values assigned to cases on a given variable.
the-content-analysis-guidebook-2e-6	10	This is particularly appropriate to measures that are categorical (i.e., nominal), wherein each pair of coded values is either a hit or a miss.
the-content-analysis-guidebook-2e-6	10	There are two ways to calculate simple agreement: 1.
the-content-analysis-guidebook-2e-6	10	Percent agreement (sometimes called “crude agreement”): This is a simple percentage, representing number of agreements divided by total number of cases.
the-content-analysis-guidebook-2e-6	11	A conceptual formula for percent agreement could be written as follows: PAO = A/n PAO stands for “proportion agreement, observed,” A is the number of agreements between two coders, and n is the total number of cases the two coders have coded for the test (also, the maximum Page 11 of 42 The Content Analysis Guidebook pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	11	agreement or A they could achieve).
the-content-analysis-guidebook-2e-6	11	This statistic ranges from .00 (no agreement) to 1.00 (perfect agreement).
the-content-analysis-guidebook-2e-6	11	2.
the-content-analysis-guidebook-2e-6	11	Holsti’s method (1969): In cases in which two coders code the same cases (which is the recommended method), this is equal to percent agreement.
the-content-analysis-guidebook-2e-6	11	The formula differs only a little: PAO = 2A/(nA + nB) PAO stands for “proportion agreement, observed,” A is the number of agreements between two coders on the commonly coded cases, and nA and nB are the number of cases coded by coders A and B, respectively.
the-content-analysis-guidebook-2e-6	11	This statistic also ranges from .00 (no agreement) to 1.00 (perfect agreement, with all cases coded by both coders).
the-content-analysis-guidebook-2e-6	11	Note that Holsti’s method includes information about (or we might say “is confounded with”) how much overlap there is between the cases coded by the two coders.
the-content-analysis-guidebook-2e-6	11	Historically, simple agreement has been one of the most commonly reported coefficients.
the-content-analysis-guidebook-2e-6	11	For example, Hughes and Garrett (1990) found that 65% of the reported reliability coefficients in their sample of marketing research articles were simple percent agreement; Manganello and Blake (2010) found 45% of studies of mass media health messages in their review reporting simple agreement.
the-content-analysis-guidebook-2e-6	11	And Feng’s study (2015) of content analyses in two communication journals from 1980 to 2011 found the most commonly reported coefficients to be percent agreement (23%), pi (19%), Holsti’s (15%), and kappa (12%).
the-content-analysis-guidebook-2e-6	11	However, simple agreement has important drawbacks, such as the failure to account for potential chance agreement and the rigid requirement of the precise matching of coders’ scores.
the-content-analysis-guidebook-2e-6	11	Thus, percent agreement and Holsti’s method are often viewed as insufficient tests of intercoder agreement (“Interrater Reliability,” 2001; Lombard et al., 2002).
the-content-analysis-guidebook-2e-6	11	This text recommends reporting simple agreement, but only when accompanied by chance-corrected coefficients.
the-content-analysis-guidebook-2e-6	11	In applying simple agreement assessment to variables that are ordinal, interval, or ratio, some researchers have expanded the notion of precise agreement to what we might call range agreement—counting a hit any time two coders come within a certain numeric distance of one another.
the-content-analysis-guidebook-2e-6	11	Tinsley and Weiss (1975) report on the Lawlis and Lu method of setting up a decision rule for ±1-point or ±2-point agreement.
the-content-analysis-guidebook-2e-6	11	In an application of a similar technique, Dominick (1999) defined an agreement as ratings that were within one point of one another on a 1-to-10 scale.
the-content-analysis-guidebook-2e-6	11	Notably, in his study of personal web sites, the measure in question tapped a rather subjective concept—whether the coders felt they “knew” the person from the content in his or her home page.
the-content-analysis-guidebook-2e-6	12	Page 12 of 42 The Content Analysis Guidebook pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	12	For interval or ratio measures, some researchers have proposed standardizing each coder’s values (Tinsley & Weiss, 1975) before applying agreement assessment procedures so that the coders are statistically calibrated.
the-content-analysis-guidebook-2e-6	12	It is debatable as to whether this forced calibration is desirable over an alternative of more extensive coder training for calibration.
the-content-analysis-guidebook-2e-6	12	Agreement Controlling for the Impact of Chance Agreement Some portion of coders’ agreement might be considered to be due to chance.
the-content-analysis-guidebook-2e-6	12	That is, if two coders are assessing whether audio-recorded voices are male or female, and they flip coins instead of actually listening, they’ll agree about 50% of the time.4 Historically, researchers have been interested in accounting for this presumed chance component.
the-content-analysis-guidebook-2e-6	12	However, this assumption that chance can play such a big role in the coding process has come under fire from some statisticians (Aickin, 1990; Gwet, 2010; Uebersax, 1987).
the-content-analysis-guidebook-2e-6	12	Indeed, coders are not flipping coins, and the portion of their ratings that might lead to agreement by true chance is hopefully very small.
the-content-analysis-guidebook-2e-6	12	Still, to assess the reliability of codings via simple agreement, we should be interested in knowing how much agreement would be expected even if the codings were not reliable.
the-content-analysis-guidebook-2e-6	12	Each chance-corrected statistic proposes a particular model for the latter and then summarizes these two pieces of information by expressing the amount of agreement achieved, above what is expected by chance and relative to maximum improvement.
the-content-analysis-guidebook-2e-6	12	We’re not correcting for an amount actually attributable to chance, just summarizing the observed agreement in a way that is comparable across studies.
the-content-analysis-guidebook-2e-6	12	That is, chance-corrected statistics can be understood as providing context for simple agreement.
the-content-analysis-guidebook-2e-6	12	Several popular agreement-based coefficients serve as “beyond-chance” or chance-corrected indicators: 1.
the-content-analysis-guidebook-2e-6	12	Scott’s pi (π):5 In correcting for the role of chance agreement, this statistic uses a joint distribution across two coders.
the-content-analysis-guidebook-2e-6	12	This takes into account not just the number of categories, but how these categories are used by the coders.
the-content-analysis-guidebook-2e-6	12	The statistic’s typical range is from .00 (agreement at chance level) to 1.00 (perfect agreement), and a value of less than .00 indicates agreement less than chance.
the-content-analysis-guidebook-2e-6	12	The statistic assumes nominal-level data and ignores differences in how the two coders distribute their evaluations across coding categories for that variable (Scott, 1955).
the-content-analysis-guidebook-2e-6	12	2.
the-content-analysis-guidebook-2e-6	12	Cohen’s kappa (κ): This statistic was planned as an improvement over pi, taking into account the differences in coders’ distributions by using a multiplicative term instead of an additive one (Cohen, 1960).
the-content-analysis-guidebook-2e-6	13	Since its introduction, numerous adaptations of this agreement coefficient have been proposed Page 13 of 42 The Content Analysis Guidebook pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	13	(Banerjee et al., 1999; Falotico & Quatto, 2015; Hsu & Field, 2003; Kraemer, 1980).6 Like pi, it assumes nominal-level data and has a typical range from .00 (agreement at chance level) to 1.00 (perfect agreement), and a value of less than .00 again indicates agreement less than chance.
the-content-analysis-guidebook-2e-6	13	Cohen (1968) also took into account the differing importance of misses in his adaptation of kappa, the “weighted kappa coefficient.” In this application, not all misses are treated equally.
the-content-analysis-guidebook-2e-6	13	For example, Bartko and Carpenter (1976) give an example in which psychiatric diagnoses are made by raters (coders): If two raters diagnose a patient as manic-depressive and reactive psychotic depression .
the-content-analysis-guidebook-2e-6	13	.
the-content-analysis-guidebook-2e-6	13	.
the-content-analysis-guidebook-2e-6	13	this disagreement might be weighted 2, while a manic-depression–schizophrenic disagreement might be weighted 4.
the-content-analysis-guidebook-2e-6	13	The more serious the disagreement, the larger the weight.
the-content-analysis-guidebook-2e-6	13	(p.
the-content-analysis-guidebook-2e-6	13	311) Both Scott’s pi and Cohen’s kappa are derived from the same conceptual formula (as are a number of other coefficients, eg., the S coefficient [Zwick, 1988] and Gwet’s AC1 [2010]): pi or kappa = PAO − PAE 1 − PAE In this formula, PAO stands for “proportion agreement, observed,” and PAE stands for “proportion agreement, expected by chance” (under an appropriate model for what is meant by chance).
the-content-analysis-guidebook-2e-6	13	A number of sources report kappa to be the most widely used reliability coefficient (after simple percent agreement; eg., Hsu & Field, 2003; Manganello & Blake, 2010; Perreault & Leigh, 1989; Zwick, 1988).
the-content-analysis-guidebook-2e-6	13	Although the most popular, Cohen’s kappa has come under much criticism, having been identified as having two “paradoxes” or built-in disadvantages, which will be explained further in sections to follow.
the-content-analysis-guidebook-2e-6	13	3.
the-content-analysis-guidebook-2e-6	13	Krippendorff’s alpha (α): This statistic takes into account chance agreement and, in addition, the magnitude of the misses, adjusting for whether the variable is measured as nominal, ordinal, interval, or ratio (Krippendorff, 2013).
the-content-analysis-guidebook-2e-6	13	Its conceptual formula is as follows: alpha = 1 − DO DE In this formula, DO = observed disagreement and DE = expected disagreement.
the-content-analysis-guidebook-2e-6	14	(Note that this is equivalent to the conceptual formula for both pi and kappa, but is just expressed in disagreement rather than agreement Page 14 of 42 The Content Analysis Guidebook pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	14	terms.) The three statistics described above (pi, kappa, alpha) have all been designed to account for “chance agreement,” giving credit only to agreement above or beyond estimated chance agreement, which can be a tough challenge in the case of extreme or unbalanced distributions (Perreault & Leigh, 1989; Potter & Levine-Donnerstein, 1999), or what we might think of as “rare event” variables (e.g., Janstova, 2006).
the-content-analysis-guidebook-2e-6	14	For example, when two categories are joint coded at 90% and 10%, “chance” agreement would be .82, and even 90% raw agreement would net a beyond-chance pi of only .44.
the-content-analysis-guidebook-2e-6	14	This influence of prevalence of certain coding categories has been identified as an important disadvantage by many content analysts (e.g., what to do about this “high agreement but low kappa” outcome has been one of the most-asked questions by readers of this book seeking advice); some arguments about the deficiencies of chance-corrected agreement coefficients have been presented in the literature over the past several decades (Cicchetti, 2007; Cicchetti et al., 2006; Di Eugenio & Glass, 2004; Falotico & Quatto, 2015; Feinstein & Cicchetti, 1990; Gwet, 2002b).
the-content-analysis-guidebook-2e-6	14	Specifically, Feinstein and Cicchetti (1990) identified two key “paradoxes” with such statistics: (1) the “high agreement, low kappa” paradox (as described previously), the tendency for coefficients such as kappa to be unduly affected by a skewed, or unbalanced, distribution, and (2) the tendency of kappa to penalize judges with similar marginals compared to judges who produce different marginals (Warrens, 2010).
the-content-analysis-guidebook-2e-6	14	As Gwet notes, “Pi and kappa are two chance-corrected agreement measures that are highly sensitive to the trait prevalence in the [sample] as well as to differences in rater marginal probabilities.
the-content-analysis-guidebook-2e-6	14	.
the-content-analysis-guidebook-2e-6	14	.
the-content-analysis-guidebook-2e-6	14	.
the-content-analysis-guidebook-2e-6	14	These two properties make the pi and kappa statistics very unstable and often difficult to interpret” (2002a, p 2).
the-content-analysis-guidebook-2e-6	14	One alternative to relying on the popular Cohen’s kappa or similar statistics, particularly in cases of unbalanced distributions, is to report a separate reliability coefficient for each category on a variable (Cicchetti & Feinstein, 1990; they base their approach on the need for both good sensitivity and specificity in testing7).
the-content-analysis-guidebook-2e-6	14	Another alternative is to employ a statistic that is designed to avoid the paradoxes, such as Gwet’s reliability coefficients.
the-content-analysis-guidebook-2e-6	14	4.
the-content-analysis-guidebook-2e-6	14	Gwet’s AC1: Although fairly new on the scene, Gwet’s AC1 reliability coefficient (Gwet 2008a, 2008b; Jimenez, 2014; Wongpakaran et al., 2013) has begun to be a recommended coefficient in the science fields (e.g., Heyman et al., 2014).
the-content-analysis-guidebook-2e-6	14	Gwet’s stated goal in the creation of the coefficient is the diminishment of the impact of the two so-called “kappa paradoxes” on reliability outcomes (Feinstein & Cicchetti, 1990).
the-content-analysis-guidebook-2e-6	15	The AC1 statistic was designed as a “paradox-robust” substitute for existing Page 15 of 42 The Content Analysis Guidebook pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	15	agreement reliability statistics for nominal data, and both Gwet (2002a) and Heyman et al.
the-content-analysis-guidebook-2e-6	15	(2014) present statistical trials that seem to confirm this.
the-content-analysis-guidebook-2e-6	15	Values of AC1 are not substantially lower for unbalanced or skewed distributions, as are pi, kappa, and alpha.
the-content-analysis-guidebook-2e-6	15	Gwet’s correction tries to avoid the instability that comes from a category with low prevalence by using a chance model that assumes only an unknown subset of codings are assigned at random.8 The AC1 statistic can range in value from .00 (indicating no agreement beyond chance) to 1.00 (indicating perfect agreement).
the-content-analysis-guidebook-2e-6	15	Gwet’s AC2 is a weighted version to be used for ordinal, interval, or ratio data.
the-content-analysis-guidebook-2e-6	15	The conceptual formula for Gwet’s AC1 is the same as for pi, kappa and alpha: AC1 = PAO − PAE 1 − PAE Which of the agreement coefficients is best suited to a given task of intercoder reliability assessment is open to debate—a debate that is properly focused on the nature of what is considered “chance,” or expected agreement (PAE in many of the conceptual formulas) for each statistic.
the-content-analysis-guidebook-2e-6	15	Covariation For measures that are ordinal or metric (measured at the interval or ratio level), researchers are often interested in the level of covariation of coders’ scores, particularly in instances where precise agreement is unlikely.
the-content-analysis-guidebook-2e-6	15	For example, if two coders are scoring television characters’ estimated ages in years, it’s unlikely that they will score a precise hit very often.
the-content-analysis-guidebook-2e-6	15	But reliability may be shown in the covariation of their scores—quite literally, whether their age estimates “co-vary”—that is, when one is high, the other is high, and when one is low, the other is low.
the-content-analysis-guidebook-2e-6	15	Thus, the following pairs of age scores for eight characters would show high covariation: Coder A: 25 55 68 35 34 58 72 18 Coder B: 27 58 70 33 30 57 75 17 Notice that the two coders’ level of agreement would be 0% (unless we use some sort of range agreement).
the-content-analysis-guidebook-2e-6	15	Yet we would agree that the two coders display at least an acceptable level of reliability and therefore we might prefer to use a reliability statistic that gives credit for covariation.
the-content-analysis-guidebook-2e-6	16	Coefficients that take covariation into Page 16 of 42 The Content Analysis Guidebook pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	16	account include four statistics that are used more commonly for testing relationships between two variables rather than two coders: 1.
the-content-analysis-guidebook-2e-6	16	Spearman’s rho (ρ): This statistic assumes rank order ordinal data, so its calculation must be conducted on data that are converted to rank-ordered data if they were not collected that way.
the-content-analysis-guidebook-2e-6	16	It ranges from −1.00 (perfect negative relationship, or disagreement) through .00 (no relationship between the two coders’ rankings) to 1.00 (perfect agreement on rankings).
the-content-analysis-guidebook-2e-6	16	2.
the-content-analysis-guidebook-2e-6	16	Intraclass correlation coefficient (ICC): The ICC is a set of statistics within the framework of analysis of variance (ANOVA) that express a ratio of the variance of interest (i.e., shared variance between coders here) to the sum of the variance of interest plus error (Shrout & Fleiss, 1979).
the-content-analysis-guidebook-2e-6	16	It assumes interval/ratio data.
the-content-analysis-guidebook-2e-6	16	For the typical instance of checking intercoder reliability, the two-way random effects, absolute agreement version of the ICC is most appropriate (Heyman et al., 2014).
the-content-analysis-guidebook-2e-6	16	However, like a number of the reliability coefficients for nominal data, the ICC is compromised by skewed distributions.
the-content-analysis-guidebook-2e-6	16	The ICC statistic ranges in value from .00 (no shared variance between coders) to 1.00 (perfect correspondence between coders).
the-content-analysis-guidebook-2e-6	16	3.
the-content-analysis-guidebook-2e-6	16	Pearson correlation coefficient (r): This statistic assesses the degree of linear (i.e., straight line) correspondence between two sets of interval or ratio numbers.
the-content-analysis-guidebook-2e-6	16	The more tightly clustered the data points are around a line, the higher the absolute value of r It should be noted that some prefer the reporting of r2 (the coefficient of determination), in that this represents the proportion of shared variance between the two sets of coder scores and is therefore closer in form to such reliability coefficients as percent agreement and Scott’s pi.
the-content-analysis-guidebook-2e-6	16	In the foregoing age score example, the Pearson r is .995 and r2 is .99.
the-content-analysis-guidebook-2e-6	16	That is, 99% of the variance of Coder A’s age scores is shared with the variance of Coder B’s age scores.
the-content-analysis-guidebook-2e-6	16	The r statistic ranges from −1.00 (perfect negative linear relationship) through .00 (no linear relationship) to 1.00 (perfect positive linear relationship).
the-content-analysis-guidebook-2e-6	16	There are criticisms that coefficients such as r overestimate reliability.
the-content-analysis-guidebook-2e-6	16	Because the Pearson r inherently standardizes the coders’ scores, covariation is assessed, but level of agreement is completely ignored.
the-content-analysis-guidebook-2e-6	16	That is, envision a case in which Coder A always reports a character age that is exactly 10 years older than Coder B’s estimate (i.e., 40/30 years, 70/60 years, 25/15 years, etc.).
the-content-analysis-guidebook-2e-6	16	The Pearson r for these values would be 1.0—a perfect linear relationship, with an r2 of 1.0 (100%).
the-content-analysis-guidebook-2e-6	16	This is for two sets of values with zero agreement, even within a reasonable range, which prompts us to question the validity of the measures.
the-content-analysis-guidebook-2e-6	16	Some adjustment for this coder bias might be advisable.
the-content-analysis-guidebook-2e-6	17	Page 17 of 42 The Content Analysis Guidebook pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	17	In a real example of this issue, while comparing human and computer scoring of verbal samples for level of anxiety, Gottschalk and Bechtel (1993) reported that the computer scoring was significantly lower than the human-coding scores.
the-content-analysis-guidebook-2e-6	17	They reported a correlation between the two sets of scores of r = .85, which seems reasonably reliable.
the-content-analysis-guidebook-2e-6	17	However, the strong covariation notwithstanding, a significant difference would remain, without some type of statistical correction (Gottschalk and Bechtel did employ a correction for their computerscored data).
the-content-analysis-guidebook-2e-6	17	4.
the-content-analysis-guidebook-2e-6	17	Lin’s concordance correlation coefficient (CCC): This is an alternative to Pearson r for measuring covariation of interval or ratio data that additionally takes systematic coding errors into account (Lin, 1989).
the-content-analysis-guidebook-2e-6	17	It assesses the linear relationship between two sets of metric scores under the constraints that the correlation line passes through the origin and has a slope of 1—that is, it assumes a correlation line that shows perfect agreement (Chinchilli et al., 1996).
the-content-analysis-guidebook-2e-6	17	So in a case such as that described earlier, in which one coder always rates age higher than the other coder, the CCC would be lower than the Pearson correlation r, having taken the coder bias into account.
the-content-analysis-guidebook-2e-6	17	Like the Pearson r, this statistic ranges from −1.00 (perfect negative linear relationship) through .00 (no linear relationship) to 1.00 (perfect positive linear relationship).
the-content-analysis-guidebook-2e-6	17	The Lin’s CCC statistic shows promise for rectifying the problems of other covariation statistics used for reliability.
the-content-analysis-guidebook-2e-6	17	Its use has grown exponentially in the science and medical fields, and it has been called the “most popular” reliability indicator in the statistical literature (Barnhart, Haber, & Lin, 2007), but it has not yet established a strong foothold outside of these arenas.9 Originally written for a two-coder scenario, the Lin’s CCC has been extended to multiple coders (Lin, Hedayat, & Wu, 2007).
the-content-analysis-guidebook-2e-6	17	Calculating Intercoder Reliability Coefficients In Box 6.2, formulas for some of the more popular agreement coefficients are presented, and sample calculations are given.
the-content-analysis-guidebook-2e-6	17	And for some, a 95% confidence interval is also reported in an endnote.10 In Box 6.3, the featured covariation coefficients are presented, with calculations for one ordinal-level statistic and outcomes for five interval-or ratio-level statistics.
the-content-analysis-guidebook-2e-6	18	Page 18 of 42 The Content Analysis Guidebook pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	18	Box 6.2 Example Calculations for Agreement Coefficients for Nominal Data Calculating Percent Agreement, Scott’s pi, Cohen’s kappa, Krippendorff’s alpha, and Gwet’s AC1 for Nominal Data In the following example, two coders have coded one nominal (categorical) variable for 10 cases.
the-content-analysis-guidebook-2e-6	18	(Please note that the n of 10 is chosen to simplify the example; no real reliability check should have only 10 cases.) The two coders have assessed web banner ads for type, with three categories: 1 = product ad, 2 = corporate ad, and 3 = other.
the-content-analysis-guidebook-2e-6	18	Both Coder A and Coder B have coded the same 10 banner ads.
the-content-analysis-guidebook-2e-6	18	The outcome of this coding is as follows: First, we can calculate simple percent agreement: PAO = TotalA’s 7 = = .70 (70% agreement) n 10 Another way we could look for agreements (hits) and disagreements (misses) is by generating a crosstabulation table.
the-content-analysis-guidebook-2e-6	18	The bold numbers are the hits, the numbers of cases for which the coders agree (a total of seven out of 10 times).
the-content-analysis-guidebook-2e-6	19	Page 19 of 42 The Content Analysis Guidebook pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	19	Using the marginals (totals) for each coder, we can multiply and add to produce the bits of information we’ll need for Scott’s pi and Cohen’s kappa.
the-content-analysis-guidebook-2e-6	20	Marginals Product of Marginals Sum of Marginals A + Proportion for Sum of Marginals B pi n for Coder n for Coder A B 3 3 9 6 6/20 =.30 5 3 15 8 8/20 =.40 3 (Other) 2 4 8 6 6/20 =.30 TOTALS 10 10 32 20 1.00 Category 1 (Product ad) 2 (Corporate ad) Scott’s pi = PAO – PAE 1 – PAE A x B (pmi) where PAE = Σp2i pi = each proportion for sum of marginals So: PAE = Σp2i = (.30)2 + (.40)2 + (.30)2 Page 20 of 42 The Content Analysis Guidebook pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	21	= .09 + .16 + .09 = .34 = PAO – PAE 1 – PAE .70 – .34 And Scott’s pi = 1 – .34 .36 = .66 = .545 Cohen’s kappa = Where: PAO – PAE 1 – PAE PAE = (1/n2)(∑ pmi) n = number of cases coded in common by coders pmi = each product of marginals Therefore: PAE = (1/n2)(∑ pmi) = (1/102)(9 + 15 + 8) = (1/100)(32) = .32 And Cohen’s kappa = = .70 – .32 1 – .32 = .38 .68 PAO – PAE 1 – PAE = .56 For Krippendorff’s alpha (nominal data only; other forms of data require weightings of products of marginals with various coefficients, beyond the scope of this example), the data must be reconfigured: Page 21 of 42 The Content Analysis Guidebook pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	22	Frequencies for Both Coders Case Coder A Coder B Category 1 Category 2 Category 3 Ad 1 1 1 2 0 0 Ad 2 2 2 0 2 0 Ad 3 2 3 0 1 1 Ad 4 1 3 1 0 1 Ad 5 3 3 0 0 2 Ad 6 1 1 2 0 0 Ad 7 2 2 0 2 0 Ad 8 3 3 0 0 2 Ad 9 2 1 1 1 0 Ad 10 2 2 0 2 0 Σ=6 8 6 The calculation formula for Krippendorff’s alpha (nominal) may be represented as: Krippendorff’s alpha (nominal) = 1 – Where: Page 22 of 42 ( nm – 1 Σpfu m – 1 Σpmt ) pfu = product of any frequencies for a given case that are different (i.e., show disagreeThe Content Analysis Guidebook pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	22	ment) pmt = each product of total marginals n = number of cases coded in common by coders m = number of coders So: pfu = (1 × 1) + (1 × 1) + (1 × 1) [disagreements for Cases 3, 4, and 9] =3 And: pmt = (6 x 8) + (6 x 6) + (8 × 6) [all pairings are added] = 48 + 36 + 48 = 132 ) (10)(2) – 1 3 =1– ( 132 ) 2–1 19 3 =1– 1 ( 132 ) Alpha = 1 – ( nm – 1 Σpfu m – 1 Σpmt = 1 – 19 (.023) = 1 – .43 = .57 Using the same 10-ad example, the following shows the calculation of Gwet’s AC1 statistic.
the-content-analysis-guidebook-2e-6	22	Gwet’s AC1 employs the same conceptual formula as pi and kappa: Box 6.3 Example Calculations for Covariation Coefficients for Ordinal and Ratio Data Demonstrations for Spearman’s rho, the intraclass correlation coefficient (ICC), Krippendorff’s alpha, Pearson correlation (r), and Lin’s concordance correlation coefficient (CCC) In this example of ordinal data, two coders have coded 10 web banner ads for one ordinal, rank-ordering variable, such as vibrancy of the colors in the ad.
the-content-analysis-guidebook-2e-6	23	Case Page 23 of 42 Coder A Coder B Coding Discrepancy (d) The Content Analysis Guidebook pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	24	Ad 1 2 3 −1 Ad 2 6 8 −2 Ad 3 9 10 −1 Ad 4 1 1 0 Ad 5 8 6 2 Ad 6 3 2 1 Ad 7 10 9 1 Ad 8 4 4 0 Ad 9 5 5 0 Ad 10 7 7 0 Spearman’s rho = 1 – Where 6 × Σd2 n3 – n n = number of cases coded in common by coders d = each coding discrepancy (Coder A ranking minus Coder B ranking) Page 24 of 42 The Content Analysis Guidebook pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	24	Spearman’s rho = 1 – =1– So: 6 × Σd2 n3 – n [ 6 × ( – 1)2 + ( – 2)2 + ( – 1)2 + 02 + 22 + 12 + 12 + 02 + 02 + 02 ] 103 – 10 6 × 12 = 1 – 1000 – 10 = 1 – 72 / 990 = 1 – .073 = .927 In the following example, two coders have coded 10 web banner ads for a ratio-level variable, the number of identifiable human characters shown.
the-content-analysis-guidebook-2e-6	25	In this table, the coders’ scores are also shown squared and cross-multiplied in preparation for calculating the Pearson r Case Coder A Coder B A2 B2 A×B Ad 1 3 3 9 9 9 Ad 2 2 1 4 1 4 Ad 3 7 8 49 64 56 Ad 4 0 0 0 0 0 Ad 5 0 1 0 1 0 Ad 6 5 5 25 25 25 Ad 7 3 3 9 9 9 Ad 8 12 10 144 100 120 Page 25 of 42 The Content Analysis Guidebook pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	25	Ad 9 1 1 1 1 1 Ad 10 2 2 4 4 4 Σ = 35 34 245 214 226 A variety of formulas exist for the Pearson r A good conceptual formula would be as follows:11 rab = Σab √(Σa2)(Σb2) Where: a = each deviation score (Coder A score minus mean for A) b = each deviation score (Coder B score minus mean for B) This is the ratio between the covariation of A’s and B’s deviation scores and the product of their individual variations.
the-content-analysis-guidebook-2e-6	25	For the ratio-level data example, the following intercoder reliability statistics are found: ICC (Two-way random, absolute) = .97 Pearson r= .98 Krippendorff’s alpha (interval) = .97 Krippendorff’s alpha (ratio) = .74 Lin’s CCC = .97 Now, suppose that Coder B has a bias such that she or he now codes two values higher for each case (e.g., 5 instead of 3, 3 instead of 1, etc.).
the-content-analysis-guidebook-2e-6	26	Given this, the statistics will be as follows: ICC (Two-way random, absolute) = .85 Pearson r= .98 Krippendorff’s alpha (interval) = .83 Krippendorff’s alpha (ratio) = .21 Lin’s CCC = .83 Page 26 of 42 The Content Analysis Guidebook pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	26	The Reliability Subsample There are several key decisions to make in selecting and using reliability subsamples, both for the pilot reliability assessment and for final reliability.
the-content-analysis-guidebook-2e-6	26	They have to do with subsample size, sampling type, and assignment of cases to coders.
the-content-analysis-guidebook-2e-6	26	Subsample Size How many cases should be used in each reliability assessment? A certain proportion or percentage of the total sample or a certain n? Unfortunately, there is no set standard for this decision.
the-content-analysis-guidebook-2e-6	26	General textbooks on social science research methods present rough guidelines, such as 10% to 20% of the total sample.
the-content-analysis-guidebook-2e-6	26	In a selective review of content analyses, Potter and Levine-Donnerstein (1999) found the reliability subsample size to range from 10% to 100% of the full sample.
the-content-analysis-guidebook-2e-6	26	In their study of 200 content analyses published in the communication literature between 1994 and 1998, Lombard et al.
the-content-analysis-guidebook-2e-6	26	(2002) found that 43% of studies reported a reliability subsample size ranging from 1 to 1,300 with a median of n = 79.
the-content-analysis-guidebook-2e-6	26	Riffe et al.
the-content-analysis-guidebook-2e-6	26	(2014) have provided tabled guidelines for needed subsample size based on three criteria: the full sample size, the minimum desired percent agreement reliability, and the desired confidence level for the estimate.
the-content-analysis-guidebook-2e-6	26	If one could attempt to make a general statement from the accumulated knowledge so far, it would be that the reliability subsample should be at least 10% of the full sample, probably never be smaller than 50, and should rarely need to be larger than about 300.
the-content-analysis-guidebook-2e-6	26	The factors that would indicate the need for a subsample to be at the high end of this range are (a) a large population (i.e., full sample) size and (b) a lower assumed level of reliability in the population (i.e., full sample; Lacy & Riffe, 1996).
the-content-analysis-guidebook-2e-6	26	Sampling Type There are two schools of thought about the type of sampling technique that is appropriate for the reliability subsamples.
the-content-analysis-guidebook-2e-6	26	The most popular choice is to execute a probability subsample by randomly selecting cases from the full message sample.
the-content-analysis-guidebook-2e-6	26	This type of sampling should follow the same guidelines for randomness as extracting the full sample of message units.
the-content-analysis-guidebook-2e-6	26	As outlined in Chapter 3, this probability sampling will be either simple random sampling or systematic sampling.
the-content-analysis-guidebook-2e-6	27	In this case, the sampling frame is the list of all elements in Page 27 of 42 The Content Analysis Guidebook pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	27	the full sample.
the-content-analysis-guidebook-2e-6	27	The second school of thought is focused on applying the content analysis scheme to a wide variety of cases so that all aspects of the coding scheme might receive a “test run.” This type of purposive, nonprobability sample, which might be called a “rich range” subsample (Neuendorf, 2009), can be a useful test of the utility of a coding scheme.
the-content-analysis-guidebook-2e-6	27	For example, the incidence of sex appeals in Thai commercials might be quite low (Wongthongsri, 1993), and a randomly selected reliability subsample is therefore unlikely to include any.
the-content-analysis-guidebook-2e-6	27	This may result in a misleading 100% agreement between coders who uniformly code the total absence of the appeal.
the-content-analysis-guidebook-2e-6	27	Only by making sure that some ads that include sex appeals are in the subsample will the researcher be able to see whether coders can agree on its presence versus absence.
the-content-analysis-guidebook-2e-6	27	Further, a rich range subsample may provide a means of avoiding the first of the two so-called “kappa paradoxes” when calculating reliability coefficients.
the-content-analysis-guidebook-2e-6	27	One way to assure a rich range subsample yet maintain some degree of representativeness would be to use stratified sampling.
the-content-analysis-guidebook-2e-6	27	If strata can be defined that will ensure the presence of attributes on variables of interest, then probability sampling can be employed within the strata.
the-content-analysis-guidebook-2e-6	27	Since currently there is no widespread support for the rich range approach, a compromise might be to use a rich range subsample for training and probability subsamples for both the pilot and the final reliability assessments, as demonstrated by Hubbell and Dearing’s (2003) study of news coverage of health improvement projects.
the-content-analysis-guidebook-2e-6	27	Assignment of Cases to Coders The typical situation is for all coders to receive the same cases to code for reliability purposes.
the-content-analysis-guidebook-2e-6	27	When all coders do not code the same cases from the subsample, the assignment of cases to coders should be random.
the-content-analysis-guidebook-2e-6	28	(This applies to coder assignments for the full sample as well, as indicated in Chapter 3.) Page 28 of 42 The Content Analysis Guidebook pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	28	Treatment of Variables That Do Not Achieve an Acceptable Level of Reliability Assuming that reliability testing has been completed, called-for changes have been made in the coding scheme, and rigorous training of the coders has been conducted, there should be few variables that do not achieve an acceptable level of reliability in the final reliability check.
the-content-analysis-guidebook-2e-6	28	But there probably will be some.
the-content-analysis-guidebook-2e-6	28	The options are several: 1.
the-content-analysis-guidebook-2e-6	28	Drop the variable from all analyses (e.g., Naccarato & Neuendorf, 1998).
the-content-analysis-guidebook-2e-6	28	2.
the-content-analysis-guidebook-2e-6	28	Reconfigure the variable with fewer and better-defined categories (e.g., Fink & Gantz, 1996).
the-content-analysis-guidebook-2e-6	28	Of course, this should be done during the pilot-coding process, prior to the final data collection.
the-content-analysis-guidebook-2e-6	28	3.
the-content-analysis-guidebook-2e-6	28	Use the variable only as a component in a multimeasure index, which itself has been shown to be reliable (e.g., Schulman, Castellon, & Seligman, 1989; Smith, 1999).
the-content-analysis-guidebook-2e-6	28	This is a somewhat questionable practice in that it obscures unacceptable reliabilities for individual aspects of the index.
the-content-analysis-guidebook-2e-6	28	On the other hand, a pragmatic approach would focus on what the original intent was; if it was to measure extraversion, then perhaps the reliability coefficient that counts should be the one for extraversion rather than the ones for its several individual components.12 (See Chapter 5 for a discussion of index construction in content analysis.) 4.
the-content-analysis-guidebook-2e-6	28	Use noncontent analysis data (e.g., survey data) for that particular variable, and integrate the data into the study with other content analysis variables.
the-content-analysis-guidebook-2e-6	28	For example, Kalis and Neuendorf (1989) used survey response data for the variable “perceived level of aggressiveness” for cues present in music videos.
the-content-analysis-guidebook-2e-6	28	And Sweeney and Whissell (1984, following the work of Heise, 1965) created a dictionary of affect in language by presenting research participants with individual words that they were asked to rate along the dimensions of pleasantness and activation.
the-content-analysis-guidebook-2e-6	28	Ratings were obtained from adults for 4,300 words, and these ratings have been used in conjunction with subsequent content analyses, both human coded and computer coded (Whissell, 1994a, 1994b; Whissell et al., 1986).
the-content-analysis-guidebook-2e-6	28	Based on survey work using a large sample of words (n = 15,761), Whissell (2000) was also able to identify distinct emotional characteristics of phonemes, the basic sound units of a language.
the-content-analysis-guidebook-2e-6	28	This allowed her to create profiles for texts in terms of their preferential use of different types of phonemes and therefore their phonoemotional tone.
the-content-analysis-guidebook-2e-6	28	Note that for such research, perceptual ratings may be closer to the researcher’s conceptual definitions of the variables and therefore preferred over traditional content analytic measures.
the-content-analysis-guidebook-2e-6	29	Page 29 of 42 The Content Analysis Guidebook pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	29	The Use of Multiple Coders Many content analyses involve the use of more than two coders.
the-content-analysis-guidebook-2e-6	29	How to conduct reliability analyses in these cases has not been well discussed in the literature.
the-content-analysis-guidebook-2e-6	29	There are several possibilities: 1.
the-content-analysis-guidebook-2e-6	29	Use reliability statistics designed to accommodate multiple-coder statistics—these include Fleiss’ adaptation of Cohen’s kappa (Fleiss, 1971), Krippendorff’s alpha, Gwet’s AC1 and AC2, the ICC, and Lin’s CCC.
the-content-analysis-guidebook-2e-6	29	This will provide a single reliability coefficient for each variable across all coders simultaneously, which is quite useful for the reporting of final reliabilities.
the-content-analysis-guidebook-2e-6	29	However, it is problematic for pilot reliability analyses in that the coefficients obscure pairwise intercoder differences, making it impossible to identify coders who might need extra training or the odd rogue coder.
the-content-analysis-guidebook-2e-6	29	2.
the-content-analysis-guidebook-2e-6	29	Use two-coder reliability statistics in a pairwise fashion, creating a matrix of reliabilities for each variable.
the-content-analysis-guidebook-2e-6	29	This is highly useful as a diagnostic for the pilot reliabilities but can be cumbersome for reporting of final reliabilities.
the-content-analysis-guidebook-2e-6	29	3.
the-content-analysis-guidebook-2e-6	29	Average reliability coefficients across all pairs of coders.
the-content-analysis-guidebook-2e-6	29	This is routinely done for simple percent agreement coefficients but has not been widely used for the other statistics (the reporting of reliability protocols is so often obscure or incomplete, it is possible that this practice is more common than we might think).
the-content-analysis-guidebook-2e-6	29	However, a reporting practice that is common in the science fields might be used for such coefficients as Pearson r or Lin’s CCC—reporting both the average intercoder reliability coefficient and the minimum pairwise reliability coefficient so as not to obscure any problems.
the-content-analysis-guidebook-2e-6	29	However, the use of a Cronbach’s alpha coefficient or the comparable Rosenthal’s R (1987) is not advisable.13 4.
the-content-analysis-guidebook-2e-6	29	There is also the possibility of establishing a distribution for the reliability coefficient across the coders on each variable to examine its shape and look for outliers for possible exclusion.
the-content-analysis-guidebook-2e-6	29	Intracoder Reliability—Assessing Stability Over Time The assessment of the constancy of a given coder’s ratings over time—that is, checking for stability reliability (Weber, 1990)—has gained some traction in recent years (Neuendorf, 2009).
the-content-analysis-guidebook-2e-6	29	Similar to the test–retest method for assessing internal consistency reliability of a set of measures (Carmines & Zeller, 1979), this procedure requires coders to code a given set of cases at more than one point in time, and any changes in their coding are identified.
the-content-analysis-guidebook-2e-6	29	Although Krippendorff (2013) cautions that it is the “weakest form of reliability” (p.
the-content-analysis-guidebook-2e-6	30	271), some researchers have applied intracoder reliability assessment throughout the years (particularly in clinical Page 30 of 42 The Content Analysis Guidebook pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	30	rating situations, eg., Fryday-Field et al., 1994).
the-content-analysis-guidebook-2e-6	30	Increasingly, content analysis scholars have begun to report intracoder reliability as a supplement to their reportage of intercoder reliability (Neuendorf, 2009; eg., Shephard & Cairney, 2005; Sieben, 2014).
the-content-analysis-guidebook-2e-6	30	Examining intracoder reliability is the means by which researchers can pinpoint undesired changes in coder performance over the period of a study—that is, coder drift, which can affect both intercoder reliability and the validity of the measures.
the-content-analysis-guidebook-2e-6	30	Retraining throughout the coding schedule can be used to recalibrate the coders’ application of the codebook and protocol.
the-content-analysis-guidebook-2e-6	30	Unitizing Reliability As described in Chapter 3, clear agreement on the identification of codable units in the message pool is of utmost importance.
the-content-analysis-guidebook-2e-6	30	Reliability is compromised whenever coders have difficulty in identifying units.
the-content-analysis-guidebook-2e-6	30	Currently, few researchers report such reliability (e.g., Neuendorf et al., 2010; Rodriguez et al., 2010; Smith et al., 2010; Strijbos et al., 2006), but the issue is certainly an important one for future consideration (Rudy, Popova, & Linz, 2010).
the-content-analysis-guidebook-2e-6	30	Unfortunately, a standard has not been set for statistical assessment of unitizing reliability.
the-content-analysis-guidebook-2e-6	30	One statistic that has been proposed, Guetzkow’s agreement statistic U (Guetzkow, 1950), assesses only the comparative number of units identified by the coders, not whether the precise units are the same between coders.
the-content-analysis-guidebook-2e-6	30	One may apply the typical nominal-level agreement coefficients to data-describing agreements and disagreements in unitizing, but it may be difficult to identify a clear “hit” versus “miss” in instances where content is a continuous stream that must be segmented, such as transcripts and video presentations.
the-content-analysis-guidebook-2e-6	30	Krippendorff (2013) has extended his alpha intercoder reliability coefficient to apply to differences in coders’ unitizing of continuous records (only), but at present, no easy way to apply such an analysis is in place.
the-content-analysis-guidebook-2e-6	31	Calculating Reliability Coefficients: Programs and Calculators The by-hand calculation examples shown earlier for selected reliability coefficients are intended to help the Page 31 of 42 The Content Analysis Guidebook pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	31	reader better understand the underpinnings of these statistics.
the-content-analysis-guidebook-2e-6	31	In practice, there is no need to hand calculate, as a variety of computer programs and calculators have been developed; as of this writing, at least nine different tools are available.
the-content-analysis-guidebook-2e-6	31	They range in features, cost, and flexibility, but as no single tool provides the full set of reliability coefficients, they offer a patchwork of possibilities.
the-content-analysis-guidebook-2e-6	31	In Table 6.1, the available options are indicated.
the-content-analysis-guidebook-2e-6	31	Special Issues in Reliability Coefficient Decision-Making Alternative Coefficients There are dozens of alternative reliability coefficients (e.g., Popping, 1988; Shoukri, 2011; Zhao et al., 2013), used either for specialized applications or not established and widely used among content analysts.
the-content-analysis-guidebook-2e-6	31	These include the C and S coefficients, alternatives to kappa.
the-content-analysis-guidebook-2e-6	31	(Janson & Vegelius, 1979), Finn’s r (Whitehurst, 1984), the Lawlis-Lu chi-square (Tinsley & Weiss, 1975), and Winer’s dependability index (Hughes & Garrett, 1990), among others.
the-content-analysis-guidebook-2e-6	31	Heyman et al.
the-content-analysis-guidebook-2e-6	31	(2014) include Holley and Guilford’s G statistic (1964) in their head-to-head comparisons of nominal-level statistics and find this older statistic’s performance to be comparable to that of Gwet’s AC1.
the-content-analysis-guidebook-2e-6	32	Page 32 of 42 The Content Analysis Guidebook pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	33	Table 6.1 Programs and Calculators for Reliability Coefficients Page 33 of 42 The Content Analysis Guidebook pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	34	* http://www2.gsu.edu/~psyrab/BakemanPrograms.htm Page 34 of 42 The Content Analysis Guidebook pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	34	** http://www.afhayes.com/spss-sas-and-mplus-macros-and-code.html (see Hayes & Krippendorff, 2007) Traub (1994) presents an interesting statistical complement to the traditional use of Cohen’s kappa, proposing the use of a ratio of kappa/kappamax, where kappamax is the largest achievable kappa, with the marginals of the joint distribution held constant.
the-content-analysis-guidebook-2e-6	34	Standard Errors and Confidence Intervals As noted earlier, there is increasing interest among content analysts in the merits of constructing confidence intervals around reliability estimates (Gwet, 2008a).
the-content-analysis-guidebook-2e-6	34	Perreault and Leigh’s I coefficient (1989) is designed as an estimate of a true population level of agreement and as such welcomes the calculation of confidence intervals.
the-content-analysis-guidebook-2e-6	34	Hopefully, tools to more readily construct confidence intervals for all the various statistical options are forthcoming.
the-content-analysis-guidebook-2e-6	34	(See also endnote 10.) Controlling for Covariates A little-used diagnostic for reliability analyses involves comparing reliability coefficients across values of a control variable.
the-content-analysis-guidebook-2e-6	34	For example, Kacmar and Hochwarter (1996) used ANOVA to compare intercoder agreement scores across three combinations of medium types—transcripts–audio, audio–video, and transcripts–video.
the-content-analysis-guidebook-2e-6	34	They found no significant differences between the three types, showing that this potential covariate was not significantly related to reliability outcomes.
the-content-analysis-guidebook-2e-6	34	Sequential Overlapping Reliability Coding Researchers have considered the possibility of using sequential-overlapping coding for reliability testing.
the-content-analysis-guidebook-2e-6	34	For example, Coder A codes cases 1 through 20, Coder B codes cases 11 through 30, Coder C codes cases 21 through 40, and so on so that every case in the reliability subsample is coded by precisely two coders, but there are multiple coders used.
the-content-analysis-guidebook-2e-6	34	This overlapping of coder assignments is used to maximize the reliability subsample size and to better meet the second goal of intercoder reliability—the practical advantage of more coders and more cases coded.
the-content-analysis-guidebook-2e-6	34	This technique is not standard at present, and more research is needed to establish the utility of this option.
the-content-analysis-guidebook-2e-6	35	Potter and Levine-Donnerstein (1999) do not support its use, but they also do Page 35 of 42 The Content Analysis Guidebook pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	35	not present a statistical argument against it.
the-content-analysis-guidebook-2e-6	35	The practice of having different cases rated by different coders does have some precedent; Fleiss (1971) and Kraemer (1980) have developed extensions of kappa in which cases are rated by different sets of judges.
the-content-analysis-guidebook-2e-6	35	Bringing Order to Reliability: Building Models That Specify Sources of Variation The lack of uniform standards for reliability assessment in content analysis has led to much debate.
the-content-analysis-guidebook-2e-6	35	Different researchers promote particular statistics, sometimes of their own invention, and the few reviews of multiple statistics that have been published come to somewhat different conclusions (e.g., Heyman et al., 2014; Kottner et al., 2011; Zhao et al., 2013).
the-content-analysis-guidebook-2e-6	35	No third-party assessment body has performed a full, comprehensive review and analysis.
the-content-analysis-guidebook-2e-6	35	Such a review of the current techniques and the development of additional techniques to “fill gaps” is needed.
the-content-analysis-guidebook-2e-6	35	At a fundamental level, this could begin with researchers building custom models that specify the options for sources of variation in human coding, allowing the tailoring of reliability assessment to the goals at hand.
the-content-analysis-guidebook-2e-6	35	That is, the needs for reliability assessment may vary study by study, or even variable by variable, in that the assumptions about the sources of coder variation may differ.
the-content-analysis-guidebook-2e-6	35	As Shrout and Fleiss (1979) noted, a chosen model will specify the decomposition of a rating, made by the ith judge on the jth target in terms of various effects.
the-content-analysis-guidebook-2e-6	35	Among the possible effects are those for the ith judge, for the jth target, for the interaction between judge and target, for the constant level of ratings, and for a random error component.
the-content-analysis-guidebook-2e-6	35	(p.
the-content-analysis-guidebook-2e-6	35	421) Banerjee et al.
the-content-analysis-guidebook-2e-6	35	(1999) advocated the use of statistical models to represent the structure of agreement rather than summarizing it with a single number.
the-content-analysis-guidebook-2e-6	35	Bayerl and Paul (2011) provide an interesting example, in which they focus on identifying factors that influence reported agreement values for manual annotations in computational linguistics; in a meta-analysis, they found seven, including the annotation domain and intensity of annotator training.
the-content-analysis-guidebook-2e-6	36	Page 36 of 42 The Content Analysis Guidebook pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	36	Such modeling of the assessment of reliability has received attention in the realm of ratings for clinical applications in medicine and science (e.g., Agresti, 1992; Banerjee et al., 1999; Nelson & Edwards, 2015; Shoukri, 2011; Uebersax, 1992), but has largely been ignored in the arena of content analysis.
the-content-analysis-guidebook-2e-6	36	Specifying a model for a given investigation would guide the researcher to make decisions about assumptions about coders.
the-content-analysis-guidebook-2e-6	36	This would include, for example, a researcher deciding whether coders are assumed to be experts, in which case their coding differences might be valued, analyzed, and summarized, as is sometimes done with clinical applications of coding/rating (e.g., Goodwin, 2001).
the-content-analysis-guidebook-2e-6	36	It could provide a basis for deciding whether one is warranted in using final data from more than one coder in order to increase the power of the measures.
the-content-analysis-guidebook-2e-6	36	Modeling would also guide the researcher to make decisions regarding assumptions about the measures.
the-content-analysis-guidebook-2e-6	36	As such, it could include decisions as to whether unreliability-based reduction of relationships among variables might support correction for attenuation, as is used for internal consistency unreliability for multimeasure indexes (Carmines & Zeller, 1979).
the-content-analysis-guidebook-2e-6	36	And it could help clarify whether order effects for the measurement across messages coded might be an issue.
the-content-analysis-guidebook-2e-6	36	At a more focused level, model specification can guide the researcher in using pilot reliability assessment as a set of diagnostics in order to refine coding protocols and coder training.
the-content-analysis-guidebook-2e-6	36	For example, confusion matrices can allow the microscopic examination of categories within variables, revealing how often coders might mistake one category for another.
the-content-analysis-guidebook-2e-6	36	Modeling can also guide analyses and interpretations of final reliability assessment.
the-content-analysis-guidebook-2e-6	36	For example, there are aspects of coder performance that might be considered to be part of a model of coder variation—coder “drift,” including coder fatigue and coder learning, which would be expected to change over the time period of a study’s execution.
the-content-analysis-guidebook-2e-6	36	The inclusion of over-time shifts in a model for a given study might dictate that date/time of coding should be a variable to assess for its relationship to intercoder reliability.
the-content-analysis-guidebook-2e-6	36	And most basically, modeling coder variability offers the possibility of accounting for this additional source of variation in analyses and in study design.
the-content-analysis-guidebook-2e-6	36	Full Reportage As with other aspects of methodology, reliability assessment should be given full reportage.
the-content-analysis-guidebook-2e-6	37	Content analysis results should include details on the pilot and final reliability samples (size, type of sampling; eg., Kaye & Page 37 of 42 The Content Analysis Guidebook pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	37	Sapolsky, 2009) for intercoder reliability checks, and if intracoder and unitizing reliability assessments have been made, these should be described.
the-content-analysis-guidebook-2e-6	37	Reliability coefficients should be reported variable by variable (e.g., Conrad, Dixon, & Zhang, 2009; Danowski & Park, 2009; Magi, 2010), and optimally a combination of coefficients should be reported in order to get a broader impression of the outcome (e.g., percent agreement and a chance-corrected agreement coefficient such as Cohen’s kappa or Gwet’s AC1; Kottner et al., 2011).
the-content-analysis-guidebook-2e-6	37	Whenever possible, confidence intervals for reliability statistics should be given (Kottner et al., 2011).
the-content-analysis-guidebook-2e-6	37	Further reliability diagnostics would be a plus—that is, an analysis of confusion matrices to discover problematic categories and by-coder analyses to identify problematic or rogue coders.
the-content-analysis-guidebook-2e-6	37	And if a full model clarifying assumptions about the various sources of variance for coder ratings can be specified, that should be done as well.
the-content-analysis-guidebook-2e-6	37	Variables that do not achieve acceptable reliability levels should not be included in subsequent analyses.
the-content-analysis-guidebook-2e-6	37	How measures that make the cut are analyzed and the results reported are the province of Chapter 8.
the-content-analysis-guidebook-2e-6	37	Notes for Chapter 6 1.
the-content-analysis-guidebook-2e-6	37	Noting that there have been several variations on the calculation of kappa’s standard deviation, Bartko and Carpenter (1976, p 310) present this pair of formulas for testing the size of a kappa coefficient with a z-test: √ σk = PAO(1 − PAo) / n(1 − PAE)2 z = kappa / σk Where σk is the estimate of the standard deviation of kappa, PAO is the proportion agreement, observed, PAE is the proportion agreement expected by chance, and n is the total number of cases the coders have assessed.
the-content-analysis-guidebook-2e-6	37	The value of z is assessed in a table of normal curve probabilities.
the-content-analysis-guidebook-2e-6	37	A statistically significant z indicates that the kappa coefficient is significantly different from zero (i.e., not significantly “beyond chance”).
the-content-analysis-guidebook-2e-6	37	However, a stronger alternative to Bartko and Carpenter’s method would be to test the kappa against a higher, predetermined threshold, such as a minimal recommended level.
the-content-analysis-guidebook-2e-6	37	2.
the-content-analysis-guidebook-2e-6	38	Stated differently, we could say, “We are 95% confident that the true population reliability for this variable—number of verbal nonfluencies—is between .88 and .96.” Page 38 of 42 The Content Analysis Guidebook pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	38	3.
the-content-analysis-guidebook-2e-6	38	Occasionally, an internal consistency reliability coefficient is used in content analysis when an index is constructed.
the-content-analysis-guidebook-2e-6	38	For example, Peterson, Bettes, and Seligman (1985) combined the ratings of four judges and properly reported Cronbach’s alphas for three different composite indexes of ratings for “bad event” attributions from respondent essays.
the-content-analysis-guidebook-2e-6	38	And Lunk (2008) declined to construct indexes of items designed to measure individualism and collectivism in social media postings when Cronbach’s alphas were found to be insufficient.
the-content-analysis-guidebook-2e-6	38	4.
the-content-analysis-guidebook-2e-6	38	When considering agreement due to chance, there is an implicit assumption on what the distribution of coder ratings is.
the-content-analysis-guidebook-2e-6	38	Here, the assumption is that the coders pick uniformly between categories and independently of the case.
the-content-analysis-guidebook-2e-6	38	5.
the-content-analysis-guidebook-2e-6	38	In this text, repeated use of Greek letters will be minimized to avoid the glazing over of the reader’s eyes.
the-content-analysis-guidebook-2e-6	38	6.
the-content-analysis-guidebook-2e-6	38	The most sophisticated and particularized applications of all the reliability coefficients appear in the medical literature, wherein rater or coder decisions can have life-and-death implications.
the-content-analysis-guidebook-2e-6	38	Banerjee et al.’s (1999) fine review of agreement coefficients was partly funded by the National Cancer Institute.
the-content-analysis-guidebook-2e-6	38	7.
the-content-analysis-guidebook-2e-6	38	Sensitivity of a test is the proportion of true positives that are correctly identified, and specificity of a test is the proportion of true negatives that are correctly identified.
the-content-analysis-guidebook-2e-6	38	For example, the proportion of sick people who test positive for disease would be sensitivity, while the proportion of physically well people who test negative would be specificity.
the-content-analysis-guidebook-2e-6	38	Cicchetti and Feinstein (1990) argue that since both are important to assess, separate reliability coefficients for each should be calculated.
the-content-analysis-guidebook-2e-6	38	8.
the-content-analysis-guidebook-2e-6	38	It is worth noting that Gwet redefines PAE to mean something different than what is meant by Scott, Cohen, or others.
the-content-analysis-guidebook-2e-6	38	He is no longer comparing the observed agreement to what could be expected by chance if raters had rated randomly, but to something else instead.
the-content-analysis-guidebook-2e-6	38	In his words, “Unlike the kappa- and pi-statistics, this agreement coefficient uses a chance-agreement probability that is calibrated to be consistent with the propensity of random rating that is suggested by the observed ratings” (Gwet, 2008a, p 38).
the-content-analysis-guidebook-2e-6	38	When one category has very low prevalence, Gwet’s PAE approaches zero, and his statistic reduces to observed agreement.
the-content-analysis-guidebook-2e-6	38	9.
the-content-analysis-guidebook-2e-6	38	A Web of Science citation check shows over 2,000 cites to Lin (1989), nearly all of which are in the Science Citation Index, with most applying the statistic as a general indicator of concordance between variables, not as an intercoder/interrater reliability check.
the-content-analysis-guidebook-2e-6	38	10.
the-content-analysis-guidebook-2e-6	39	To show the application of confidence intervals to the reportage of reliability coefficients, the confidence intervals (CIs) for the percent agreement, Scott’s pi, and Cohen’s kappa from Box 6.2 are calculated as folPage 39 of 42 The Content Analysis Guidebook pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	39	lows.
the-content-analysis-guidebook-2e-6	39	For percent agreement: 95% CI = percent agreement ± 1.96 SE Where : SE = √ (PAO)(1 − PAO) n SE = √(.7) (.3) / 10 = .145 95% CI = .70 ± (1.96)(.145) = .70 ± .284 = .416 – .984 For Scott’s pi: 95% CI = pi ± (1.96)σπ Where : σ2π = = .70 (1 − .70) 10 (1 − .34)2 = .70 (.30) 10 (.44) PAO(1 − PAO) n (1 − PAE) 2 = .048 So : σπ = .22 95% CI = .545 ± (1.96)(.22) = .545 ± .43 = .115 − .975 For Cohen’s kappa: Notice how large the CIs are for pi and kappa—we are 95% confident that the population (i.e., full sample) Scott’s pi reliability for banner ad type is between .115 and .975, and we are 95% confident that the population Cohen’s kappa reliability for banner ad type is between .15 and .97.
the-content-analysis-guidebook-2e-6	39	These unacceptably huge intervals are a result of the very small reliability subsample size (n = 10) chosen to facilitate the by-hand calculations here.
the-content-analysis-guidebook-2e-6	40	Page 40 of 42 The Content Analysis Guidebook pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	40	With a subsample of 100, the CIs would instead be .408 ↔ .682 and .42 ↔ .70, respectively.
the-content-analysis-guidebook-2e-6	40	11.
the-content-analysis-guidebook-2e-6	40	For hand calculations, a better formula for Pearson r would be as follows: nΣAB – (ΣA)(ΣB) r= √[nΣA – (ΣA)2][nΣB2 – (ΣB)2] 2 = = = 10 × 226 – 35 × 34 √[10 × 245 – 352][10 × 214 – 342] 2260 – 1190 √[2450 – 1225][2140 – 1156] 1070 √1225 × 984 = .97 And the formula for Lin’s concordance coefficient (CCC) is: estimated CCC = 2 Σa2 Σb2 n + n + ( Σabn ) (MeanA – MeanB) 2 Where: a = each deviation score (Coder A score minus mean for Coder A) b = each deviation score (Coder B score minus mean for Coder B) n = number of cases coded in common by coders 12.
the-content-analysis-guidebook-2e-6	40	On the other hand, using the approach of retaining only the reliable individual indicators may be seen as capitalizing on chance.
the-content-analysis-guidebook-2e-6	40	That is, if 50 items measuring masculinity are attempted, and only five items achieve an acceptable level of reliability of .90, the reliability of the total set may be seen as not exceeding chance.
the-content-analysis-guidebook-2e-6	40	Controls comparable to the Bonferroni adjustment for multiple statistical tests might be employed in future efforts.
the-content-analysis-guidebook-2e-6	40	13.
the-content-analysis-guidebook-2e-6	40	Cronbach’s alpha, most typically used as an internal consistency reliability statistic for multiple items in an index, is essentially an averaged r with an adjustment for number of items in an index or scale (Carmines & Zeller, 1979).
the-content-analysis-guidebook-2e-6	40	It has on occasion been used to assess intercoder reliability (e.g., Robertson & Murachver, 2006; Schulman et al., 1989).
the-content-analysis-guidebook-2e-6	40	Rosenthal’s R (1987) was introduced specifically as a proposed intercoder reliability coefficient for interval/ratio data, with a formula identical to that for Cronbach’s alpha.
the-content-analysis-guidebook-2e-6	40	Rosenthal’s R is calculated from the mean intercoder correlation coefficient with an adjustment for number of coders.
the-content-analysis-guidebook-2e-6	41	In the case of the use of Cronbach’s alpha or Rosenthal’s R effective reliability for intercoder reliability, the asPage 41 of 42 The Content Analysis Guidebook pgbrkSage Sage Research Methods © 2017 by SAGE Publications, Inc.
the-content-analysis-guidebook-2e-6	41	sumption is that the researcher is attempting to generalize from a set of coders to a population of potential coders, not something that is currently widely embraced as the principal aspect of intercoder reliability (although Gwet’s [2010] inferential procedures take this into account).
the-content-analysis-guidebook-2e-6	41	Simply adding coders without increasing average intercoder reliability may result in greatly inflated apparent reliability coefficients.
the-content-analysis-guidebook-2e-6	41	This seeming advantage to adding coders should be viewed critically.
the-content-analysis-guidebook-2e-6	41	For example, in the case of six coders who have an average correlation of only .40, the Cronbach’s alpha/Rosenthal’s R adjusts upward to .80.
the-content-analysis-guidebook-2e-6	41	For 10 coders and an average correlation of .40, the Cronbach’s alpha/Rosenthal’s R is .87.
the-content-analysis-guidebook-2e-6	41	Notice that given a consistent level of reliability averaged among pairs of coders, increasing the number of coders inevitably results in the (arbitrary) inflation of R, the effective reliability.
the-content-analysis-guidebook-2e-6	42	This adjustment for number of coders can be seen in the following formula: Cronbach’s alpha/Rosenthal’s R = m(Meanr)/[1 + (Meanr)(m – 1)] Where: Meanr = the mean of all intercoder correlations for a given variable m = the number of coders https://doi.org/10.4135/9781071802878 Page 42 of 42 The Content Analysis Guidebook pgbrk
